<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>CAB403-Systems-Programming</title>
        <meta name="robots" content="noindex" />


        <!-- Custom HTML head -->
        
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->

        <!-- MathJax -->
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

        <!-- Custom JS scripts for mdbook-pdf PDF generation -->
        <script type='text/javascript'>
            let markAllContentHasLoadedForPrinting = () =>
                window.setTimeout(
                    () => {
                        let p = document.createElement('div');
                        p.setAttribute('id', 'content-has-all-loaded-for-mdbook-pdf-generation');
                        document.body.appendChild(p);
                    }, 100
                );

            window.addEventListener('load', () => {
                // Expand all the <details> elements for printing.
                r = document.getElementsByTagName('details');
                for (let i of r)
                    i.open = true;

                try {
                    MathJax.Hub.Register.StartupHook('End', markAllContentHasLoadedForPrinting);
                } catch (e) {
                    markAllContentHasLoadedForPrinting();
                }
            });
        </script>
    <div style="display: none"><a href="#preface">preface</a><a href="#.week_1-preface">.week_1-preface</a><a href="#week_1-operating-systems">week_1-operating-systems</a><a href="#week_1-input_and_output">week_1-input_and_output</a><a href="#week_1-pointers">week_1-pointers</a><a href="#week_1-functions">week_1-functions</a><a href="#.week_2-preface">.week_2-preface</a><a href="#week_2-operating-system-structures">week_2-operating-system-structures</a><a href="#week_2-arrays">week_2-arrays</a><a href="#week_2-strings">week_2-strings</a><a href="#week_2-structures">week_2-structures</a><a href="#week_2-memory_management">week_2-memory_management</a><a href="#.week_3-preface">.week_3-preface</a><a href="#week_3-processes">week_3-processes</a><a href="#.week_4-preface">.week_4-preface</a><a href="#week_4-threads">week_4-threads</a><a href="#.week_5-preface">.week_5-preface</a><a href="#week_5-synchronisation">week_5-synchronisation</a><a href="#week_5-synchronisation_examples">week_5-synchronisation_examples</a><a href="#.week_8-preface">.week_8-preface</a><a href="#week_8-cpu_scheduling">week_8-cpu_scheduling</a></div>
    </head>
    <body>
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="preface.html">Preface</a></li><li class="chapter-item expanded "><a href=".week_1/preface.html"><strong aria-hidden="true">1.</strong> Week 1</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="week_1/operating-systems.html"><strong aria-hidden="true">1.1.</strong> Operating Systems</a></li><li class="chapter-item expanded "><a href="week_1/input_and_output.html"><strong aria-hidden="true">1.2.</strong> Input and Output</a></li><li class="chapter-item expanded "><a href="week_1/pointers.html"><strong aria-hidden="true">1.3.</strong> Pointers</a></li><li class="chapter-item expanded "><a href="week_1/functions.html"><strong aria-hidden="true">1.4.</strong> Functions</a></li></ol></li><li class="chapter-item expanded "><a href=".week_2/preface.html"><strong aria-hidden="true">2.</strong> Week 2</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="week_2/operating-system-structures.html"><strong aria-hidden="true">2.1.</strong> Operating System Structures</a></li><li class="chapter-item expanded "><a href="week_2/arrays.html"><strong aria-hidden="true">2.2.</strong> Arrays</a></li><li class="chapter-item expanded "><a href="week_2/strings.html"><strong aria-hidden="true">2.3.</strong> Strings</a></li><li class="chapter-item expanded "><a href="week_2/structures.html"><strong aria-hidden="true">2.4.</strong> Structures</a></li><li class="chapter-item expanded "><a href="week_2/memory_management.html"><strong aria-hidden="true">2.5.</strong> Dynamic Memory Management</a></li></ol></li><li class="chapter-item expanded "><a href=".week_3/preface.html"><strong aria-hidden="true">3.</strong> Week 3</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="week_3/processes.html"><strong aria-hidden="true">3.1.</strong> Processes</a></li></ol></li><li class="chapter-item expanded "><a href=".week_4/preface.html"><strong aria-hidden="true">4.</strong> Week 4</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="week_4/threads.html"><strong aria-hidden="true">4.1.</strong> Threads</a></li></ol></li><li class="chapter-item expanded "><a href=".week_5/preface.html"><strong aria-hidden="true">5.</strong> Week 5</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="week_5/synchronisation.html"><strong aria-hidden="true">5.1.</strong> Synchronisation</a></li><li class="chapter-item expanded "><a href="week_5/synchronisation_examples.html"><strong aria-hidden="true">5.2.</strong> Synchronisation Examples</a></li></ol></li><li class="chapter-item expanded "><a href=".week_8/preface.html"><strong aria-hidden="true">6.</strong> Week 8</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="week_8/cpu_scheduling.html"><strong aria-hidden="true">6.1.</strong> CPU Scheduling</a></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">CAB403-Systems-Programming</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="cab403-study-guide--2023-semester-1"><a class="header" href="#cab403-study-guide--2023-semester-1">CAB403 Study Guide | 2023 Semester 1</a></h1>
<p>Timothy Chappell | Notes for CAB403 at the Queensland University of Technology</p>
<h2 id="unit-description"><a class="header" href="#unit-description">Unit Description</a></h2>
<h2 id="disclaimer"><a class="header" href="#disclaimer">Disclaimer</a></h2>
<p>Everything written here is based off the QUT course content and the recommended
text books. If any member of the QUT staff or a representative of such finds 
any issue with these guides please contact me at jeynesbrook@gmail.com. </p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="week-1"><a class="header" href="#week-1">Week 1</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="operating-systems"><a class="header" href="#operating-systems">Operating Systems</a></h1>
<h2 id="what-is-an-operating-system"><a class="header" href="#what-is-an-operating-system">What is an Operating System</a></h2>
<p>An operating system is a program that acts as an intermediary between a user of
a computer and the computer hardware. It's acts as a resource allocator managing 
all resources and decides between conflicting requests for efficient and fair 
resource use. An OS also controls the execution of programs to prevent errors 
and improper use of the computer.</p>
<p>The operating system is responsible for:</p>
<ul>
<li>Executing programs</li>
<li>Make solving user problems easier</li>
<li>Make the computer system convenient to use</li>
<li>Use the computer hardware in an efficient manner</li>
</ul>
<h2 id="computer-system-structure"><a class="header" href="#computer-system-structure">Computer System Structure</a></h2>
<p>Computer systems can be divided into four main components</p>
<ol>
<li><strong>Hardware</strong>: These items provide basic computing resources, i.e. CPU, memory, 
I/O devices.</li>
<li><strong>Operating system</strong>: Controls and coordinates the use of hardware among 
various applications and users.</li>
<li><strong>Application programs</strong>: These items define the ways in which the system
resources are used to solve the computing problems of the use, i.e. word processors,
compilers, web browsers, database systems, video games.</li>
<li><strong>Users</strong>: People, machines, or other computers.</li>
</ol>
<h2 id="computer-startup"><a class="header" href="#computer-startup">Computer Startup</a></h2>
<p>A bootstrap program is loaded at power-up or reboot. This program is typically
stored in ROM or EPROM and is generally know as firmware. This bootstrap program
is responsible for initialising all aspects of the system, loading the operating
system kernel, and starting execution.</p>
<h2 id="computer-system-organisation"><a class="header" href="#computer-system-organisation">Computer System Organisation</a></h2>
<ul>
<li>I/O devices and the CPU can execute concurrently.</li>
<li>Each device controller is in charge of a particular device type and has a local
buffer.</li>
<li>The CPU moves data from/to the main memory to/from local buffers.</li>
<li>I/O is from the device to the local buffer of a particular controller.</li>
<li>The device controller informs the CPU that it has finished its operation by
causing an interrupt.</li>
</ul>
<h2 id="common-functions-of-interrupts"><a class="header" href="#common-functions-of-interrupts">Common Functions of Interrupts</a></h2>
<p>Operating systems are interrupt driven. Interrupts transfer control to the 
interrupt service routine. This generally happens through the interrupt vector 
which contains the addresses of all the service routines. The interrupt architecture 
must save the address of the interrupted instruction.</p>
<p>A trap or exception is a software-generated interrupt caused by either an error
or a user request.</p>
<h2 id="interrupt-handling"><a class="header" href="#interrupt-handling">Interrupt Handling</a></h2>
<p>The operating systems preserves the state of the CPU by storing registers and the
program counter. It then determines which type of interrupt occured, polling or 
vectored interrupt system.</p>
<p>Once determined what caused the interrupt, separate segments of code determine
what action should be taken for each type of interrupt.</p>
<p><img src="week_1/../media/interrupt_timeline.jpg" alt="Figure: Interrupt timeline for a single program doing output." /><br />
<strong>Figure: Interrupt timeline for a single program doing output.</strong></p>
<h2 id="io-structure"><a class="header" href="#io-structure">I/O Structure</a></h2>
<p>There are two ways I/O is usually structured:</p>
<ol>
<li>After I/O starts, control returns to the user program only upon I/O completion.
<ul>
<li>Wait instructions idle the CPU until the next interrupt.</li>
<li>At most, one I/O request is outstanding at a time. This means no simultaneous 
I/O processing.</li>
</ul>
</li>
<li>After I/O starts, control returns to the user program without waiting for I/O 
completion.
<ul>
<li><strong>System call</strong>: Request to the OS to allow users to wait for I/O completion.</li>
<li>A <strong>device-status table</strong> containes entries for each I/O device indicating
its type, address, and state.</li>
<li>The OS indexes into the I/O device table to determine the device status and
to modify a table entry to include an interrupt.</li>
</ul>
</li>
</ol>
<h2 id="storage-definitions-and-notation-review"><a class="header" href="#storage-definitions-and-notation-review">Storage Definitions and Notation Review</a></h2>
<p>The basic unit of computer storage is a bit. A bit contains one of two values,
0 and 1. A byte is 8 bits, and on most computers is the smallest convenient chunck
of storage.</p>
<ul>
<li>A kilobyte, or KB, is \(1,024\) bytes</li>
<li>A megabyte, or MB, is \(1,024^2\) bytes</li>
<li>A gigabyte, or GB, is \(1,024^3\) bytes</li>
<li>A terabyte, or TB, is \(1,024^4\) bytes</li>
<li>A petabyte, or PB, is \(1,024^5\) bytes</li>
</ul>
<h2 id="direct-memory-access-structure"><a class="header" href="#direct-memory-access-structure">Direct Memory Access Structure</a></h2>
<p>This method is used for high-speed I/O devices able to transmit information at
close to memory speeds. Device controllers transfer blocks of data from buffer
storage directly to main memory without CPU intervention. This means only one
interrupt is generated per block rather than the one interrupt per byte.</p>
<h2 id="storage-structure"><a class="header" href="#storage-structure">Storage Structure</a></h2>
<ul>
<li><strong>Main memory</strong>: Only large storage media that the CPU can access directly.
<ul>
<li>Random access</li>
<li>Typically volatile</li>
</ul>
</li>
<li><strong>Secondary storage</strong>: An extension of main memory that provides large non-volatile
storage capacity.</li>
<li><strong>Magnetic discs</strong>: Rigid metal or glass platters covered with magnetic recording
material. The disk surface is logically divided into tracks which are sub-diveded
into sectors. The disk controller determines the logical interaction between the
device and the computer.</li>
<li><strong>Solid-state disks</strong>: Achieves faster speeds than magnetic disks and non-volatile
storage capacity through various technologies.</li>
</ul>
<h2 id="storage-hierarchy"><a class="header" href="#storage-hierarchy">Storage Hierarchy</a></h2>
<p>Storages systems are organised into a hierarchy:</p>
<ul>
<li>Speeds</li>
<li>Cost</li>
<li>Volatility.</li>
</ul>
<p>There is a device driver for each device controller used to manage I/O. They provide
uniform interfaces between controllers and the kernel.</p>
<h2 id="caching"><a class="header" href="#caching">Caching</a></h2>
<p>Caching allows information to be copied into a faster storage system. The main memory
can be viewed as a cache for the secondary storage.</p>
<p>Faster storage (cache) is checked first to determine if the information is there:</p>
<ul>
<li>If so, information is used directly from the cache</li>
<li>If not, data is copied to the cache and used there</li>
</ul>
<p>The cache is usually smaller and more expensive that the storage being cached. 
This means cache management is an important design problem.</p>
<h2 id="computer-system-architecture"><a class="header" href="#computer-system-architecture">Computer-System Architecture</a></h2>
<p>Most systems use a single general-purpose processor. However, most systems have
special-purpose processors as well.</p>
<p>Multi-processor systems, also known as parallel systems or tightly-coupled systems, 
usually come in two types; Asymmetric Multi-processing or Symmetric Multi-processor.
Multi-processor systems have a few advantages over a single general-purpose processor:</p>
<ul>
<li>Increase throughput</li>
<li>Economy of scale</li>
<li>Increased reliability, i.e. graceful degradation or fault tolerance</li>
</ul>
<h2 id="clustered-systems"><a class="header" href="#clustered-systems">Clustered Systems</a></h2>
<p>Clustered systems are like Multi-processor systems, they have multiple systems
working together.</p>
<ul>
<li>These systems typically share storage via a storage-area network (SAN).</li>
<li>Provide a high-availability service which survices failures:
<ul>
<li>Asymmetric clustering have one machine in hot-standby mode.</li>
<li>Symmetric clustering have multiple nodes running applications, monitoring 
each other.</li>
</ul>
</li>
<li>Some clusters are for high-performance computing (HPC). Applications running on 
these clusters must be written to use parallelisation.</li>
<li>Some have a distributed lock manager (DLM) to avoid conflicting operations.</li>
</ul>
<h2 id="operating-system-structure"><a class="header" href="#operating-system-structure">Operating System Structure</a></h2>
<p>Multi-programming organises jobs (code and data) so the CPU always has one to 
execute. This is needed for efficiency as a single user cannot keep a CPU and I/O
devices busy at all times. Multi-programming works by keeping a subset of total
jobs in the system, in memory. One job is selected and run via job scheduling.
When it has to wait (for I/O for example), the OS will switch to another job.</p>
<p>Timesharing is a logical extension in which the CPU switches jobs so frequently
that users can interact with each job while it is running.</p>
<ul>
<li>The response time should be less than one second.</li>
<li>Each user has at least one program executing in memory (process).</li>
<li>If processes don't fit in memory, swapping moves them in and out to run.</li>
<li>Virtual memory allows execution of processes not completely in memory.</li>
<li>If several jobs are ready to run at the same time, the CPU scheduler handles 
which to run.</li>
</ul>
<h2 id="operating-system-operations"><a class="header" href="#operating-system-operations">Operating-System Operations</a></h2>
<p>Dual-mode operations (user mode and kernel mode) allow the OS to protect itself 
and other system components. A mode bit provided by the hardware provides the ability
to distinguish when a system is running user code or kernel code. Some instructions 
are designated as privileged and are only executable in kernel mode. System calls
are used to change the mode to kernel, a return from call resets the mode back to 
user.</p>
<p>Most CPUs also support multi-mode operations, i.e. virtual machine manages (VMM)
mode for guest VMs.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="input-and-output"><a class="header" href="#input-and-output">Input and Output</a></h1>
<h2 id="printf"><a class="header" href="#printf"><code>printf()</code></a></h2>
<p><code>printf()</code> is an output function included in <code>stdio.h</code>. It outputs a character
stream to the standard output file, also known as <code>stdout</code>, which is normally
connected to the screen.</p>
<p>It takes 1 or more arguments with the first being called the control string.</p>
<p>Format specifications can be used to interpolate values within the string. A
format specification is a string that begins with <code>%</code> and ends with a conversion
character. In the above example, the format specifications <code>%s</code> and <code>%d</code> were used. 
Characters in the control string that are not part of a format specification are 
placed directly in the output stream; characters in the control string that are 
format specifications are replaced with the value of the corresponding argument.</p>
<p><strong>Example 1: Output with <code>printf()</code></strong></p>
<pre><code class="language-c">printf(&quot;name: %s, age: %d\n&quot;, &quot;John&quot;, 24); // &quot;name: John, age: 24&quot;
</code></pre>
<h2 id="scanf"><a class="header" href="#scanf"><code>scanf()</code></a></h2>
<p><code>scanf()</code> is an input function included in <code>stdio.h</code>. It reads a series of characters
from the standard input file, also known as <code>stdin</code>, which is normally connected
to the keyboard.</p>
<p>It takes 1 or more arguments with the first being called the control string.</p>
<p><strong>Example 2: Reading input with <code>scanf()</code></strong></p>
<pre><code class="language-c">char a, b, c, s[100];
int n;
double x;

scanf(&quot;%c%c%c%d%s%lf&quot;, &amp;a, &amp;b, &amp;c, &amp;n, n, &amp;x);
</code></pre>
<h2 id="relevant-links"><a class="header" href="#relevant-links">Relevant Links</a></h2>
<ul>
<li><a href="https://en.cppreference.com/w/c/io/fprintf">cppreference - printf</a></li>
<li><a href="https://en.cppreference.com/w/c/io/fscanf">cppreference - scanf</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="pointers"><a class="header" href="#pointers">Pointers</a></h1>
<p>A pointer is a variable used to store a memory address. They can be used to access
memory and manipulate an address.</p>
<p><strong>Example 1: Various ways of declaring a pointer</strong></p>
<pre><code class="language-c">// type *variable;

int *a;
int *b = 0;
int *c = NULL;
int *d = (int *) 1307;

int e = 3;
int *f = &amp;e; // `f` is a pointer to the memory address of `e`
</code></pre>
<p><strong>Example 2: Dereferencing pointers</strong></p>
<pre><code class="language-c">int a = 3;
int *b = &amp;a;

printf(&quot;Values: %d == %d\nAddresses: %p == %p\n&quot;, *b, a, b, &amp;a);
</code></pre>
<h2 id="relevant-links-1"><a class="header" href="#relevant-links-1">Relevant Links</a></h2>
<ul>
<li><a href="https://en.cppreference.com/w/cpp/language/pointer">cppreference - pointer</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="functions"><a class="header" href="#functions">Functions</a></h1>
<p>A function construct in C is used to write code that solves a (small) problem.
A procedural C program is made up of one or more functions, one of them being
<code>main()</code>. A C program will always begin execution with <code>main()</code>.</p>
<p>Function parameters can be passed into a function in one of two ways; pass by value
and pass by reference. When a parameter is passed in via value, the data for the
parameters are copied. This means any changes to said variables within the function
will not affect the original values passed in. Pass by reference on the other hand
passes in the memory address of each variable into the function. This means that
changes to the variables within the function will affect the original variables.</p>
<p><strong>Example 1: Function control</strong></p>
<pre><code class="language-c">#include &lt;stdio.h&gt;

void prn_message(const int k);

int main(void) {
    int n;

    printf(&quot;There is a message for you.\n&quot;);
    printf(&quot;How many times do you want to see it?\n&quot;);

    scanf(&quot;%d&quot;, &amp;n);

    prn_message(n);

    return 0;
}

void prn_message(const int k) {
    printf(&quot;Here is the message:\n&quot;);

    for (size_t i = 0; i &lt; k; i++) {
        printf(&quot;Have a nice day!\n&quot;);
    }
}
</code></pre>
<p><strong>Example 2: Pass by values</strong></p>
<pre><code class="language-c">#include &lt;stdio.h&gt;

void swapx(int a, int b);

int main(void) {
    int a = 10;
    int b = 20;

    // Pass by value
    swapx(a, b);

    printf(&quot;within caller - a: %d, b: %b\n&quot;, a, b); // &quot;within caller - a: 10, b: 20&quot;

    return 0;
}

void swapx(int a, int b) {
    int temp;

    temp = a;
    a = b;
    b = temp;

    printf(&quot;within function - a: %d, b: %b\n&quot;, a, b); // &quot;within function - a: 20, b: 10&quot;
}
</code></pre>
<p><strong>Example 3: Pass by value</strong></p>
<pre><code class="language-c">#include &lt;stdio.h&gt;

void swapx(int *a, int *b);

int main(void) {
    int a = 10;
    int b = 20;

    // Pass by reference
    swapx(&amp;a, &amp;b);

    printf(&quot;within caller - a: %d, b: %b\n&quot;, a, b); // &quot;within caller - a: 20, b: 10&quot;

    return 0;
}

void swapx(int *a, int *b) {
    int temp;

    temp = *a;
    *a = *b;
    *b = temp;

    printf(&quot;within function - a: %d, b: %b\n&quot;, *a, *b); // &quot;within function - a: 20, b: 10&quot;
}
</code></pre>
<p><strong>Example 4: Function pointers</strong></p>
<pre><code class="language-c">#include &lt;stdio.h&gt;

void function_a(int num) {
    printf(&quot;Function A: %d\n&quot;, num);
}

void function_b(int num) {
    printf(&quot;Function B: %d\n&quot;, num);
}

void caller(void (*function) (int)) {
    function(1);
    function(2);
    function(3);
}

int main(void) {
    caller(function_a);
    caller(function_b);

    return 0;
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="week-2"><a class="header" href="#week-2">Week 2</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="operating-system-structures"><a class="header" href="#operating-system-structures">Operating System Structures</a></h1>
<h2 id="operating-system-services"><a class="header" href="#operating-system-services">Operating System Services</a></h2>
<p>Operating systems provide an environment for execution of programs and services
to programs and users.</p>
<p>There are many operating system services that provide functions that are
helpful to the user such as:</p>
<ul>
<li><strong>User interface</strong>: Almost all operating systems have a user interface. This can
be in the form of a graphical user interface (GUI) or a command-line (CLI).</li>
<li><strong>Program execution</strong>: The system must be able to load a program into memory
and run that program, end execution, either normally or abnormally.</li>
<li><strong>I/O operations</strong>: A running program may require I/O, which may involve a file
or an I/O device.</li>
<li><strong>File-system manipulation</strong>: The file system is of particular interest. Programs
need to read and write files and directories, create and delete them, search
them, list file information, manage permissions, and more.</li>
<li><strong>Communication</strong>: Processors may exchange information, on the same computer
or between computers over a network.</li>
<li><strong>Error detection</strong>: OS needs to be constantly aware of possible errors:
<ul>
<li>May occur in the CPU and memory hardware, in I/O devices, in user programs,
and more.</li>
<li>For each type of error, the OS should take the appropriate action to ensure
correct and consistent computing.</li>
<li>Debugging facilities can greatly enhance the user's and programmer's abilities
to efficiently use the system.</li>
</ul>
</li>
</ul>
<p>Another set of OS functions exist for ensuring the efficient operation of the
system itself via resource sharing.</p>
<ul>
<li><strong>Resource allocation</strong>: When multiple users or multiple jobs are running
concurrently, resources must be allocated to each of them.</li>
<li><strong>Accounting</strong>: To keep track of which users use how much and what kinds of
resources.</li>
<li><strong>Protection and security</strong>: The owners of information stored in a multi-user
or networked computer system may want to control use of that information. Concurrent
processes should not interfere with each other.
<ul>
<li>Protection involves ensuring that all access to system resources is controlled.</li>
<li>Security of the system from outsiders requires user authentication. This also
extends to defending external I/O devices from invalid access attempts.</li>
<li>If a system is to be protected and secure, pre-cautions must be instituted
throughout it. A chain is only as strong as its weakest link.</li>
</ul>
</li>
</ul>
<h2 id="system-calls"><a class="header" href="#system-calls">System Calls</a></h2>
<p>System calls provide an interface to the services made available by an operating
system. These calls are generally written in higher-level languages such as
C and C++. These system calls however, are mostly accessed by programs via
a high-level application programming interface (API) rather than direct system
call use.</p>
<p>The three most common APIs are Win32 API for Windows, POSIX API for POSIX-based systems,
and JAVA API for the Java virtual machine (JVM)</p>
<p><img src="week_2/../media/standard_api_example.png" alt="Figure: Example of standard api." /></p>
<p>Typically, a number is associated with each system call. The system-call
interface maintains a table indexed according to these numbers. The system call
interface invokes the intended system call in the OS kernel and returns a status
of the systema call and any return values. The caller needs to know nothing about
how the system call is implemented, it just needs to obey the API and understand
what the OS will do as a result call.</p>
<p><img src="week_2/../media/open_system_call_example.png" alt="Figure: The handling of a user application invoking the open() system call." />
<strong>Figure: The handling of a user application invoking the <code>open()</code> system call.</strong></p>
<p>There are many types of system calls:</p>
<ul>
<li>Process control</li>
<li>File management</li>
<li>Device management</li>
<li>Information maintenance</li>
<li>Communications</li>
<li>Protection</li>
</ul>
<p>Often, more information is required than simply the identity of the system call.
There are three general methods used to pass parameters to the OS:</p>
<ol>
<li>Pass parameters into registers. This won't always work however as there may
be more parameters than registers.</li>
<li>Store parameters in a block, or table, in memory, and pass the address of 
the block as a parameter in a register.</li>
<li>Parameters are placed, or pushed, onto the stack by the program and popped
off the stack by the operating system. This method does not limit the number
length of the parameters being passed.</li>
</ol>
<p><img src="week_2/../media/pass_params_as_table.png" alt="Figure: Passing of parameters as a table." />
<strong>Figure: Passing of parameters as a table.</strong></p>
<h2 id="system-programs"><a class="header" href="#system-programs">System Programs</a></h2>
<p>System programs provide a convenient environment for program development and 
execution. They can be generally divided into:</p>
<ul>
<li>File manipulation</li>
<li>Status information sometimes stored in a file modification</li>
<li>Programming language support</li>
<li>Program loading and execution</li>
<li>Communications</li>
<li>Background services</li>
<li>Application programs</li>
</ul>
<h2 id="unix"><a class="header" href="#unix">UNIX</a></h2>
<p>UNIX is limited by hardware functionality. The original UNIX operating system
had limited structing. The UNIX OS consists of two separable parts:</p>
<ol>
<li>Systems programs</li>
<li>The kernel:
<ul>
<li>Consists of everything below the system-call interface and above the
physical hardware.</li>
<li>Provides the file system, CPU scheduling, memory management, and other
operating-system functions.</li>
</ul>
</li>
</ol>
<h2 id="operating-system-structure-1"><a class="header" href="#operating-system-structure-1">Operating System Structure</a></h2>
<p>There are a few ways to organise an operating system.</p>
<h3 id="layered"><a class="header" href="#layered">Layered</a></h3>
<p>The operating system is divided into a number of layers, each built on top
of the lower layers. The bottom layer (layer 0), is the hardware; the highest
is the user interface.</p>
<p>Due to the modularity, layers are selected such that each uses functions and 
services of only lower-level layers.</p>
<p><img src="week_2/../media/layered_os.png" alt="Figure: A layered operating system." />
<strong>Figure: A layered operating system.</strong></p>
<h3 id="microkernel-system"><a class="header" href="#microkernel-system">Microkernel System</a></h3>
<p>In this organisation method, as much as possible is moved from the kernel into 
user space. An example OS that uses a microkernel is Mach, which parts of the
MacOSX kernel (Darwin) is based upon. Communication takes place between user 
modules via message passing.</p>
<div class="table-wrapper"><table><thead><tr><th>Advantages</th><th>Disadvantages</th></tr></thead><tbody>
<tr><td>Easier to extend a microkernel</td><td>Performance overhead of user space to kernel space communication</td></tr>
<tr><td>Easier to port the operating system to new architectures</td><td></td></tr>
<tr><td>More reliable (less code is running in kernel mode)</td><td></td></tr>
<tr><td>More secure</td><td></td></tr>
</tbody></table>
</div>
<p><img src="week_2/../media/microkernel_os.png" alt="Figure: Architecture of a typical microkernel." />
<strong>Figure: Architecture of a typical microkernel.</strong></p>
<h3 id="hybrid-system"><a class="header" href="#hybrid-system">Hybrid System</a></h3>
<p>Most modern operating systems don't use a single model but a use concepts from
a variety. Hybrid systems combine multiple approaches to address performance, security,
and usability needs.</p>
<p>For example, Linux is monolithic, because having the operating system in a 
single address space provides very efficient performance. However, it's also 
modular, so that new functionality can be dynamically added to the kernel.</p>
<h2 id="modules"><a class="header" href="#modules">Modules</a></h2>
<p>Most modern operating systems implement loadable kernel modules (LKMs).
Here, the kernel has a set of core components and can link in additional 
services via modules, either at boot time or during run time</p>
<p>Each core component is separate, can talk to others via known interfaces,
and is loadable as needed within the kernel.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="arrays"><a class="header" href="#arrays">Arrays</a></h1>
<p>An array is a contiguous sequence of data items of the same type.
An array name is an address, or constant pointer value, to the first element 
in said array.</p>
<p>Aggregate operations on an array are not valid in C, this means that you cannot
assign an array to another array. To copy an array you must either copy it component-wise 
(typically via a loop) or via the <code>memcpy()</code> function in <code>string.h</code>.</p>
<p><strong>Example 1: Arrays in practice</strong></p>
<pre><code class="language-c">#include &lt;stdio.h&gt;

const int N = 5;

int main(void) {
    // Allocate space for a[0] to a[4]
    int a[N];
    int i;
    int sum = 0;

    // Fill the array
    for (i = 0; i &lt; N; i++) {
        a[i] = 7 + i * i;
    }

    // Print the array
    for (i = 0; i &lt; N; i++) {
        printf(&quot;a[%d] = %d\n&quot;, i, a[i]);
    }

    // Sum the elements
    for (i = 0; i &lt; N; i++) {
        sum += a[i];
    }

    printf(&quot;\nsum = %d\n&quot;, sum);

    return 0;
}
</code></pre>
<p><strong>Example 2: Arrays and Pointers</strong></p>
<pre><code class="language-c">#include &lt;stdio.h&gt;

const int N = 5;

int main(void) {
    int a[N];
    int sum;
    int *p;

    // The following two calls are the same
    p = a;
    p = &amp;a[0];

    // The following two calls are the same
    p = a + 1;
    p = &amp;a[1];
    

    // Version 1
    sum = 0;

    for (int i = 0; i &lt; N; i++) {
        sum += a[i];
    }
    
    // Version 2
    sum = 0;

    for (int i = 0; i &lt; N; i++) {
        sum += *(a + i);
    }
}
</code></pre>
<p><strong>Example 3: Bubble Sort</strong></p>
<pre><code class="language-c">#include &lt;stdio.h&gt;

void swap(int *arr, int i, int j);
void bubble_sort(int *arr, int n);

void main(void) {
    int arr[] = { 5, 1, 4, 2, 8 };
    int N = sizeof(arr) / sizeof(int);

    bubble_sort(arr, N);

    for (int i = 0; i &lt; N; i++) {
        printf(&quot;%d: %d\n&quot;, i, arr[i]);
    }

    return 0;
}

void swap(int *arr, int i, int j) {
    int temp = arr[i];
    arr[i] = arr[j];
    arr[j] = temp;
}

void bubble_sort(int *arr, int n) {
    for (int i = 0; i &lt; n - 1; i++) {
        for (int j = 0; j &lt; n - 1; j++) {
            if (arr[j] &gt; arr[j + 1]) {
                swap(arr, j, j + 1);
            }
        }
    }
}
</code></pre>
<p><strong>Example 4: Copying an Array</strong></p>
<pre><code class="language-c">#include &lt;stdio.h&gt;
#include &lt;string.h&gt;

int main(void) {
    // Copying an array component-wise
    int array_one[5] = { 1, 2, 3, 4, 5 };
    int array_two[5];

    for (int idx = 0; idx &lt; 5; idx++) {
        array_two[idx] = array_one[idx];
    }

    // Copying an array via memcpy
    memcpy(array_two, array_one, sizeof(int) * 5);
}
</code></pre>
<h2 id="relevant-links-2"><a class="header" href="#relevant-links-2">Relevant Links</a></h2>
<ul>
<li><a href="https://en.cppreference.com/w/cpp/container/array">cppreference - array</a></li>
<li><a href="https://en.cppreference.com/w/cpp/string/byte/memcpy">cppreference - memcpy</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="strings"><a class="header" href="#strings">Strings</a></h1>
<p>A string is a one-dimensional array of type <code>char</code>. All strings must end with a
null character <code>\0</code> which is a byte used to represent the end of a string.</p>
<p>A character in a string can
be accessed either by an element in an array of by making use of a pointer.</p>
<p><strong>Example 1: Strings in practice</strong></p>
<pre><code class="language-c">char *first = &quot;john&quot;;
char last[6];

last[0] = 's';
last[1] = 'm';
last[2] = 'i';
last[3] = 't';
last[4] = 'h';
last[5] = '\0';

printf(&quot;Name: %s, len: %lu&quot;, first, strlen(first));
</code></pre>
<h2 id="relevant-links-3"><a class="header" href="#relevant-links-3">Relevant Links</a></h2>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Null-terminated_string">Wikipedia - Null-terminated string</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="structures"><a class="header" href="#structures">Structures</a></h1>
<p>Structures are named collections of data which are able to be of varying types.</p>
<p><strong>Example 1: Structures in practice</strong></p>
<pre><code class="language-c">struct student {
    char *last_name;
    int student_id;
    char grade;
};

// By using `typedef` we can avoid prefixing the type with `struct`
typedef struct unit {
    char *code;
    char *name;
} unit;

void update_student(struct student *student);
void update_grade(unit *unit);

int main(void) {
    struct student s1 = {
        .last_name = &quot;smith&quot;,
        .student_id = 119493029,
        .grade = 'B',
    };

    s1.grade = 'A';

    update_student(&amp;s1);


    unit new_unit;

    new_unit.name = &quot;Microprocessors and Digital Systems&quot;;

    update_unit(&amp;new_unit);
}

void update_student(struct student *student) {
    // `-&gt;` shorthand for dereference of struct
    student-&gt;last_name = &quot;doe&quot;;
    student-&gt;grade = 'C';
}

void update_unit(unit *unit) {
    // `-&gt;` shorthand for dereference of struct
    unit-&gt;code = &quot;CAB403&quot;;
    unit-&gt;name = &quot;Systems Programming&quot;;
}
</code></pre>
<h2 id="relevant-links-4"><a class="header" href="#relevant-links-4">Relevant Links</a></h2>
<ul>
<li><a href="https://en.cppreference.com/w/c/language/struct">cppreference - Struct declaration</a></li>
<li><a href="https://en.cppreference.com/w/cpp/language/typedef">cppreference - typedef specifier</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="dynamic-memory-management"><a class="header" href="#dynamic-memory-management">Dynamic Memory Management</a></h1>
<p>Memory in a C program can be divided into four categories:</p>
<ol>
<li>Code memory</li>
<li>Static data memory</li>
<li>Runtime stack memory</li>
<li>Heap memory</li>
</ol>
<h2 id="code-memory"><a class="header" href="#code-memory">Code Memory</a></h2>
<p>Code memory is used to store machine instructions. As a program runs,
machine instructions are read from memory and executed.</p>
<h2 id="static-data-memory"><a class="header" href="#static-data-memory">Static Data Memory</a></h2>
<p>Static data memory is used to store static data. There are two categories of
static data: global and static variables.</p>
<p>Global variables are variables defined outside the scope of any function as 
can be seen in example 1. Static variables on the other hand are defined with
the <code>static</code> modifier as seen in example 2.</p>
<p>Both global and static variables have one value attached to them; they are
assigned memory once; and they are initialised before <code>main</code> begins execution
and will continue to exist until the end of execution.</p>
<p><strong>Example 1: Global variables.</strong></p>
<pre><code class="language-c">int counter = 0;

int increment(void) {
    counter++;

    return counter;
}
</code></pre>
<p><strong>Example 2: Static variables.</strong></p>
<pre><code class="language-c">int increment(void) {
    // will be initialised once
    static int counter = 0;

    // increments every time the function is called
    counter++;

    return counter;
}
</code></pre>
<h2 id="runtime-stack-memory"><a class="header" href="#runtime-stack-memory">Runtime Stack Memory</a></h2>
<p>Runtime stack memory is used by function calls and is FILO (First in, Last out).
When a function is invoked, a block of memory is allocated by the runtime 
stack to store the information about the function call. This block of memory 
is termed as an <em>Activation Record</em>.</p>
<p>The information about the function call includes:</p>
<ul>
<li>Return address.</li>
<li>Internal registers and other machine-specific information.</li>
<li>Parameters.</li>
<li>Local variables.</li>
</ul>
<h2 id="heap-memory"><a class="header" href="#heap-memory">Heap Memory</a></h2>
<p>Heap memory is memory that is allocated during the runtime of the program.
On many systems, the heap is allocated in an opposite direction to the stack
and grows towards the stack as more is allocated. On simple systems without 
memory protection, this can cause the heap and stack to collide if too much
memory is allocated to either one.</p>
<p>To deal with this, C provides two functions in the standard library to handle
dynamic memory allocation; <code>calloc()</code> (contiguous allocation) and <code>malloc()</code>
(memory allocation).</p>
<p><code>void *calloc(size_t n, size_t s)</code> returns a pointer to enough space in memory
to store <code>n</code> objects, each of <code>s</code> bytes. The storage set aside is automatically
initialised to zero.</p>
<p><code>void *malloc(size_t s)</code> returns a pointer to a space of size <code>s</code> and leaves the
memory uninitialised.</p>
<p><strong>Example 3: <code>malloc()</code> and <code>calloc()</code></strong></p>
<pre><code class="language-c">#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;

int main() {
    int num_of_elements;
    int *ptr;
    int sum = 0;
  
    printf(&quot;Enter number of elements: &quot;);
    scanf(&quot;%d&quot;, &amp;num_of_elements);
  
    ptr = malloc(num_of_elements * sizeof(int));
    // or
    // ptr = calloc(num_of_elements, sizeof(int));
   
    if (ptr == NULL) {
        printf(&quot;[Error] - Memory was unable to be allocated.&quot;);

        exit(0);
    }
  
    printf(&quot;Enter elements: &quot;);

    for (int i = 0; i &lt; n; i++) {
        scanf(&quot;%d&quot;, ptr + i);

        sum += *(ptr + i);
    }
  
    printf(&quot;Sum = %d&quot;, sum);
    
    free(ptr);
  
    return 0;
}
</code></pre>
<h2 id="relevant-links-5"><a class="header" href="#relevant-links-5">Relevant Links</a></h2>
<ul>
<li><a href="https://en.cppreference.com/w/c/memory/malloc">cppreference - malloc</a></li>
<li><a href="https://en.cppreference.com/w/c/memory/calloc">cppreference - calloc</a></li>
<li><a href="https://en.cppreference.com/w/c/memory/realloc">cppreference - realloc</a></li>
<li><a href="https://en.cppreference.com/w/c/memory/free">cppreference - free</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="week-3"><a class="header" href="#week-3">Week 3</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="processes"><a class="header" href="#processes">Processes</a></h1>
<p>An operating system executes a variety of programes either via:</p>
<ul>
<li>Batch systems (jobs)</li>
<li>or Time-shared systems (user programs or tasks)</li>
</ul>
<p>A process, sometimes referred to as a job, is simply a program in execution.
The status of the current activity of a process is represented by the value 
of the program counter and the contents of the processor’s registers.</p>
<p>A process is made up of multiple parts:</p>
<ul>
<li><strong>Text section</strong>: The executable code</li>
<li><strong>Data section</strong>: Global variables</li>
<li><strong>Heap section</strong>: Memory that is dynamically allocated during program run
time</li>
<li><strong>Stack section</strong>: Temporary data storage when invoking functions 
(such as function parameters, return addresses, and local variables)</li>
</ul>
<p>It's important to note that a program itself is not a process but rather a 
passive entity. In contrast, a process is an active entity, with a program 
counter specifying the next instruction to execute and a set of associated 
resources.</p>
<p>As a process executes, it changes state. A process may be in one of the following 
states:</p>
<ul>
<li><strong>new</strong>: The process is being created.</li>
<li><strong>running</strong>: Instructions are being executed.</li>
<li><strong>waiting</strong>: The process is waiting for some event to occur.</li>
<li><strong>ready</strong>: The process is waiting to be assigned to a processor.</li>
<li><strong>terminated</strong>: The process has finished execution.</li>
</ul>
<p><img src="week_3/../media/process_state.png" alt="Figure: Diagram of process state." />
<strong>Figure: Diagram of process state.</strong></p>
<h2 id="process-control-block-pcb"><a class="header" href="#process-control-block-pcb">Process Control Block (PCB)</a></h2>
<p>Each process is represented in the OS by a process control block, also known as
a task control block. It contains information associated with a specific process
such as:</p>
<ul>
<li><strong>Process state</strong>: The state of the process.</li>
<li><strong>Program counter</strong>: The address of the next instruction to be executed for 
this process.</li>
<li><strong>CPU registers</strong>: The contents of all process-centric registers. Along with 
the program counter, this state information must be saved when an interrupt 
occurs, to allow the process to be continued correctly afterward when it is 
rescheduled to run.</li>
<li><strong>CPU scheduling information</strong>: Information about process priority, pointers 
to scheduling queues, and any other scheduling parameters.</li>
<li><strong>Memory-management information</strong>: This information may include such items 
as the value of the base and limit registers and the page tables, or the 
segment tables, depending on the memory system used by the operating system.</li>
<li><strong>Accounting information</strong>: This information includes the amount of CPU and 
real time used, time limits, account numbers, job or process numbers, etc..</li>
<li><strong>I/O status information</strong>: This information includes the list of I/O 
devices allocated to the process, a list of open files, etc..</li>
</ul>
<h2 id="threads"><a class="header" href="#threads">Threads</a></h2>
<p>In a single-threaded model, only a single thread of instructions can be executed.
This means only a single tasks can be completed at any given time. For example,
in a word document, the user cannot simultaneously type in characters and run 
the spell checker.</p>
<p>In most modern operating systems however, the use of multiple threads allows more
than one task to be performed at any given moment. A multithreaded word 
processor could, for example, assign one thread to manage user input while 
another thread runs the spell checker. </p>
<p>In a multi-threaded system, the PCB is expanded to include information for 
each thread.</p>
<h2 id="process-scheduling"><a class="header" href="#process-scheduling">Process Scheduling</a></h2>
<p>The objective of multi-programming is to have some process running at all times 
so as to maximize CPU utilization. A process scheduler is used to determine which
process should be executed. The number of processes currently in memory is known 
as the degree of multiprogramming</p>
<p>When a process enters the system, it's put into a <strong>ready queue</strong> where it then waits
to be executed. When a process is allocated a CPU core for execution it executes 
for a while and eventually terminates, is interrupted, or waits for the 
occurrence of a particular event. Any process waiting for an event to occur gets
placed into a <strong>wait queue</strong>.</p>
<p><img src="week_3/../media/process_queue.png" alt="Figure: Queueing-diagram representation of process scheduling." />
<strong>Figure: Queueing-diagram representation of process scheduling.</strong></p>
<p>Most processos can be described as either:</p>
<ul>
<li><strong>I/O bound</strong>: A I/O bound process that spends more of its time doing I/O operations.</li>
<li><strong>CPU bound</strong>: Spends more of its time doing more calculations with infrequent
I/O requests.</li>
</ul>
<h2 id="context-switch"><a class="header" href="#context-switch">Context Switch</a></h2>
<p>Interrupts cause the operating system to change a CPU core from its current task
and to run a kernel routine. These operations happen frequently so it's important
to ensure that when returning to the process, no information was lost.</p>
<p>Switching the CPU core to another process requires performing a state save of 
the current process and a state restore of a different process. This task is 
known as a context switch. When a context switch occurs, the kernel saves the 
context of the old process in its PCB and loads the saved context of the new 
process scheduled to run.</p>
<p>The time between a context switch is considered as overhead as no useful work is
done while switching. The more complex the OS and PCB, the longer it takes to
context switch.</p>
<h2 id="process-creation"><a class="header" href="#process-creation">Process Creation</a></h2>
<p>During execution, a process may need to create more processes. The creating 
process is called a parent process, and the new processes are called the 
children of that process. Each of these new processes may in turn create other 
processes, forming a tree of processes. Processes are identified by their process 
identifier (PID).</p>
<p>When a process is created, it will generally require some amount of resources to
accomplish its task. A child process may be able to obtain its resources directly 
from the operating system, or it may be constrained to a subset of the resources 
of the parent process.</p>
<p>When a process creates a new process, two possibilities for execution exist:</p>
<ol>
<li>The parent continues to execute concurrently with its children.</li>
<li>The parent waits until some or all of its children have terminated.</li>
</ol>
<p>There are also two address-space possibilities for the new process:</p>
<ol>
<li>The child process is a duplicate of the parent process (it has the same program 
and data as the parent).</li>
<li>The child process has a new program loaded into it.</li>
</ol>
<p>A new process is created by the <code>fork()</code> system call. The new process consists 
of a copy of the address space of the original process. The return code for the 
<code>fork()</code> is zero for the new (child) process, whereas the (nonzero) process identifier 
of the child is returned to the parent.</p>
<p>Once forked, it's typical for <code>exec()</code> to be called on one of the two processes.
The <code>exec()</code> system call loads a binary file into memory (destroying the memory 
image of the program containing the exec() system call) and starts its execution.</p>
<p>For example, this code forks a new process and, using <code>execlp()</code>, a version of 
the <code>exec(</code>) system call, overlays the process address space with the UNIX command 
<code>/bin/ls</code> (used to get a directory listing).</p>
<pre><code class="language-c">#include &lt;sys/types.h&gt;
#include &lt;sys/wait.h&gt;
#include &lt;stdio.h&gt;
#include &lt;unistd.h&gt;

int main() {
    pid_t pid;
    /* fork a child process */
    pid = fork();

    if (pid &lt; 0) { /* error occurred */
        fprintf(stderr, &quot;Fork failed\n&quot;);
    
        return 1;
    } else if (pid == 0) { /* child process */
        execlp(&quot;/bin/ls&quot;, &quot;ls&quot;, NULL);
    } else { /* parent process */
        /* parent will wait for the child to complete */
        wait(NULL);

        printf(&quot;Child complete\n&quot;);
    }

    return 0;
}
</code></pre>
<p><img src="week_3/../media/fork_system_call.png" alt="Figure: Process creation using the fork() system call." />
<strong>Figure: Process creation using the fork() system call.</strong></p>
<h2 id="process-termination"><a class="header" href="#process-termination">Process Termination</a></h2>
<p>A process terminates when it finishes executing its final statement and asks the 
operating system to delete it by using the <code>exit()</code> system call. At that point, 
the process may return a status value (typically an integer) to its waiting parent 
process (via the <code>wait()</code> system call).</p>
<p>A parent may terminate the execution of one of its children for a variety of 
reasons, such as:</p>
<ul>
<li>The child has exceeded its usage of some of the resources that it has been 
allocated.</li>
<li>The task assigned to the child is no longer required.</li>
<li>The parent is exiting, and the operating system does not allow a child to
continue if its parent terminates.</li>
</ul>
<p>A parent process may wait for the termination of a child process by using the 
<code>wait()</code> system call. The <code>wait()</code> system call is passed a parameter that allows 
the parent to obtain the exit status of the child. This system call also returns 
the process identifier of the terminated child so that the parent can tell which 
of its children has terminated:</p>
<pre><code class="language-c">pid t pid; 
int status;

pid = wait(&amp;status);
</code></pre>
<p>When a process terminates, its resources are deallocated by the operating system. 
However, its entry in the process table must remain there until the parent calls 
<code>wait()</code>, because the process table contains the process’s exit status. </p>
<p>If a child process is terminated but the parent has not called <code>wait()</code>, the process
is known as a zombie process. If a parent is terminated before calling <code>wait()</code>, the process
is know as an orphan.</p>
<h2 id="interprocess-communication"><a class="header" href="#interprocess-communication">Interprocess Communication</a></h2>
<p>Processes within a system may be independent or cooperating. A process is cooperating 
if it can affect or be affected by the other processes executing in the system.</p>
<p>There are a variety of reasons for providing an environment that allows process 
cooperation:</p>
<ul>
<li>Information sharing</li>
<li>Computational speedup</li>
<li>Modularity</li>
<li>Convenience</li>
</ul>
<p>Cooperating processes require an interprocess communication (IPC) mechanism that 
will allow them to exchange data. There are two fundamental models of interprocess 
communication: shared memory and message passing.</p>
<p><img src="week_3/../media/communication_models.png" alt="Figure: Communications models. (a) Shared memory. (b) Message passing." />
<strong>Figure: Communications models. (a) Shared memory. (b) Message passing.</strong></p>
<p>In the shared-memory model, a region of memory that is shared by the cooperating 
processes is established. Processes can then exchange information by reading and 
writing data to the shared region. In the message-passing model, communication 
takes place by means of messages exchanged between the cooperating processes.</p>
<h2 id="producer-consumer-problem"><a class="header" href="#producer-consumer-problem">Producer-Consumer Problem</a></h2>
<p>The Producer-Consumer problem is a common paradigm for cooperating processes.
A producer process produces information that is consumed by a consumer process.</p>
<p>One solution to the producer–consumer problem uses shared memory. To allow producer 
and consumer processes to run concurrently, we must have available a buffer of 
items that can be filled by the producer and emptied by the consumer. This buffer 
will reside in a region of memory that is shared by the producer and consumer 
processes.</p>
<p>Two types of buffers can be used. The <strong>unbounded buffer</strong> places no practical limit 
on the size of the buffer. The consumer may have to wait for new items, but the 
producer can always produce new items. The <strong>bounded buffer</strong> assumes a fixed buffer 
size. In this case, the consumer must wait if the buffer is empty, and the producer 
must wait if the buffer is full.</p>
<h2 id="message-passing"><a class="header" href="#message-passing">Message Passing</a></h2>
<p>Message passing provides a mechanism to allow processes to communicate and to 
synchronize their actions without sharing the same address space.</p>
<p>A message-passing facility provides at least two operations:</p>
<ol>
<li><code>send(message)</code></li>
<li><code>receive(message)</code></li>
</ol>
<p>Before two processes can communicate, they first need to establish a communication
link. </p>
<p>This could be via physical hardware:</p>
<ul>
<li>Shared memory.</li>
<li>Hardware bus.</li>
</ul>
<p>or logical:</p>
<ul>
<li>Direct or indirect communication.</li>
<li>Synchronous or asynchronous communication.</li>
<li>Automatic or explicit buffering.</li>
</ul>
<h2 id="direct-communication"><a class="header" href="#direct-communication">Direct Communication</a></h2>
<p>Under direct communication, each process that wants to communicate must explicitly 
name the recipient or sender of the communication.</p>
<ul>
<li><code>send(P, message)</code> - send a message to process P.</li>
<li><code>receive(Q, message)</code> - receive a message from process Q.</li>
</ul>
<p>A communication link in this scheme has the following properties:</p>
<ul>
<li>A link is established automatically. </li>
<li>The processes need to know only each other’s identity to communicate.</li>
<li>A link is associated with exactly two processes.</li>
<li>Between each pair of processes, there exists exactly one link.</li>
</ul>
<h2 id="indirect-communication"><a class="header" href="#indirect-communication">Indirect Communication</a></h2>
<p>With indirect communication, the messages are sent to and received from mailboxes, 
or ports. A mailbox can be viewed abstractly as an object into which messages can 
be placed by processes and from which messages can be removed. Each mailbox has 
a unique identification.</p>
<ul>
<li><code>send(A, message)</code> — Send a message to mailbox A.</li>
<li><code>receive(A, message)</code> — Receive a message from mailbox A.</li>
</ul>
<p>The operating system then must provide a mechanism that allows a process to do 
the following:</p>
<ul>
<li>Create a new mail box.</li>
<li>Send and receive messages through the mailbox.</li>
<li>Delete a mail box.</li>
</ul>
<p>In this scheme, a communication link has the following properties:</p>
<ul>
<li>A link is established between a pair of processes only if both members of the 
pair have a shared mailbox.</li>
<li>A link may be associated with more than two processes.</li>
<li>Between each pair of communicating processes, a number of different links
may exist, with each link corresponding to one mailbox.</li>
</ul>
<p>Now suppose that processes \(P_1\), \(P_2\), and \(P_3\) all share mailbox A. 
Process \(P_1\) sends a message to A, while both \(P_2\) and \(P_3\) execute 
a <code>receive()</code> from A. Which process will receive the message sent by \(P_3\)? 
The answer depends on which of the following methods we choose:</p>
<ul>
<li>Allow a link to be associated with at most two processes</li>
<li>Allow only one process at a time to execute a receive operation</li>
<li>Allow the system to select arbitrarily the receiver. Sender is notified who
the receiver was.</li>
</ul>
<h2 id="synchronisation"><a class="header" href="#synchronisation">Synchronisation</a></h2>
<p>Communication between processes takes place through calls to <code>send()</code> and <code>receive()</code> 
primitives. Message passing may be either blocking or nonblocking also known as 
synchronous and asynchronous.</p>
<ul>
<li><strong>Blocking send</strong>: The sending process is blocked until the message is received 
by the receiving process or by the mailbox.</li>
<li><strong>Nonblocking send</strong>: The sending process sends the message and resumes operation.</li>
<li><strong>Blocking receive</strong>: The receiver blocks until a message is available.</li>
<li><strong>Nonblocking receive</strong>: The receiver retrieves either a valid message or a null.</li>
</ul>
<p>Different combinations of <code>send()</code> and <code>receive()</code> are possible. When both <code>send()</code> 
and <code>receive()</code> are blocking, we have a rendezvous between the sender and the 
receiver.</p>
<h2 id="buffering"><a class="header" href="#buffering">Buffering</a></h2>
<p>Whether communication is direct or indirect, messages exchanged by communicating 
processes reside in a temporary queue. These queues can be implemented in three 
ways:</p>
<ol>
<li><strong>Zero capacity</strong>: The queue has a maximum length of zero; thus, the link cannot 
have any messages waiting in it. In this case, the sender must block until the 
recipient receives the message.</li>
<li><strong>Bounded capacity</strong>: The queue has finite length \(n\); thus, at most \(n\) 
messages can reside in it. If the queue is not full when a new message is sent, 
the message is placed in the queue (either the message is copied or a pointer to 
the message is kept), and the sender can continue execution without waiting. The 
link’s capacity is finite, however. If the link is full, the sender must block 
until space is available in the queue.</li>
<li><strong>Unbounded capacity</strong>: The queue’s length is potentially infinite; thus, any 
number of messages can wait in it. The sender never blocks.</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="week-4"><a class="header" href="#week-4">Week 4</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="threads-1"><a class="header" href="#threads-1">Threads</a></h1>
<p>A thread is a basic unit of CPU utilisation. A thread consists of:</p>
<ul>
<li>A thread ID</li>
<li>A program counter PC</li>
<li>A register set </li>
<li>A stack</li>
</ul>
<p>A thread shares it's code section, data section, and other operating-system resources
with others threads within the same process. A traditional process usually consists
of a single thread of control, this is called a single-threaded process. A process
with multiple threads of control can therefore perform more than one task at
any given moment, this is called a multi-threaded process.</p>
<p><img src="week_4/../media/single-vs-multi-threaded.png" alt="Figure: Single-threaded and multithreaded processes." />
<strong>Figure: Single-threaded and multithreaded processes.</strong></p>
<p>Most programs that run on modern computers and mobile devices are multithreaded.
For example, A word processor may have a thread for displaying graphics, another 
thread for responding to keystrokes from the user, and a third thread for performing 
spelling and grammar checking in the background.</p>
<p>In certain situations, a single application may be required to perform several 
tasks at any one time. For example, a web server needs to accept many client requests
concurently. A solution to this is to have the server run a single process that 
accepts requests. When a request is recieved, a new process is created to service
the request. Before threads became popular, this was the most common way to handle
such situation.</p>
<p>The problem with this however is that processes are expensive to create. If the
new process will perform the same tasks as the existing process, why incur the
overhead of creating another. If a web server is multithreaded, the server will
create a separate thread that listens for client requests. When a new requests
comes in, the server will create a new thread to service the requests and resume
listening for more requests.</p>
<p><img src="week_4/../media/multi-threaded-web-server.png" alt="Figure: Multithreaded server architecture." />
<strong>Figure: Multithreaded server architecture.</strong></p>
<p>Most operating system kernels are also typically multithreaded. During system
boot time on Linux systems, several kernel threads are created to handle tasks
such as managing devices, memory management, and interrupt handling.</p>
<p>There are many benefits to using a multithreaded programming approach:</p>
<ol>
<li><strong>Responsiveness</strong>: Multithreading an interactive application may allow a 
program to continue running even if part of it is blocked or is performing a 
lengthy operation.</li>
<li><strong>Resouce sharing</strong>: Processes can share resources only through techniques 
such as shared memory and message passing. However, threads share the memory and 
the resources of the process to which they belong by default.</li>
<li><strong>Economy</strong>: Allocating memory and resources for process creation is costly. 
Because threads share the resources of the process to which they belong, it is 
more economical to create and context-switch threads.</li>
<li><strong>Scalability</strong>: The benefits of multithreading can be even greater in a 
multiprocessor architecture, where threads may be running in parallel on different 
processing cores.</li>
</ol>
<h2 id="multicore-programming"><a class="header" href="#multicore-programming">Multicore Programming</a></h2>
<p>Due to the need for more computing performance, single-CPU systems evolved into
multi-CPU systems. A trend in system design was to place multiple computing cores
on a single processing chip where each core would then appear as a separate CPU 
to the operating system, such systems are referred to as multicore systems.</p>
<p>Imagine an application with four threads. On a system with a single computing core,
concurrency merely means that the execution of the threads will be interleaved
over time due to the processing core only being capable of executing a single
thread at a time.</p>
<p><img src="week_4/../media/concurrent-single-core.png" alt="Figure: Concurrent execution on a single-core system." />
<strong>Figure: Concurrent execution on a single-core system.</strong></p>
<p>On a system with multiple cores, concurrency means that some threads can run in
parallel due to the system being capable of assigning a separate thread to each
core.</p>
<p><img src="week_4/../media/parallel-multi-core.png" alt="Figure: Parallel execution on a multicore system." /><br />
<strong>Figure: Parallel execution on a multicore system.</strong></p>
<h2 id="types-of-parallelism"><a class="header" href="#types-of-parallelism">Types of Parallelism</a></h2>
<p>There are two types of parallelism:</p>
<ol>
<li><strong>Data parallelism</strong>: Focuses on distributing subsets of the same data across 
multiple computing cores and performing the same operation on each core.</li>
<li><strong>Task parallelism</strong>: Involves distributing not data but tasks (threads) 
across multiple computing cores. Each thread is performing a unique operation. 
Diferent threads may be operating on the same data, or they may be operating on 
different data.</li>
</ol>
<p>It's important to note that these two methods are not mutually exclusive and an
application may use a hybrid method of both strategies.</p>
<h2 id="multithreading-models"><a class="header" href="#multithreading-models">Multithreading Models</a></h2>
<p>Support for threads may be provided either at the user level (user threads) or
by the kernel (kernel threads). User threads are supported above the kernel and
are managed without kenel support. Kernel threads on the other hand are supported
and managed directly by the operating system.</p>
<p>There are three common relationships between user threads and kernel threads.</p>
<ol>
<li><strong>Many-to-One Model</strong>: The many-to-one model maps many user-level threads to 
one kernel thread. Thread management is done by the thread library in user space, 
so it is efficient. However, the entire process will block if a thread makes a 
blocking system call. Also, because only one thread can access the kernel at a time, 
multiple threads are unable to run in parallel on multicore systems.</li>
</ol>
<p><img src="week_4/../media/threads-many-one.png" alt="Figure: Many-to-one model." /><br />
<strong>Figure: Many-to-one model.</strong></p>
<ol start="2">
<li><strong>One-to-One Model</strong>: The one-to-one model maps each user thread to a kernel 
thread. It provides more concurrency than the many-to-one model by allowing another 
thread to run when a thread makes a blocking system call. It also allows multiple 
threads to run in parallel on multiprocessors. The only drawback to this model 
is that creating a user thread requires creating the corresponding kernel thread, 
and a large number of kernel threads may burden the performance of a system.</li>
</ol>
<p><img src="week_4/../media/threads-one-one.png" alt="Figure: One-to-one model." /><br />
<strong>Figure: One-to-one model.</strong></p>
<ol start="3">
<li><strong>Many-to-Many Model</strong>: The many-to-many model (Figure 4.9) multiplexes many 
user-level threads to a smaller or equal number of kernel threads. Although the 
many-to-many model appears to be the most flexible of the models discussed, in 
practice it is difficult to implement.</li>
</ol>
<p><img src="week_4/../media/threads-many-many.png" alt="Figure: Many-to-many model." /><br />
<strong>Figure: Many-to-many model.</strong></p>
<h2 id="creating-threads"><a class="header" href="#creating-threads">Creating Threads</a></h2>
<p>There are two general strategies forr creating multiple threads:</p>
<ol>
<li><strong>Asynchronous threading</strong>: Once the parent creates a child thread, the parent 
resumes its execution, so that the parent and child execute concurrently and 
independently of one another.</li>
<li><strong>Synchronous threading</strong>: The parent thread creates one or more children and 
then must wait for all of its children to terminate before it resumes. Here, the 
threads created by the parent perform work concurrently, but the parent cannot 
continue until this work has been completed. Once each thread has finished its 
work, it terminates and joins with its parent. Only after all of the children 
have joined can the parent resume execution.</li>
</ol>
<h2 id="pthreads"><a class="header" href="#pthreads">Pthreads</a></h2>
<p>Pthreads refers to the POSIX standard (IEEE 1003.1c) defining an API fo thread
creation and synchronisation. It's important to know that Pthreads is simply a
specification for thread behaviour and not an implementation, that is left up to
the operating-system designers.</p>
<p>Below is an example application using Ptheads to calculate the summation of a 
non-negative integer in a separate thread.</p>
<pre><code class="language-c">#include &lt;pthread.h&gt;
#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;

int sum; // The data shared among the threads.
void *runner(void *param); // The function called by each thread.

int main(int argc, char *argv[]) {
    pthread_t tid; // The thread identifier.
    pthread_attr_t attr; // Set of thread attributes.

    // Set the default attributes of the thread.
    pthread_attr_init(&amp;attr);

    // Create the thread.
    pthread_create(&amp;tid, &amp;attr, runner, argv[1]);

    // Wait for the thead to finish executing.
    pthead_join(tid, NULL);

    printf(&quot;Sum: %d\n&quot;, sum);
}

void *runner(void *param) {
    int upper = atoi(param);
    int sum = 0;

    for (int i = 1; i &lt;= upper; i++) {
        sum += 1;
    }

    pthread_exit(0);
}
</code></pre>
<p>This example program creates only a single thread. With the growing dominance of 
multicore systems, writing programs containing several threads has become increasingly 
common. A simple method for waiting on several threads using the <code>pthread_join()</code> 
function is to enclose the operation within a simple for loop.</p>
<pre><code class="language-c">#define NUM_THREADS 10

pthread_t workers[NUM_THREADS];

for (int i = 0; i &lt; NUM_THREADS; i++) {
    pthread_join(workers[i], NULL);
}
</code></pre>
<h2 id="thread-pools"><a class="header" href="#thread-pools">Thread Pools</a></h2>
<p>The idea behing a thread pool is to create a number of threads at start-up and
place them into a pool where they sit and wait for work. In the context of a web
server, when a request is recieved, rather than creating a new thread, it instead
submits the request to the thread pool and resumes waiting for additional requests.
Once the thread completes its service, it returns to the pool and awaits more 
work.</p>
<p>A thread pool has many benefits such as:</p>
<ul>
<li>Servicing a equest within an existing thead is often faster than waiting to 
create a new thread.</li>
<li>A thread pool limits the number of threads that exist at any one point. This
ensures that the system does not get overwhelmed when creating more threads than
it can handle.</li>
<li>Separating the task to be performed from the mechanics of creating the task 
allows us to use different strategies for running the task. For example, the task 
could be scheduled to execute after a time delay or to execute periodically.</li>
</ul>
<p>The number of threads in the pool can be set heuristically based on factors such
as the number of CPUs in the system, amount of physical memory, and the expected
number of concurrent client requests. More sophisticated thread pool architectures
are able to dynamically adjust the number of threads in the pool based off usage
patterns.</p>
<h2 id="fork-join"><a class="header" href="#fork-join">Fork Join</a></h2>
<p>The fork-join method is one in which when the main parent thread creates one or 
more child threads and then waits for the children to terminate and join with it.</p>
<p>This synchronous model is often characterised as explicit thread creation, but 
it is also an excellent candidate for implicit threading. In the latter situation, 
threads are not constructed directly during the fork stage; rather, parallel 
tasks are designated. A library manages the number of threads that are created 
and is also responsible for assigning tasks to threads.</p>
<h2 id="threading-issues"><a class="header" href="#threading-issues">Threading Issues</a></h2>
<ul>
<li><strong>The <code>fork()</code> and <code>exec()</code> system calls</strong></li>
<li><strong>Signal handling</strong></li>
<li><strong>Thread cancellation</strong></li>
<li><strong>Thread-local storage</strong></li>
<li><strong>Scheduler activations</strong></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="week-5"><a class="header" href="#week-5">Week 5</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="synchronisation-1"><a class="header" href="#synchronisation-1">Synchronisation</a></h1>
<p>A cooperating process is one that can affect or be affected by other processes 
executing in the system. Cooperating processes can either directly share a logical
address space or be allowed to share data through shared memory or message passing.
Concurrent access to shared data may result in data inconsistency.</p>
<p>A race condition occurs when several processes access and manipulate the same 
data concurrently and the outcome of the execution depends on the particular order 
in which the access takes place.</p>
<h2 id="the-critical-section-problem"><a class="header" href="#the-critical-section-problem">The Critical-Section Problem</a></h2>
<p>Consider a system of \(n\) processes. Each proces has a segment of code, called
the critical section, in which the process may be accessing - and updating - data
that is shared with at least one other process. When one process is executing in 
its critical section, no other process is allowed  to execute in its critical 
section. The critical-section problem is to design a protocol that the processes 
can use to synchronise their activity so as to cooperatively share data.</p>
<p>Each process must request permission to enter its critical section. The code
implementing this request is the entry section. The critical section may be followed 
by an exit section. The remaining code is the remainder section.</p>
<p><img src="week_5/../media/process-structure.png" alt="Figure: General structure of a typical process." /><br />
<strong>Figure: General structure of a typical process.</strong></p>
<p>A solution to the critical-section problem must satisfy the following three
requirements:</p>
<ol>
<li><strong>Mutual exclusion</strong>: If process \(P_i\) is executing in its critical section,
then no other process can be executing in their critical sections.</li>
<li><strong>Progress</strong>: If no process is executing in its critical section and some processes
wish to enter their critical sections, then only those processes that are not executing
in their remainder sections can participate in deciding which will enter its critical 
section next, and this selection cannot be postponed indefinitely.</li>
<li><strong>Bounded waiting</strong>: There exists a bound, or limit, on the number of times 
that other processes are allowed to enter their critical sections after a process 
has made a request to enter its critical section and before that request is granted.</li>
</ol>
<p>There are two general approaches used to handle critical sections in operating
systems:</p>
<ol>
<li><strong>Preemptive kernels</strong>: A preemptive kernel allows a process to be preemped while 
it's running in kernel mode.</li>
<li><strong>Non-preemptive kernels</strong>: A non-preemptive kernel does not allow a process running
in kernel mode to be preempted; A kernel-mode process will run until it exits kernel
mode, blocks, or voluntarily yields control of the CPU.</li>
</ol>
<p>A non-preemptive kernel is essentially free from race conditions on kernel data
structures as only one process is active in the kernel at a time. Preemptive
kernels on the other hand are not and must be carefully designed to ensure that 
shared kernel data is free from race conditions.</p>
<p>Despite this, preemptive kernels are still preferred as:</p>
<ul>
<li>They allow a real-time process to preempt a process currently running in kernel 
mode</li>
<li>They are more responsive since there is less risk that a kernel-mode process 
will run for an arbitrarily long period before relinquishing the processor to 
waiting processes.</li>
</ul>
<h2 id="petersons-solution"><a class="header" href="#petersons-solution">Peterson's Solution</a></h2>
<p>Peterson's solution is a software-based solution to the critical-section problem.
Due to how modern computer architectures perform basic machine-language instructions,
there are no guarantees that Peterson's solution will work correctly on such
architectures.</p>
<pre><code>int turn;
boolean flag[2];

while (true) {
    flag[i] = true;
    turn = j;

    while (flag[j] &amp;&amp; turn == j);

    // Critical section

    flag[i] = false;

    // Remainder section
}
</code></pre>
<h2 id="hardware-support-for-synchronisation"><a class="header" href="#hardware-support-for-synchronisation">Hardware Support for Synchronisation</a></h2>
<h3 id="memory-barriers"><a class="header" href="#memory-barriers">Memory Barriers</a></h3>
<p>How a computer architecture determines what memory guarantees it will provide
to an application program is known as its memory model. A memory model falls into
one of two categories:</p>
<ol>
<li><strong>Strongly ordered</strong>: Where a memory modification on one processor is immediately
visible to all other processors.</li>
<li><strong>Weakly ordered</strong>: Where modifications to memory on one processor may not be.
immediately visible to other processors.</li>
</ol>
<p>Memory models vary by processor type, so kernel developers cannot make assumptions
regarding the visibility of modifications to memory on a shared-memory multiprocessor.
To address this issue, computer architectures provide instructions that can force
any changes in memory to be propagated to all other processors. Such instructions
are know as memory barriers or memory fences.</p>
<p>When a memory barrier instruction is performed, the system ensures that all loads
and stores are completed before any subsequent load or store operations are performed.
This ensures that even if instructions were re-ordered, the store operations are
completed in memory and visible to other processors before future load or store
operations are performed.</p>
<p>Memory barriers are considered very low-level operations are are typically only
used by kernel developers when writing specialised code that ensures mutual
exclusion.</p>
<h3 id="hardware-instructions"><a class="header" href="#hardware-instructions">Hardware Instructions</a></h3>
<p>Many computer systems provide special hardware instructions that allow us either 
to test and modify the content of a word or to swap the contents of two words
atomically - that is, as one uninterruptible unit. These special instructions 
can be used to solve the critical-section problem. Such examples of these instructions
are <a href="https://en.wikipedia.org/wiki/Test-and-set"><code>test_and_set()</code></a> and 
<a href="https://en.wikipedia.org/wiki/Compare-and-swap"><code>compare_and_swap</code></a>.</p>
<pre><code class="language-c">boolean test_and_set(boolean *target) {
    boolean rv = *target;
    *target = true;

    return rv;
}
</code></pre>
<p><strong>Figure: The definition of the atomic <code>test_and_set()</code> instruction.</strong></p>
<pre><code class="language-c">int compare_and_swap(int *value, int expected, int new_value) {
    int temp = *value;

    if (*value == expected) {
        *value = new_value;
    }

    return temp;
}
</code></pre>
<p><strong>Figure: The definition of the atomic <code>compare_and_swap()</code> instruction.</strong></p>
<h3 id="atomic-variables"><a class="header" href="#atomic-variables">Atomic Variables</a></h3>
<p>An atomic variable provides atomic operations on basic data types such as integers
and booleans. Most systems that support atomic variables provide special atomic 
data types as well as functions for acessing and manipulating atomic variables.
These functions are often implemented using <code>compare_and_swap()</code> operations.</p>
<p>For example, the following increments the atomic integer sequence:</p>
<pre><code class="language-c">increment(&amp;sequence);
</code></pre>
<p>where the <code>increment()</code> function is implemented using the CAS instruction:</p>
<pre><code class="language-c">void increment(atomic_int *v) {
    int temp;

    do {
        temp = *v;
    } while (temp != compare_and_swap(v, temp, temp + 1));
}
</code></pre>
<p>It's important to note however that although atomic variables provide atomic 
updates, they do not entirely solve race conditions in all circumstances.</p>
<h2 id="mutex-locks"><a class="header" href="#mutex-locks">Mutex Locks</a></h2>
<p>Mutex, short for mutual exclusion, locks are used to protect critical sections 
and thus prevent race conditions. They act as high-level software tools to solve
critical-section problems.</p>
<p>A process must first acquire a lock before entering a critical section; it then
releases the lock when it exits the critical section. The <code>acquire()</code> function
acquires the lock, and the <code>release()</code> function releases the lock. A mutex lock 
has a boolean variable <code>available</code> whose value indicates if the lock is available 
or not. Calls to either <code>acquire()</code> or <code>release()</code> must be performed atomically.</p>
<pre><code>acquire() {
    while (!available); /* busy wait */
    available = false;;
}

release() {
    available = true;
}
</code></pre>
<p>The type of mutex lock described above is also called a spin-lock due to the 
process &quot;spinning&quot; while waiting for the lock to become available. The main 
disadvantage with spin locks is that they require busy waiting. While a process 
is in its critical section, any other process that tries to entir its critical 
section must loop continuously in the call to <code>acquire()</code>. This wastes CPU cycles 
that some other process might be able to use productively. On the other hand, 
spinlocks do have an advantage in that no context switch is required when a process
must wait on a lock.</p>
<h2 id="semaphores"><a class="header" href="#semaphores">Semaphores</a></h2>
<p>A semaphore \(S\) is an integer variable that, apart from initialisation, is
accessed only through two standard atomic operations: <code>wait()</code> and <code>signal()</code>.</p>
<p>Operating systems often distinguish between counting and binary semaphores. The
value of a counting semaphore can range over an unrestricted domain. The value
of a binary semaphore can range only between 0 and 1. </p>
<p>Counting semaphores can be used to control access to a given resource consisting
of a finite number of instances. The semaphore is initialised to a number of 
resources available. Each process that wishes to use a resource performs a <code>wait()</code>
operation on the semaphore (decrementing the count). When a process releases 
resource, it performs a <code>signal()</code> operation (incrementing the count). When the 
count for the semaphore goes to 0, all resources are being used. Processes wishing 
to use a resource will block until the count becomes greater than 0.</p>
<pre><code>wait(S) {
    while (S &lt;= 0); // busy wait
    S--;
}

signal (S) {
    S++;
}
</code></pre>
<p><strong>Figure: Semaphore with busy waiting.</strong></p>
<p>It's important to note that some definitions of the <code>wait()</code> and <code>signal()</code> 
semaphore operations, like the example above, present the same problem that 
spinlocks do, busy waiting. To overcome this, other definition of these functions 
are modified as to when a process executes <code>wait()</code>, it suspends itself rather 
than busy waiting. Suspending the process puts it back a waiting queue associated 
with the semaphore. Control is then transferred to the CPU scheduler, which selects 
another process to execute. A process that is suspended, waiting on a semaphore 
\(S\), should be restarted when some other process executes a <code>signal()</code> operation. 
A process that is suspended can be restarted by a <code>wakeup()</code> operation which 
changes the process from the waiting state to the ready state subsequently placing 
it into the ready queue.</p>
<pre><code>typedef struct{
    int value;
    struct process *list;
} semaphore;

wait(semaphore *S) {
    S-&gt;value--;

    if (S-&gt;value &lt; 0) {
        add this process to S-&gt;list;
        block();
    }
}

signal(semaphore *S) {
    S-&gt;value++;

    if (S-&gt;value &lt;= 0) {
        remove a process P from S-&gt;list;
        wakeup(P);
    }
}
</code></pre>
<p><strong>Figure: Semaphore without busy waiting.</strong></p>
<h2 id="monitors"><a class="header" href="#monitors">Monitors</a></h2>
<p>An abstract data type - or ADT - encapsulates data with a set of functions to
operate on that data that are independent of any specific implementation of the
ADT. A monitor type is an ADT that includes a set of programmer-defined operations
that are provided with mutual exclusion within the monitor. The monitor type
also declares the variables whose values define the state of an instance of that
type, along with the bodies of functions that operate on those variables.</p>
<p>The representation of a monitor type cannot be used directly by the various 
processes. Thus, a function defined within a monitor can access only those variables 
declared locally within the monitor and its formal parameters. Similarly, the 
local variables of a monitor can be accessed by only the local functions.</p>
<p>The monitor construct ensures that only one process at a time is active within 
the monitor. Consequently, the programmer does not need to code this synchronization 
constraint explicitly. In some instances however, we need to define additional 
synchronization mechanisms. These mechanisms are provided by the <code>condition</code> 
construct. A programmer who needs to write a tailor-made synchronization scheme 
can define one or more variables of type condition. The only operations that can 
be invoked on a condition variable are <code>wait()</code> and <code>signal()</code>.</p>
<p>The <code>wait()</code> means that the process invoking this operation is suspended until 
another process invokes whereas the <code>signal()</code> operation resumes exactly one 
suspended process. If no process is suspended, then the <code>signal()</code> operation has 
no effect. Contrast this operation with the <code>signal()</code> operation associated with 
semaphores, which always affects the state of the semaphore.</p>
<p>Now suppose that, when the <code>x.signal()</code> operation is invoked by a process \(P\), 
there exists a suspended process \(Q\) associated with condition <code>x</code>. Clearly, 
if the suspended process \(Q\) is allowed to resume its execution, the signaling 
process \(P\) must wait. Otherwise, both \(P\) and \(Q\) would be active 
simultaneously within the monitor. Two possibilities exist:</p>
<ol>
<li><strong>Signal and wait</strong>: \(P\) either waits until \(Q\) leaves the monitor 
or waits for another condition.</li>
<li><strong>Signal and continue</strong>: \(Q\) either waits until \(P\) leaves the monitor 
or waits for another condition.</li>
</ol>
<h2 id="liveness"><a class="header" href="#liveness">Liveness</a></h2>
<p>Liveness refers to a set of properties that a system must satisfy to ensure that 
processes make progress during their execution life cycle. A process waiting
indefinitely is an example of a &quot;liveness failure&quot;. There are many different
forms of liveness failure; however, all are generally characterised by poor
performance and responsiveness.</p>
<h3 id="deadlock"><a class="header" href="#deadlock">Deadlock</a></h3>
<p>The implementation of a semaphore with a waiting queue may result in a situation 
where two or more processes are waiting indefinitely for an event that can be
caused only by one of the waiting processes. When such a state is reached, these
processes are said to be deadlocked.</p>
<h3 id="priority-inversion"><a class="header" href="#priority-inversion">Priority Inversion</a></h3>
<p>A scheduling challenge arises when a higher-priority process needs to read or 
modify kernel data that are currently being accessed by a lower-priority process 
— or a chain of lower-priority processes. Since kernel data are typically protected 
with a lock, the higher-priority process will have to wait for a lower-priority 
one to finish with the resource. The situation becomes more complicated if the 
lower-priority process is preempted in favor of another process with a higher 
priority.</p>
<p>This liveness problem is known as priority inversion, and it can occur only in 
systems with more than two priorities. Typically, priority inversion is avoided 
by implementing a priority-inheritance protocol. According to this protocol, 
all processes that are accessing resources needed by a higher-priority process 
inherit the higher priority until they are finished with the resources in question. 
When they are finished, their priorities revert to their original values.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="synchronisation-examples"><a class="header" href="#synchronisation-examples">Synchronisation Examples</a></h1>
<h2 id="bounded-buffer-problem"><a class="header" href="#bounded-buffer-problem">Bounded-Buffer Problem</a></h2>
<p>In this problem, the producer and consumer processes share the following data
structures.</p>
<pre><code>int n;
semaphore mutex = 1;
semaphore empty = n;
semaphore full = 0;
</code></pre>
<p>We assume that the pool consits of <code>n</code> buffers, each capable of holding one item.
The <code>mutex</code> binary semaphore provides mutual exclusion for accesses to the buffer 
pool and is initialised to the value 1. The <code>empty</code> and <code>full</code> semaphores count
the number of empty and full buffers. </p>
<pre><code>while (true) {
    // ...
    // Produce an item in next_produced
    // ...

    wait(empty);
    wait(mutex);

    // ...
    // Add next_produced to the buffer
    // ...

    signal(mutex);
    signal(full);
}
</code></pre>
<p><strong>Figure: The structure of the producer process.</strong></p>
<pre><code>while (true) {
    wait(full);
    wait(mutex);

    // ...
    // Remove an item from the buffer to next_consumed
    // ...

    signal(mutex);
    signal(empty);

    // ...
    // consume the item in next_consumed
    // ...
}
</code></pre>
<p><strong>Figure: The structure of the consumer process.</strong></p>
<p>We can interpret this code as the producer producing full buffers for the consumer 
or as the consumer producing empty buffers for the producer.</p>
<h2 id="readers-writers-problem"><a class="header" href="#readers-writers-problem">Readers-Writers Problem</a></h2>
<p>Suppose that the database is to be shared among several concurrent processes.
Some of these processes may want only to read the database (readers), whereas 
others may want to update the database (writers). Two readers can access the 
shared data simultaneously with no adverse effects however, if a writer and some 
other process (either reader or writer) access the data simultaneously, chaos 
may ensure.</p>
<p>To avoid these situations from arising, it's required that the writers have
exclusive access to the shared database while writing to the database. This 
synchronisation problem is referred to as the readers-writers problem. This 
problem has several variations, all involving priorities.</p>
<p>The first readers-writers problem requires that no reader be kept waiting unless 
a writer has already obtained permission to use the shared object. No reader should
wait for other readers to finish simply because a writer is waiting. The second 
readers-writers problem requires that once a writer is ready, that writer peform 
its write as soon as possible. If a writer is waiting to access the object, no 
new readers may start reading. </p>
<p>A solution to either may result in starvation. In the first case, writers may
starve, in the second case, readers may starve. It's because of this that other
variants of the problem have been proposed.</p>
<p>In the following solution to the first readers-writers problem, the reader processes
share the following data structures:</p>
<pre><code>semaphore rw_mutex = 1;
semaphore mutex = 1;
int read_count = 0;
</code></pre>
<p>The semaphore <code>rw_mutex</code> is common to both reader and writer processes. The <code>mutex</code>
semaphore is used to ensure mutual exclusion when the variable <code>read_count</code> is
updated. The <code>read_count</code> variable keeps track of how many process are currently
reading the object. The semaphore <code>rw_mutex</code> functions as a mutual exclusion
semaphore for the writers. It is also used by the first or last reader that
enters or exits the critical section. It is not used by readers that enter or 
exit while other readers are in their critical sections.</p>
<pre><code>while (true) {
    wait(rw_mutex);

    // ...
    // writing is performed
    // ...

    signal(rw_mutex);
}
</code></pre>
<p><strong>Figure: The structure of a writer process.</strong></p>
<pre><code>while (true) {
    wait(mutex);
    read_count++;

    if (read_count == 1) {
        wait(rw_mutex);
    }

    signal(mutex);

    // ...
    // reading is performed
    // ...

    wait(mutex);
    read_count--;

    if (read_count == 0) {
        signal(rw_mutex);
    }

    signal(mutex);
}
</code></pre>
<p><strong>Figure:The structure of a reader process.</strong></p>
<h2 id="dining-philosophers-problem"><a class="header" href="#dining-philosophers-problem">Dining-Philosophers Problem</a></h2>
<p>Consider five philosophers who spend their lives thinking and eating. The philosophers
share a circular table surrounded by five chairs. In the center of the table is
a bowl of rice, and the table is laid with five single chopsticks. When a philosopher
thinks, she does not interact with her colleagues. From time to time, a philosopher 
gets hungry and tries to pick up the two chopsticks that are closest to her 
(the chopsticks that are between her and her left and right neighbors). A philosopher 
may pick up only one chopstick at a time. Obviously, she cannot pick up a chopstick 
that is already in the hand of a neighbor. When a hungry philosopher has both 
her chopsticks at the same time, she eats without releasing the chopsticks. 
When she is finished eating, she puts down both chopsticks and starts thinking 
again. </p>
<p>This is known as the dining-philosophers problem and is a classic synchronisation
problem because it is an example of a large class of concurrency-control problems.
It is a simple representation of the need to allocate several resources among
several processes in a deadlock-free and starvation-free manner.</p>
<pre><code>while (true) {
    wait(chopstick[i]);
    wait(chopstick[(i + 1) % 5]);

    // ...
    // eat for a while
    // ...

    signal(chopstick[i]);
    signal(chopstick[(i + 1) % 5]);

    // ...
    // think for a while
    // ...
}
</code></pre>
<p><strong>Figure: The structure of philosopher \(i\).</strong></p>
<p>One simple solution is to represent each chopstick with a semaphore. A philosopher 
tries to grab a chopstick by executing a <code>wait()</code> operation on that semaphore. 
She releases her chopsticks by executing the <code>signal()</code> operation on the appropriate 
semaphores. Thus, the shared data are</p>
<pre><code>semaphore chopstick[5];
</code></pre>
<p>where all the elements of chopstick are initialized to 1. Although this solution 
guarantees that no two neighbors are eating simultaneously, it could create a 
deadlock. Suppose that all five philosophers become hungry at the same time and 
each grabs her left chopstick. All the elements of chopstick will now be equal 
to 0. When each philosopher tries to grab her right chopstick, she will be delayed 
forever.</p>
<p>Here we presenting a deadlock-free solution to the dining-philosophers problem. 
This solution imposes the restriction that a philosopher may pick up her chopsticks 
only if both of them are available.</p>
<pre><code>monitor DiningPhilosophers {
    enum {
        THINKING, 
        HUNGRY, 
        EATING
    } state[5]; 
    condition self[5];

    void pickup(int i) { 
        state[i] = HUNGRY; 
        test(i);

        if (state[i] != EATING) {
            self[i].wait();
        }
    }

    void putdown(int i) { 
        state[i] = THINKING; 
        test((i + 4) % 5); 
        test((i + 1) % 5);
    }

    void test(int i) {
        if ((state[(i + 4) % 5] != EATING) &amp;&amp;
                (state[i] == HUNGRY) &amp;&amp;
                (state[(i + 1) % 5] != EATING) ) {
            state[i] = EATING;
            self[i].signal();
        } 
    }

    initialization code() {
        for (int i = 0; i &lt; 5; i++) {
            state[i] = THINKING;
        }
    } 
}
</code></pre>
<p><strong>Figure: A monitor solution to the dining-philosophers problem.</strong></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="week-8"><a class="header" href="#week-8">Week 8</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="cpu-scheduling"><a class="header" href="#cpu-scheduling">CPU Scheduling</a></h1>
<p>In a system with a single CPU core, only one process can run at a time. A process
is executed until it must wait. With multi-pogramming, multiple processes are
kept in memory at one time. When one process has to wait, the OS takes the CPU
away from that process and gives the CPU to another process. This selection process
is caried out by the CPU schedule. It's important to note that the queue of ready
items is not necessarily a FIFO queue. The records in the queue are typically
pocess control blocks (PCBs) of the processes.</p>
<p>CPU scheduling decisions may take place under the following four circumstances:</p>
<ol>
<li>When a process switches from the running state to the waiting state.</li>
<li>When a pocess switches fom the running state to the ready state.</li>
<li>When a process switches from the waiting state to the ready state.</li>
<li>When a process terminates.</li>
</ol>
<p>When scheduling takes place under circumstances 1 and 4, the scheduling scheme
is non-preemptive or cooperative, otherwise, it is preemptive.</p>
<p>Under non-preemptive scheduling, once the CPU has been allocated to a process,
the process keeps the CPU until it releases it either by termination or by switching 
to the waiting state. The majority of modern operating systems use non-preemptive
scheduling algorithms. Preemptive scheduling can however, result in race conditions 
when data is shared among several processes. </p>
<p>A non-preemptive kernel will wait for a system call to complete or for a process 
to block while waiting for I/O to complete to take place before doing a context 
switch. A preemptive kernel requires mechanisms such as mutex locks to prevent
race conditions when accessing shaed kernel data structues.</p>
<p>Due to interrupts being able to occur at any time, and becuase they cannot always
be ignored by the kernel, sections affected by interrupts must be guarded from
simultaneous use. So that these sections of code are not accessed concurrently 
by several processes, they disable interupts at entry and re-enable them at exit.</p>
<h2 id="dispatcher"><a class="header" href="#dispatcher">Dispatcher</a></h2>
<p>A dispatcher is a module that gives control of the CPUs core to a process selected
by the CPU scheduler. This involves tasks such as:</p>
<ul>
<li>Switching context from one process to another.</li>
<li>Switching to user mode.</li>
<li>Jumping to the proper location in the user program to resume that program.</li>
</ul>
<p>Due to the dispatcher being invoked during every context switch it must be fast.
The time it takes for the dispatcher to stop one process and start another is
known as dispatch latency.</p>
<p><img src="week_8/../media/dispatch_latency.png" alt="Figure: The role of the dispatcher" /><br />
<strong>Figure: The role of the dispatcher</strong></p>
<p>A voluntary context switch occurs when a process has given up contol of the CPU
because it requires a resource that is currently unavailable. A non-voluntary
context switch occurs when the CPU has been taken away from a process. This can
occur when its time slice has expired, its been preempted by a higher-priority
process, and more.</p>
<p>Using the <code>/proc</code> file system, the number of context switches for a given process
can be detemined. For example, the contents of the file <code>/proc/2166/status</code> provides
the following trimmed output:</p>
<pre><code>voluntary_ctxt_switches       150
nonvoluntary_ctxt_swtiches    8
</code></pre>
<p>The Linux command <code>vmstat</code> can also be used to see the number of context switches
on a system-wide level.</p>
<h2 id="scheduling-criteria"><a class="header" href="#scheduling-criteria">Scheduling Criteria</a></h2>
<p>Different CPU scheduling algorithms have diffeernt properties and the choice
of a particular algorithm may favour one class of process over another.</p>
<p>Many critera have been suggested for comparing CPU scheduling alogrithms:</p>
<ul>
<li><strong>CPU utilisation</strong>: We want to keep the CPU as busy as possible. Conceptually, 
CPU utilization can range from 0 to 100 percent. In a real system, it should range 
from 40 percent (for a lightly loaded system) to 90 percent (for a heavily loaded 
system).</li>
<li><strong>Throughput</strong>: If the CPU is busy executing processes, then work is being done. 
One measure of work is the number of processes that are completed per time unit, 
called throughput.</li>
<li><strong>Turn-around time</strong>: From the point of view of a particular process, the 
important criterion is how long it takes to execute that process. The interval 
from the time of submission of a process to the time of completion is the turnaround 
time. Turnaround time is the sum of the periods spent waiting in the ready queue, 
executing on the CPU, and doing I/O.</li>
<li><strong>Waiting time</strong>: The CPU scheduling algorithm does not affect the amount of 
time during which a process executes or does I/O. It only affects the amount of 
time that a process spends waiting in the ready queue. Waiting time is the sum 
of the periods spent waiting in the ready queue.</li>
<li><strong>Response time</strong>: In an interactive system, turnaround time may not be the 
best criterion. Often, a process can produce some output fairly early and can 
continue computing new results while previous results are being output to the 
user. Thus, another measure is the time from the submission of a request until 
the first response is produced. This measure, called response time, is the time 
it takes to start responding, not the time it takes to output the response.</li>
</ul>
<p>In general, it is desirable to maximise CPU utilisation and thoughput, but 
minimise turnaround time, waiting time, and response time. In some cases however,
we may prefer to optimize the minimum or maximum values rather than the average.</p>
<h2 id="cpu-scheduling-alogrithms"><a class="header" href="#cpu-scheduling-alogrithms">CPU Scheduling Alogrithms</a></h2>
<ul>
<li><strong>First-Come, First-Served Scheduling</strong>: First-come, first-served (FCFS) scheduling
is the simplest scheduling algorithm, but it can cause short processes to wait 
for very long processes.</li>
<li><strong>Shortest-Job-First Scheduling</strong>: Shortest-job-first (SJF) scheduling is provably
optimal, providing the shortest average waiting time. Implementing SJF scheduling 
is difficult, how- ever, because predicting the length of the next CPU burst is 
difficult.</li>
<li><strong>Round-Robin Scheduling</strong>: Round-robin (RR) scheduling allocates the CPU to 
each process for a time quantum. If the process does not relinquish the CPU before 
its time quantum expires, the process is preempted, and another process is scheduled 
to run for a time quantum.</li>
<li><strong>Priorirty Scheduling</strong>: Priority scheduling assigns each process a priority,
and the CPU is allocated to the process with the highest priority. Processes with 
the same priority can be scheduled in FCFS order or using RR scheduling.</li>
<li><strong>Multilevel Queue Scheduling</strong>: Multilevel queue scheduling partitions processes 
into several separate queues arranged by priority, and the scheduler executes the 
processes in the highest-priority queue. Different scheduling algorithms may be 
used in each queue.</li>
<li><strong>Multilevel Feedback Queue Scheduling</strong>: Multilevel feedback queues are similar
to multilevel queues, except that a process may migrate between different queues.</li>
</ul>
<h2 id="thread-scheduling"><a class="header" href="#thread-scheduling">Thread Scheduling</a></h2>
<p>On systems implementing the many-to-one and many-to-many models for thread management,
the thread library schedules user-level threads to run on an available lightweight
process (LPW). This scheme is known as process-contention scope (PCS) as competition
for the CPU takes place among threads belonging to the same process. To determine
which kernel-level thread to schedule onto a CPU, the kenel uses system-contention
scope (SCS). Competition for the CPU with SCS scheduling takes place among all 
threads in the system. Systems that use the one-to-one model schedule threads
use only SCS.</p>
<p>Typically, PCS is done according to priority. User-level thread priorities
are set by the programmer and are not adjusted by the thread libray. PCS will
typically preempt the thread currently running in favor of a higher-priority 
thread; however, there is no guarantee of time slicing among threads of equal
priority.</p>
<p>Pthreads identifies the following contention scope values:</p>
<ul>
<li><code>PTHREAD_SCOPE_PROCESS</code> schedules threads using PCS scheduling.</li>
<li><code>PTHREAD_SCOPE_SYSTEM</code> schedules threads using SCS scheduling.</li>
</ul>
<p>On systems implementing the many-to-many model, the <code>PTHREAD_SCOPE_PROCESS</code> policy
schedules user-level threads onto available LWPs. The <code>PTHREAD_SCOPE_SYSTEM</code> 
scheduling policy will create and bind an LWP for each user-level thread on
many-to-many systems. This effectively maps threads using the one-to-one policy.</p>
<p>The Pthread IPC provides two functions for setting and getting the contention
scope policy:</p>
<ul>
<li><code>pthread_attr_setscope(pthread_attr_t *attr, int scope)</code></li>
<li><code>pthread_attr_getscope(pthread_attr_t *attr, int *scope)</code></li>
</ul>
<p>Below is an example program that will first determine the existing contention 
scope and set it to <code>PTHREAD_SCOPE_SYSTEM</code>. It will then create five separate
threads that will run using the SCS scheduling policy. It's important to note 
that on some systems, only certain contention scope values are allowed, i.e. 
Linux and macOS only allow <code>PTHREAD_SCOPE_SYSTEM</code>.</p>
<pre><code class="language-c">#include &lt;pthread.h&gt;
#include &lt;stdio.h&gt;

#define NUM_THREADS 5

void *runner(void *param);

int main(void) {
    int scope;
    pthread_t tid[NUM_THREADS];
    pthread_attr_t attr;

    pthread_attr_init(&amp;attr);

    if (pthread_attr_getscope(&amp;attr, &amp;scope) != 0) {
        fprintf(stderr, &quot;[Error] - Unable to get scheduling scope.\n&quot;);
    } else {
        if (scope == PTHREAD_SCOPE_PROCESS) {
            printf(&quot;PTHREAD_SCOPE_PROCESS\n&quot;);
        } else if (scope == PTHREAD_SCOPE_SYSTEM) {
            printf(&quot;PTHREAD_SCOPE_SYSTEM\n&quot;);
        } else {
            fprintf(stderr, &quot;[Error] - Illegal scope value.\n&quot;);
        }
    }

    pthread_attr_setscope(&amp;attr, PTHREAD_SCOPE_SYSTEM);

    for (size_t i = 0; i &lt; NUM_THREADS; i++) {
        pthread_create(&amp;tid[i], &amp;attr, runner, NULL);        
    }

    for (size_t i = 0; i &lt; NUM_THREADS; i++) {
        pthread_join(tid[i], NULL);
    }
}

void *runner(void *param) {
    // Do some work

    pthread_exit(0);
}
</code></pre>
<h2 id="multi-processor-scheduling"><a class="header" href="#multi-processor-scheduling">Multi-Processor Scheduling</a></h2>
<p>If multiple CPUs are available, load sharing, where multiple threads may run
in parallel, becomes possible, however scheduling issues become correspondingly
more complex.</p>
<p>Traditionally, the term multiprocessor reffered to systems that provided multiple
physical processors. However, the definition of multiprocessor now applies to the
following system architectures:</p>
<ul>
<li>Multicore CPUs</li>
<li>Multithreaded cores</li>
<li>NUMA systems</li>
<li>Hetorogeneous multiprocessing</li>
</ul>
<p>One approach to CPU scheduling in a multiprocessor system has all scheduling
decisions, I/O processing, and other system activites handled by a single processor
called the master server. The other processors execute only user code. This 
asymmetric multiprocessing is simple because only one core accesses the system
data structures, reducing the need for data sharing. The downfall for this approach
however is that the master server becomes a potential bottleneck.</p>
<p>The standard approach for supporting multiprocessors is symmetrical multiprocessing
(SMP), where each processor is self-scheduling. The scheduler for each process
examines the ready queue and selects a thread to run. This provides two possible
strategies for organising the threads eligible to be scheduled:</p>
<ol>
<li>All threads may be in a common ready queue.</li>
<li>Each process may have its own private queue of threads.</li>
</ol>
<p><img src="week_8/../media/ready-queue-organisation.png" alt="Figure: Organisation of ready queues." /><br />
<strong>Figure: Organisation of ready queues.</strong></p>
<p>If option one is chosen, a possible race condition on the shared ready queue
could occur and therefore must ensure that two spearate processors do not
choose to schedule the same thread and that threads are not lost from the queue.
To get around this, locking could be used to protect the common ready queue.
This is not a great solution however, as all access to the queue would require 
lock ownership therefore accessing the shared queue would likely be a performance
bottleneck.</p>
<p>The second option permits each processor to schedule threads from its private
run queue. This is the most common approach on systems supporting SMP as it does 
not suffer from the possible performance problems associated with a shared run 
queue. There are possible issues with per-processor run queues such as workloads
of varying size. This however, can be solved with balancing algorithms which
equalise workloads among all processors.</p>
<h2 id="multicore-processors"><a class="header" href="#multicore-processors">Multicore Processors</a></h2>
<p>Traditionally, SMP systems have allowed several processes to run in parallel by
providing multiple physical processors. However, most contempory computer hardware
now places multiple computing cores on the same physical chip resulting in a 
multicore processor. SMP systems that use multicore processors are faster and
consume less power than systems in which each CPU has its own physical chip.</p>
<p>Multicore processors however, may complicate scheduling issues. When a processor 
accesses memory, it spends a significant amount of time waiting for the data to 
become available. This is known as a memory stall and occurs primarily because
modern processors operate at much faster speeds than memory. A memory stall can
also occur because of a cache miss, the accessing of data that is not in cache 
memory. </p>
<p><img src="week_8/../media/memory-stall.png" alt="Figure: Memory stall." /><br />
<strong>Figure: Memory stall.</strong></p>
<p>To remedy this, many recent hardware designs have implemented multithreading
processing cores in which two, or more, hardware threads are assigned to each 
core. If one hardware thread stalls, the core can switch to another thread.</p>
<p><img src="week_8/../media/multithreaded-multicore-system.png" alt="Figure: Multithreaded multicore system." /><br />
<strong>Figure: Multithreaded multicore system.</strong></p>
<p>From an operating systems perspective, each hardware thread maintains its architectural
state thus appearing as a logical CPU. This is known as chip multithreading (CMT).
Intel processors use the term hyper-threading, or simultaneous multithreading, 
to describe assigning multiple hardware threads to a single processing core.</p>
<p><img src="week_8/../media/chip-multithreading.png" alt="Figure: Chip multithreading." /><br />
<strong>Figure: Chip multithreading.</strong></p>
<p>In the above diagram, the processor contains four computing cores, each containing
two hardware threads. From the perspective of the operating system, there are 
eight logical CPUs.</p>
<p>There are two ways to multithread a processing core:</p>
<ol>
<li>Coarse-grained multithreading</li>
<li>Fine-grained multithreading</li>
</ol>
<p>With coarse-grained multithreading, a thread executes on a core until a long-latency
event occurs. Due to the delay caused by the long-latency event, the core must 
switch to another thread to begin execution, this is expensive. Fine-grained
multithreading switches between threads at a much finer level of granularity.
The architectural design of fine-grained systems includes logic for thread switching
resulting in a low cost for switching between threads.</p>
<p>A multithreaded, multicore processor requires two different levels of scheduling.
This is because the resources of the physical cores must be shared among its hardware
threads and can therefore only execute one hardware thread at a time. On one level
are the scheduling decisions that must be made by the operating system as it
chooses which software thread to run on each hardware thread. A second level of
scheduling specifies how each core decides which hardware thread to run. These
two levels are not necessarily mutally exclusive.</p>
<h2 id="load-balancing"><a class="header" href="#load-balancing">Load Balancing</a></h2>
<p>Load balancing attempts to keep the workload evenly distributed across all processors
in an SMP system. Load balancing is typically necessary only on systems where 
each processor has its own private ready queue. On systems with a common run queue,
one a processor becomes idle, it immediately extracts a runnable thread from the
common ready queue.</p>
<p>There are two general approaches to load balancing:</p>
<ul>
<li><strong>Push migration</strong>: A specific task periodically checks the load on each processor 
and, if it finds an imbalance, evenly distributes the load by moving (or pushing)
threads from the overloaded to idle or less-busy processors.</li>
<li><strong>Pull migration</strong>: A pull migration occurs when an idle processor pulls a 
waiting task from a busy processor.</li>
</ul>
<p>The concept of a balanced load may have different meanings. One view may be that
a balanced load requires that all queues have approximately the same number of
threads while another view may be that there must be an equal distribution of 
thread priorities across all queues.</p>
<h2 id="processor-affinity"><a class="header" href="#processor-affinity">Processor Affinity</a></h2>
<p>As a thread runs on a specific processor, the data it uses populates the processors 
cache. If the thread is required to migrate to another process, the contents of 
the cached memory must be invalidated for the first processor, and the cache for 
the second processor must be repopulated. This is a high cost operation and most
operating systems, with the aid of SMP, try to avoid migrating a thread from one
processor to another. Instead, they attempt to keep a thread running on the same 
processor to take advantage of the &quot;warm&quot; cache. This is known as processor affinity,
that is, a process has an affinity for the processor on which is is currently 
running.</p>
<p>If the approach of a common ready queue is adopted, a thread may be selected for 
execution by any processor. Thus, if a thread is scheduled on a new processor, 
that processor’s cache must be repopulated. With private, per-processor ready 
queues, a thread is always scheduled on the same processor and can therefore 
benefit from the contents of a warm cache, essentially providing processor 
affinity for free.</p>
<p>Soft affinity occurs when the operating system has a policy of attempting to 
keep a process running on the same process but doesn't guarantee it will do so.
In contrast, some systems provide system calls that support hard affinity, thereby
allowing a process to specify a subset of processors on which is can run.</p>
<h2 id="real-time-cpu-scheduling"><a class="header" href="#real-time-cpu-scheduling">Real-Time CPU Scheduling</a></h2>
<p>Soft real-time systems provide no guarantee as to when a critical real-time 
process will be scheduled. They guarantee only that the process will be given
preference over non-critical processes. In a hard real-time system, a task must
be serviced by its deadline; service after the deadline has expired is the same
as no service at all.</p>
<h3 id="minimising-latency"><a class="header" href="#minimising-latency">Minimising Latency</a></h3>
<p>Event latency is the amount of time that elapses from when an event occurs to when
it's serviced. Different events have different latency requirements. For example,
the latency requirement for an antilock brake system may be between 3 to 5 milliseconds
while an embedded system controlling a radar in an airliner might tolerate a 
latency period of several seconds.</p>
<p><img src="week_8/../media/event-latency.png" alt="Figure: Event latency." /><br />
<strong>Figure: Event latency.</strong></p>
<p>Two types of latencies affect the performance of real-time systems:</p>
<ol>
<li><strong>Interrupt latency</strong>: The period of time from the arrival of an interrupt to the 
CPU to the start of the routine that services the interrupt.</li>
<li><strong>Dispatch latency</strong>: The amount of time required for the scheduling dispatcher
to stop one process and start another.</li>
</ol>
<h3 id="priority-based-scheduling"><a class="header" href="#priority-based-scheduling">Priority-Based Scheduling</a></h3>
<h3 id="rate-monotonic-scheduling"><a class="header" href="#rate-monotonic-scheduling">Rate-Monotonic Scheduling</a></h3>
<h3 id="earliest-deadline-first-scheduling"><a class="header" href="#earliest-deadline-first-scheduling">Earliest-Deadline-First Scheduling</a></h3>
<h3 id="proportional-share-scheduling"><a class="header" href="#proportional-share-scheduling">Proportional Share Scheduling</a></h3>
<h3 id="posix-real-time-scheduling"><a class="header" href="#posix-real-time-scheduling">POSIX Real-Time Scheduling</a></h3>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            MathJax.Hub.Register.StartupHook('End', function() {
                window.setTimeout(window.print, 100);
            });
        });
        </script>

    </div>
    </body>
</html>
