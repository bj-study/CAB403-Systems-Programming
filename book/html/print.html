<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>CAB403-Systems-Programming</title>
        <meta name="robots" content="noindex" />


        <!-- Custom HTML head -->
        
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->

        <!-- MathJax -->
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

        <!-- Custom JS scripts for mdbook-pdf PDF generation -->
        <script type='text/javascript'>
            let markAllContentHasLoadedForPrinting = () =>
                window.setTimeout(
                    () => {
                        let p = document.createElement('div');
                        p.setAttribute('id', 'content-has-all-loaded-for-mdbook-pdf-generation');
                        document.body.appendChild(p);
                    }, 100
                );

            window.addEventListener('load', () => {
                // Expand all the <details> elements for printing.
                r = document.getElementsByTagName('details');
                for (let i of r)
                    i.open = true;

                try {
                    MathJax.Hub.Register.StartupHook('End', markAllContentHasLoadedForPrinting);
                } catch (e) {
                    markAllContentHasLoadedForPrinting();
                }
            });
        </script>
    <div style="display: none"><a href="#preface">preface</a><a href="#.week_1-preface">.week_1-preface</a><a href="#week_1-operating-systems">week_1-operating-systems</a><a href="#week_1-input_and_output">week_1-input_and_output</a><a href="#week_1-pointers">week_1-pointers</a><a href="#week_1-functions">week_1-functions</a><a href="#.week_2-preface">.week_2-preface</a><a href="#week_2-operating-system-structures">week_2-operating-system-structures</a><a href="#week_2-arrays">week_2-arrays</a><a href="#week_2-strings">week_2-strings</a><a href="#week_2-structures">week_2-structures</a><a href="#week_2-memory_management">week_2-memory_management</a><a href="#.week_3-preface">.week_3-preface</a><a href="#week_3-processes">week_3-processes</a><a href="#.week_4-preface">.week_4-preface</a><a href="#week_4-threads">week_4-threads</a><a href="#.week_5-preface">.week_5-preface</a><a href="#week_5-synchronisation">week_5-synchronisation</a><a href="#week_5-synchronisation_examples">week_5-synchronisation_examples</a><a href="#.week_6-preface">.week_6-preface</a><a href="#week_6-safety_critical_systems">week_6-safety_critical_systems</a><a href="#.week_7-preface">.week_7-preface</a><a href="#week_7-distributed_systems">week_7-distributed_systems</a><a href="#week_7-sockets">week_7-sockets</a><a href="#.week_8-preface">.week_8-preface</a><a href="#week_8-cpu_scheduling">week_8-cpu_scheduling</a><a href="#.week_9-preface">.week_9-preface</a><a href="#week_9-deadlocks">week_9-deadlocks</a><a href="#.week_11-preface">.week_11-preface</a><a href="#week_11-main-memory">week_11-main-memory</a><a href="#.week_12-preface">.week_12-preface</a><a href="#week_12-file-systems">week_12-file-systems</a><a href="#.week_13-preface">.week_13-preface</a><a href="#week_13-virtual-machines">week_13-virtual-machines</a></div>
    </head>
    <body>
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="preface.html">Preface</a></li><li class="chapter-item expanded "><a href=".week_1/preface.html"><strong aria-hidden="true">1.</strong> Week 1</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="week_1/operating-systems.html"><strong aria-hidden="true">1.1.</strong> Operating Systems</a></li><li class="chapter-item expanded "><a href="week_1/input_and_output.html"><strong aria-hidden="true">1.2.</strong> Input and Output</a></li><li class="chapter-item expanded "><a href="week_1/pointers.html"><strong aria-hidden="true">1.3.</strong> Pointers</a></li><li class="chapter-item expanded "><a href="week_1/functions.html"><strong aria-hidden="true">1.4.</strong> Functions</a></li></ol></li><li class="chapter-item expanded "><a href=".week_2/preface.html"><strong aria-hidden="true">2.</strong> Week 2</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="week_2/operating-system-structures.html"><strong aria-hidden="true">2.1.</strong> Operating System Structures</a></li><li class="chapter-item expanded "><a href="week_2/arrays.html"><strong aria-hidden="true">2.2.</strong> Arrays</a></li><li class="chapter-item expanded "><a href="week_2/strings.html"><strong aria-hidden="true">2.3.</strong> Strings</a></li><li class="chapter-item expanded "><a href="week_2/structures.html"><strong aria-hidden="true">2.4.</strong> Structures</a></li><li class="chapter-item expanded "><a href="week_2/memory_management.html"><strong aria-hidden="true">2.5.</strong> Dynamic Memory Management</a></li></ol></li><li class="chapter-item expanded "><a href=".week_3/preface.html"><strong aria-hidden="true">3.</strong> Week 3</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="week_3/processes.html"><strong aria-hidden="true">3.1.</strong> Processes</a></li></ol></li><li class="chapter-item expanded "><a href=".week_4/preface.html"><strong aria-hidden="true">4.</strong> Week 4</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="week_4/threads.html"><strong aria-hidden="true">4.1.</strong> Threads</a></li></ol></li><li class="chapter-item expanded "><a href=".week_5/preface.html"><strong aria-hidden="true">5.</strong> Week 5</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="week_5/synchronisation.html"><strong aria-hidden="true">5.1.</strong> Synchronisation</a></li><li class="chapter-item expanded "><a href="week_5/synchronisation_examples.html"><strong aria-hidden="true">5.2.</strong> Synchronisation Examples</a></li></ol></li><li class="chapter-item expanded "><a href=".week_6/preface.html"><strong aria-hidden="true">6.</strong> Week 6</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="week_6/safety_critical_systems.html"><strong aria-hidden="true">6.1.</strong> Safety Critical Systems</a></li></ol></li><li class="chapter-item expanded "><a href=".week_7/preface.html"><strong aria-hidden="true">7.</strong> Week 7</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="week_7/distributed_systems.html"><strong aria-hidden="true">7.1.</strong> Distributed Systems</a></li><li class="chapter-item expanded "><a href="week_7/sockets.html"><strong aria-hidden="true">7.2.</strong> Sockets</a></li></ol></li><li class="chapter-item expanded "><a href=".week_8/preface.html"><strong aria-hidden="true">8.</strong> Week 8</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="week_8/cpu_scheduling.html"><strong aria-hidden="true">8.1.</strong> CPU Scheduling</a></li></ol></li><li class="chapter-item expanded "><a href=".week_9/preface.html"><strong aria-hidden="true">9.</strong> Week 9</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="week_9/deadlocks.html"><strong aria-hidden="true">9.1.</strong> Deadlocks</a></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">10.</strong> Week 10</div></li><li class="chapter-item expanded "><a href=".week_11/preface.html"><strong aria-hidden="true">11.</strong> Week 11</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="week_11/main-memory.html"><strong aria-hidden="true">11.1.</strong> Main Memory</a></li></ol></li><li class="chapter-item expanded "><a href=".week_12/preface.html"><strong aria-hidden="true">12.</strong> Week 12</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="week_12/file-systems.html"><strong aria-hidden="true">12.1.</strong> File Systems</a></li></ol></li><li class="chapter-item expanded "><a href=".week_13/preface.html"><strong aria-hidden="true">13.</strong> Week 13</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="week_13/virtual-machines.html"><strong aria-hidden="true">13.1.</strong> Virtual Machines</a></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">CAB403-Systems-Programming</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="cab403-study-guide--2023-semester-1"><a class="header" href="#cab403-study-guide--2023-semester-1">CAB403 Study Guide | 2023 Semester 1</a></h1>
<p>Timothy Chappell | Notes for CAB403 at the Queensland University of Technology</p>
<h2 id="unit-description"><a class="header" href="#unit-description">Unit Description</a></h2>
<h2 id="disclaimer"><a class="header" href="#disclaimer">Disclaimer</a></h2>
<p>Everything written here is based off the QUT course content and the recommended
text books. If any member of the QUT staff or a representative of such finds 
any issue with these guides please contact me at jeynesbrook@gmail.com. </p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="week-1"><a class="header" href="#week-1">Week 1</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="operating-systems"><a class="header" href="#operating-systems">Operating Systems</a></h1>
<h2 id="what-is-an-operating-system"><a class="header" href="#what-is-an-operating-system">What is an Operating System</a></h2>
<p>An operating system is a program that acts as an intermediary between a user of
a computer and the computer hardware. It's acts as a resource allocator managing 
all resources and decides between conflicting requests for efficient and fair 
resource use. An OS also controls the execution of programs to prevent errors 
and improper use of the computer.</p>
<p>The operating system is responsible for:</p>
<ul>
<li>Executing programs</li>
<li>Make solving user problems easier</li>
<li>Make the computer system convenient to use</li>
<li>Use the computer hardware in an efficient manner</li>
</ul>
<h2 id="computer-system-structure"><a class="header" href="#computer-system-structure">Computer System Structure</a></h2>
<p>Computer systems can be divided into four main components</p>
<ol>
<li><strong>Hardware</strong>: These items provide basic computing resources, i.e. CPU, memory, 
I/O devices.</li>
<li><strong>Operating system</strong>: Controls and coordinates the use of hardware among 
various applications and users.</li>
<li><strong>Application programs</strong>: These items define the ways in which the system
resources are used to solve the computing problems of the use, i.e. word processors,
compilers, web browsers, database systems, video games.</li>
<li><strong>Users</strong>: People, machines, or other computers.</li>
</ol>
<h2 id="computer-startup"><a class="header" href="#computer-startup">Computer Startup</a></h2>
<p>A bootstrap program is loaded at power-up or reboot. This program is typically
stored in ROM or EPROM and is generally know as firmware. This bootstrap program
is responsible for initialising all aspects of the system, loading the operating
system kernel, and starting execution.</p>
<h2 id="computer-system-organisation"><a class="header" href="#computer-system-organisation">Computer System Organisation</a></h2>
<ul>
<li>I/O devices and the CPU can execute concurrently.</li>
<li>Each device controller is in charge of a particular device type and has a local
buffer.</li>
<li>The CPU moves data from/to the main memory to/from local buffers.</li>
<li>I/O is from the device to the local buffer of a particular controller.</li>
<li>The device controller informs the CPU that it has finished its operation by
causing an interrupt.</li>
</ul>
<h2 id="common-functions-of-interrupts"><a class="header" href="#common-functions-of-interrupts">Common Functions of Interrupts</a></h2>
<p>Operating systems are interrupt driven. Interrupts transfer control to the 
interrupt service routine. This generally happens through the interrupt vector 
which contains the addresses of all the service routines. The interrupt architecture 
must save the address of the interrupted instruction.</p>
<p>A trap or exception is a software-generated interrupt caused by either an error
or a user request.</p>
<h2 id="interrupt-handling"><a class="header" href="#interrupt-handling">Interrupt Handling</a></h2>
<p>The operating systems preserves the state of the CPU by storing registers and the
program counter. It then determines which type of interrupt occured, polling or 
vectored interrupt system.</p>
<p>Once determined what caused the interrupt, separate segments of code determine
what action should be taken for each type of interrupt.</p>
<p><img src="week_1/../media/interrupt_timeline.jpg" alt="Figure: Interrupt timeline for a single program doing output." /><br />
<strong>Figure: Interrupt timeline for a single program doing output.</strong></p>
<h2 id="io-structure"><a class="header" href="#io-structure">I/O Structure</a></h2>
<p>There are two ways I/O is usually structured:</p>
<ol>
<li>After I/O starts, control returns to the user program only upon I/O completion.
<ul>
<li>Wait instructions idle the CPU until the next interrupt.</li>
<li>At most, one I/O request is outstanding at a time. This means no simultaneous 
I/O processing.</li>
</ul>
</li>
<li>After I/O starts, control returns to the user program without waiting for I/O 
completion.
<ul>
<li><strong>System call</strong>: Request to the OS to allow users to wait for I/O completion.</li>
<li>A <strong>device-status table</strong> containes entries for each I/O device indicating
its type, address, and state.</li>
<li>The OS indexes into the I/O device table to determine the device status and
to modify a table entry to include an interrupt.</li>
</ul>
</li>
</ol>
<h2 id="storage-definitions-and-notation-review"><a class="header" href="#storage-definitions-and-notation-review">Storage Definitions and Notation Review</a></h2>
<p>The basic unit of computer storage is a bit. A bit contains one of two values,
0 and 1. A byte is 8 bits, and on most computers is the smallest convenient chunck
of storage.</p>
<ul>
<li>A kilobyte, or KB, is \(1,024\) bytes</li>
<li>A megabyte, or MB, is \(1,024^2\) bytes</li>
<li>A gigabyte, or GB, is \(1,024^3\) bytes</li>
<li>A terabyte, or TB, is \(1,024^4\) bytes</li>
<li>A petabyte, or PB, is \(1,024^5\) bytes</li>
</ul>
<h2 id="direct-memory-access-structure"><a class="header" href="#direct-memory-access-structure">Direct Memory Access Structure</a></h2>
<p>This method is used for high-speed I/O devices able to transmit information at
close to memory speeds. Device controllers transfer blocks of data from buffer
storage directly to main memory without CPU intervention. This means only one
interrupt is generated per block rather than the one interrupt per byte.</p>
<h2 id="storage-structure"><a class="header" href="#storage-structure">Storage Structure</a></h2>
<ul>
<li><strong>Main memory</strong>: Only large storage media that the CPU can access directly.
<ul>
<li>Random access</li>
<li>Typically volatile</li>
</ul>
</li>
<li><strong>Secondary storage</strong>: An extension of main memory that provides large non-volatile
storage capacity.</li>
<li><strong>Magnetic discs</strong>: Rigid metal or glass platters covered with magnetic recording
material. The disk surface is logically divided into tracks which are sub-diveded
into sectors. The disk controller determines the logical interaction between the
device and the computer.</li>
<li><strong>Solid-state disks</strong>: Achieves faster speeds than magnetic disks and non-volatile
storage capacity through various technologies.</li>
</ul>
<h2 id="storage-hierarchy"><a class="header" href="#storage-hierarchy">Storage Hierarchy</a></h2>
<p>Storages systems are organised into a hierarchy:</p>
<ul>
<li>Speeds</li>
<li>Cost</li>
<li>Volatility.</li>
</ul>
<p>There is a device driver for each device controller used to manage I/O. They provide
uniform interfaces between controllers and the kernel.</p>
<h2 id="caching"><a class="header" href="#caching">Caching</a></h2>
<p>Caching allows information to be copied into a faster storage system. The main memory
can be viewed as a cache for the secondary storage.</p>
<p>Faster storage (cache) is checked first to determine if the information is there:</p>
<ul>
<li>If so, information is used directly from the cache</li>
<li>If not, data is copied to the cache and used there</li>
</ul>
<p>The cache is usually smaller and more expensive that the storage being cached. 
This means cache management is an important design problem.</p>
<h2 id="computer-system-architecture"><a class="header" href="#computer-system-architecture">Computer-System Architecture</a></h2>
<p>Most systems use a single general-purpose processor. However, most systems have
special-purpose processors as well.</p>
<p>Multi-processor systems, also known as parallel systems or tightly-coupled systems, 
usually come in two types; Asymmetric Multi-processing or Symmetric Multi-processor.
Multi-processor systems have a few advantages over a single general-purpose processor:</p>
<ul>
<li>Increase throughput</li>
<li>Economy of scale</li>
<li>Increased reliability, i.e. graceful degradation or fault tolerance</li>
</ul>
<h2 id="clustered-systems"><a class="header" href="#clustered-systems">Clustered Systems</a></h2>
<p>Clustered systems are like Multi-processor systems, they have multiple systems
working together.</p>
<ul>
<li>These systems typically share storage via a storage-area network (SAN).</li>
<li>Provide a high-availability service which survices failures:
<ul>
<li>Asymmetric clustering have one machine in hot-standby mode.</li>
<li>Symmetric clustering have multiple nodes running applications, monitoring 
each other.</li>
</ul>
</li>
<li>Some clusters are for high-performance computing (HPC). Applications running on 
these clusters must be written to use parallelisation.</li>
<li>Some have a distributed lock manager (DLM) to avoid conflicting operations.</li>
</ul>
<h2 id="operating-system-structure"><a class="header" href="#operating-system-structure">Operating System Structure</a></h2>
<p>Multi-programming organises jobs (code and data) so the CPU always has one to 
execute. This is needed for efficiency as a single user cannot keep a CPU and I/O
devices busy at all times. Multi-programming works by keeping a subset of total
jobs in the system, in memory. One job is selected and run via job scheduling.
When it has to wait (for I/O for example), the OS will switch to another job.</p>
<p>Timesharing is a logical extension in which the CPU switches jobs so frequently
that users can interact with each job while it is running.</p>
<ul>
<li>The response time should be less than one second.</li>
<li>Each user has at least one program executing in memory (process).</li>
<li>If processes don't fit in memory, swapping moves them in and out to run.</li>
<li>Virtual memory allows execution of processes not completely in memory.</li>
<li>If several jobs are ready to run at the same time, the CPU scheduler handles 
which to run.</li>
</ul>
<h2 id="operating-system-operations"><a class="header" href="#operating-system-operations">Operating-System Operations</a></h2>
<p>Dual-mode operations (user mode and kernel mode) allow the OS to protect itself 
and other system components. A mode bit provided by the hardware provides the ability
to distinguish when a system is running user code or kernel code. Some instructions 
are designated as privileged and are only executable in kernel mode. System calls
are used to change the mode to kernel, a return from call resets the mode back to 
user.</p>
<p>Most CPUs also support multi-mode operations, i.e. virtual machine manages (VMM)
mode for guest VMs.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="input-and-output"><a class="header" href="#input-and-output">Input and Output</a></h1>
<h2 id="printf"><a class="header" href="#printf"><code>printf()</code></a></h2>
<p><code>printf()</code> is an output function included in <code>stdio.h</code>. It outputs a character
stream to the standard output file, also known as <code>stdout</code>, which is normally
connected to the screen.</p>
<p>It takes 1 or more arguments with the first being called the control string.</p>
<p>Format specifications can be used to interpolate values within the string. A
format specification is a string that begins with <code>%</code> and ends with a conversion
character. In the above example, the format specifications <code>%s</code> and <code>%d</code> were used. 
Characters in the control string that are not part of a format specification are 
placed directly in the output stream; characters in the control string that are 
format specifications are replaced with the value of the corresponding argument.</p>
<p><strong>Example 1: Output with <code>printf()</code></strong></p>
<pre><code class="language-c">printf(&quot;name: %s, age: %d\n&quot;, &quot;John&quot;, 24); // &quot;name: John, age: 24&quot;
</code></pre>
<h2 id="scanf"><a class="header" href="#scanf"><code>scanf()</code></a></h2>
<p><code>scanf()</code> is an input function included in <code>stdio.h</code>. It reads a series of characters
from the standard input file, also known as <code>stdin</code>, which is normally connected
to the keyboard.</p>
<p>It takes 1 or more arguments with the first being called the control string.</p>
<p><strong>Example 2: Reading input with <code>scanf()</code></strong></p>
<pre><code class="language-c">char a, b, c, s[100];
int n;
double x;

scanf(&quot;%c%c%c%d%s%lf&quot;, &amp;a, &amp;b, &amp;c, &amp;n, n, &amp;x);
</code></pre>
<h2 id="relevant-links"><a class="header" href="#relevant-links">Relevant Links</a></h2>
<ul>
<li><a href="https://en.cppreference.com/w/c/io/fprintf">cppreference - printf</a></li>
<li><a href="https://en.cppreference.com/w/c/io/fscanf">cppreference - scanf</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="pointers"><a class="header" href="#pointers">Pointers</a></h1>
<p>A pointer is a variable used to store a memory address. They can be used to access
memory and manipulate an address.</p>
<p><strong>Example 1: Various ways of declaring a pointer</strong></p>
<pre><code class="language-c">// type *variable;

int *a;
int *b = 0;
int *c = NULL;
int *d = (int *) 1307;

int e = 3;
int *f = &amp;e; // `f` is a pointer to the memory address of `e`
</code></pre>
<p><strong>Example 2: Dereferencing pointers</strong></p>
<pre><code class="language-c">int a = 3;
int *b = &amp;a;

printf(&quot;Values: %d == %d\nAddresses: %p == %p\n&quot;, *b, a, b, &amp;a);
</code></pre>
<h2 id="relevant-links-1"><a class="header" href="#relevant-links-1">Relevant Links</a></h2>
<ul>
<li><a href="https://en.cppreference.com/w/cpp/language/pointer">cppreference - pointer</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="functions"><a class="header" href="#functions">Functions</a></h1>
<p>A function construct in C is used to write code that solves a (small) problem.
A procedural C program is made up of one or more functions, one of them being
<code>main()</code>. A C program will always begin execution with <code>main()</code>.</p>
<p>Function parameters can be passed into a function in one of two ways; pass by value
and pass by reference. When a parameter is passed in via value, the data for the
parameters are copied. This means any changes to said variables within the function
will not affect the original values passed in. Pass by reference on the other hand
passes in the memory address of each variable into the function. This means that
changes to the variables within the function will affect the original variables.</p>
<p><strong>Example 1: Function control</strong></p>
<pre><code class="language-c">#include &lt;stdio.h&gt;

void prn_message(const int k);

int main(void) {
    int n;

    printf(&quot;There is a message for you.\n&quot;);
    printf(&quot;How many times do you want to see it?\n&quot;);

    scanf(&quot;%d&quot;, &amp;n);

    prn_message(n);

    return 0;
}

void prn_message(const int k) {
    printf(&quot;Here is the message:\n&quot;);

    for (size_t i = 0; i &lt; k; i++) {
        printf(&quot;Have a nice day!\n&quot;);
    }
}
</code></pre>
<p><strong>Example 2: Pass by values</strong></p>
<pre><code class="language-c">#include &lt;stdio.h&gt;

void swapx(int a, int b);

int main(void) {
    int a = 10;
    int b = 20;

    // Pass by value
    swapx(a, b);

    printf(&quot;within caller - a: %d, b: %b\n&quot;, a, b); // &quot;within caller - a: 10, b: 20&quot;

    return 0;
}

void swapx(int a, int b) {
    int temp;

    temp = a;
    a = b;
    b = temp;

    printf(&quot;within function - a: %d, b: %b\n&quot;, a, b); // &quot;within function - a: 20, b: 10&quot;
}
</code></pre>
<p><strong>Example 3: Pass by value</strong></p>
<pre><code class="language-c">#include &lt;stdio.h&gt;

void swapx(int *a, int *b);

int main(void) {
    int a = 10;
    int b = 20;

    // Pass by reference
    swapx(&amp;a, &amp;b);

    printf(&quot;within caller - a: %d, b: %b\n&quot;, a, b); // &quot;within caller - a: 20, b: 10&quot;

    return 0;
}

void swapx(int *a, int *b) {
    int temp;

    temp = *a;
    *a = *b;
    *b = temp;

    printf(&quot;within function - a: %d, b: %b\n&quot;, *a, *b); // &quot;within function - a: 20, b: 10&quot;
}
</code></pre>
<p><strong>Example 4: Function pointers</strong></p>
<pre><code class="language-c">#include &lt;stdio.h&gt;

void function_a(int num) {
    printf(&quot;Function A: %d\n&quot;, num);
}

void function_b(int num) {
    printf(&quot;Function B: %d\n&quot;, num);
}

void caller(void (*function) (int)) {
    function(1);
    function(2);
    function(3);
}

int main(void) {
    caller(function_a);
    caller(function_b);

    return 0;
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="week-2"><a class="header" href="#week-2">Week 2</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="operating-system-structures"><a class="header" href="#operating-system-structures">Operating System Structures</a></h1>
<h2 id="operating-system-services"><a class="header" href="#operating-system-services">Operating System Services</a></h2>
<p>Operating systems provide an environment for execution of programs and services
to programs and users.</p>
<p>There are many operating system services that provide functions that are
helpful to the user such as:</p>
<ul>
<li><strong>User interface</strong>: Almost all operating systems have a user interface. This can
be in the form of a graphical user interface (GUI) or a command-line (CLI).</li>
<li><strong>Program execution</strong>: The system must be able to load a program into memory
and run that program, end execution, either normally or abnormally.</li>
<li><strong>I/O operations</strong>: A running program may require I/O, which may involve a file
or an I/O device.</li>
<li><strong>File-system manipulation</strong>: The file system is of particular interest. Programs
need to read and write files and directories, create and delete them, search
them, list file information, manage permissions, and more.</li>
<li><strong>Communication</strong>: Processors may exchange information, on the same computer
or between computers over a network.</li>
<li><strong>Error detection</strong>: OS needs to be constantly aware of possible errors:
<ul>
<li>May occur in the CPU and memory hardware, in I/O devices, in user programs,
and more.</li>
<li>For each type of error, the OS should take the appropriate action to ensure
correct and consistent computing.</li>
<li>Debugging facilities can greatly enhance the user's and programmer's abilities
to efficiently use the system.</li>
</ul>
</li>
</ul>
<p>Another set of OS functions exist for ensuring the efficient operation of the
system itself via resource sharing.</p>
<ul>
<li><strong>Resource allocation</strong>: When multiple users or multiple jobs are running
concurrently, resources must be allocated to each of them.</li>
<li><strong>Accounting</strong>: To keep track of which users use how much and what kinds of
resources.</li>
<li><strong>Protection and security</strong>: The owners of information stored in a multi-user
or networked computer system may want to control use of that information. Concurrent
processes should not interfere with each other.
<ul>
<li>Protection involves ensuring that all access to system resources is controlled.</li>
<li>Security of the system from outsiders requires user authentication. This also
extends to defending external I/O devices from invalid access attempts.</li>
<li>If a system is to be protected and secure, pre-cautions must be instituted
throughout it. A chain is only as strong as its weakest link.</li>
</ul>
</li>
</ul>
<h2 id="system-calls"><a class="header" href="#system-calls">System Calls</a></h2>
<p>System calls provide an interface to the services made available by an operating
system. These calls are generally written in higher-level languages such as
C and C++. These system calls however, are mostly accessed by programs via
a high-level application programming interface (API) rather than direct system
call use.</p>
<p>The three most common APIs are Win32 API for Windows, POSIX API for POSIX-based systems,
and JAVA API for the Java virtual machine (JVM)</p>
<p><img src="week_2/../media/standard_api_example.png" alt="Figure: Example of standard api." /></p>
<p>Typically, a number is associated with each system call. The system-call
interface maintains a table indexed according to these numbers. The system call
interface invokes the intended system call in the OS kernel and returns a status
of the systema call and any return values. The caller needs to know nothing about
how the system call is implemented, it just needs to obey the API and understand
what the OS will do as a result call.</p>
<p><img src="week_2/../media/open_system_call_example.png" alt="Figure: The handling of a user application invoking the open() system call." />
<strong>Figure: The handling of a user application invoking the <code>open()</code> system call.</strong></p>
<p>There are many types of system calls:</p>
<ul>
<li>Process control</li>
<li>File management</li>
<li>Device management</li>
<li>Information maintenance</li>
<li>Communications</li>
<li>Protection</li>
</ul>
<p>Often, more information is required than simply the identity of the system call.
There are three general methods used to pass parameters to the OS:</p>
<ol>
<li>Pass parameters into registers. This won't always work however as there may
be more parameters than registers.</li>
<li>Store parameters in a block, or table, in memory, and pass the address of 
the block as a parameter in a register.</li>
<li>Parameters are placed, or pushed, onto the stack by the program and popped
off the stack by the operating system. This method does not limit the number
length of the parameters being passed.</li>
</ol>
<p><img src="week_2/../media/pass_params_as_table.png" alt="Figure: Passing of parameters as a table." />
<strong>Figure: Passing of parameters as a table.</strong></p>
<h2 id="system-programs"><a class="header" href="#system-programs">System Programs</a></h2>
<p>System programs provide a convenient environment for program development and 
execution. They can be generally divided into:</p>
<ul>
<li>File manipulation</li>
<li>Status information sometimes stored in a file modification</li>
<li>Programming language support</li>
<li>Program loading and execution</li>
<li>Communications</li>
<li>Background services</li>
<li>Application programs</li>
</ul>
<h2 id="unix"><a class="header" href="#unix">UNIX</a></h2>
<p>UNIX is limited by hardware functionality. The original UNIX operating system
had limited structing. The UNIX OS consists of two separable parts:</p>
<ol>
<li>Systems programs</li>
<li>The kernel:
<ul>
<li>Consists of everything below the system-call interface and above the
physical hardware.</li>
<li>Provides the file system, CPU scheduling, memory management, and other
operating-system functions.</li>
</ul>
</li>
</ol>
<h2 id="operating-system-structure-1"><a class="header" href="#operating-system-structure-1">Operating System Structure</a></h2>
<p>There are a few ways to organise an operating system.</p>
<h3 id="layered"><a class="header" href="#layered">Layered</a></h3>
<p>The operating system is divided into a number of layers, each built on top
of the lower layers. The bottom layer (layer 0), is the hardware; the highest
is the user interface.</p>
<p>Due to the modularity, layers are selected such that each uses functions and 
services of only lower-level layers.</p>
<p><img src="week_2/../media/layered_os.png" alt="Figure: A layered operating system." />
<strong>Figure: A layered operating system.</strong></p>
<h3 id="microkernel-system"><a class="header" href="#microkernel-system">Microkernel System</a></h3>
<p>In this organisation method, as much as possible is moved from the kernel into 
user space. An example OS that uses a microkernel is Mach, which parts of the
MacOSX kernel (Darwin) is based upon. Communication takes place between user 
modules via message passing.</p>
<div class="table-wrapper"><table><thead><tr><th>Advantages</th><th>Disadvantages</th></tr></thead><tbody>
<tr><td>Easier to extend a microkernel</td><td>Performance overhead of user space to kernel space communication</td></tr>
<tr><td>Easier to port the operating system to new architectures</td><td></td></tr>
<tr><td>More reliable (less code is running in kernel mode)</td><td></td></tr>
<tr><td>More secure</td><td></td></tr>
</tbody></table>
</div>
<p><img src="week_2/../media/microkernel_os.png" alt="Figure: Architecture of a typical microkernel." />
<strong>Figure: Architecture of a typical microkernel.</strong></p>
<h3 id="hybrid-system"><a class="header" href="#hybrid-system">Hybrid System</a></h3>
<p>Most modern operating systems don't use a single model but a use concepts from
a variety. Hybrid systems combine multiple approaches to address performance, security,
and usability needs.</p>
<p>For example, Linux is monolithic, because having the operating system in a 
single address space provides very efficient performance. However, it's also 
modular, so that new functionality can be dynamically added to the kernel.</p>
<h2 id="modules"><a class="header" href="#modules">Modules</a></h2>
<p>Most modern operating systems implement loadable kernel modules (LKMs).
Here, the kernel has a set of core components and can link in additional 
services via modules, either at boot time or during run time</p>
<p>Each core component is separate, can talk to others via known interfaces,
and is loadable as needed within the kernel.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="arrays"><a class="header" href="#arrays">Arrays</a></h1>
<p>An array is a contiguous sequence of data items of the same type.
An array name is an address, or constant pointer value, to the first element 
in said array.</p>
<p>Aggregate operations on an array are not valid in C, this means that you cannot
assign an array to another array. To copy an array you must either copy it component-wise 
(typically via a loop) or via the <code>memcpy()</code> function in <code>string.h</code>.</p>
<p><strong>Example 1: Arrays in practice</strong></p>
<pre><code class="language-c">#include &lt;stdio.h&gt;

const int N = 5;

int main(void) {
    // Allocate space for a[0] to a[4]
    int a[N];
    int i;
    int sum = 0;

    // Fill the array
    for (i = 0; i &lt; N; i++) {
        a[i] = 7 + i * i;
    }

    // Print the array
    for (i = 0; i &lt; N; i++) {
        printf(&quot;a[%d] = %d\n&quot;, i, a[i]);
    }

    // Sum the elements
    for (i = 0; i &lt; N; i++) {
        sum += a[i];
    }

    printf(&quot;\nsum = %d\n&quot;, sum);

    return 0;
}
</code></pre>
<p><strong>Example 2: Arrays and Pointers</strong></p>
<pre><code class="language-c">#include &lt;stdio.h&gt;

const int N = 5;

int main(void) {
    int a[N];
    int sum;
    int *p;

    // The following two calls are the same
    p = a;
    p = &amp;a[0];

    // The following two calls are the same
    p = a + 1;
    p = &amp;a[1];
    

    // Version 1
    sum = 0;

    for (int i = 0; i &lt; N; i++) {
        sum += a[i];
    }
    
    // Version 2
    sum = 0;

    for (int i = 0; i &lt; N; i++) {
        sum += *(a + i);
    }
}
</code></pre>
<p><strong>Example 3: Bubble Sort</strong></p>
<pre><code class="language-c">#include &lt;stdio.h&gt;

void swap(int *arr, int i, int j);
void bubble_sort(int *arr, int n);

void main(void) {
    int arr[] = { 5, 1, 4, 2, 8 };
    int N = sizeof(arr) / sizeof(int);

    bubble_sort(arr, N);

    for (int i = 0; i &lt; N; i++) {
        printf(&quot;%d: %d\n&quot;, i, arr[i]);
    }

    return 0;
}

void swap(int *arr, int i, int j) {
    int temp = arr[i];
    arr[i] = arr[j];
    arr[j] = temp;
}

void bubble_sort(int *arr, int n) {
    for (int i = 0; i &lt; n - 1; i++) {
        for (int j = 0; j &lt; n - 1; j++) {
            if (arr[j] &gt; arr[j + 1]) {
                swap(arr, j, j + 1);
            }
        }
    }
}
</code></pre>
<p><strong>Example 4: Copying an Array</strong></p>
<pre><code class="language-c">#include &lt;stdio.h&gt;
#include &lt;string.h&gt;

int main(void) {
    // Copying an array component-wise
    int array_one[5] = { 1, 2, 3, 4, 5 };
    int array_two[5];

    for (int idx = 0; idx &lt; 5; idx++) {
        array_two[idx] = array_one[idx];
    }

    // Copying an array via memcpy
    memcpy(array_two, array_one, sizeof(int) * 5);
}
</code></pre>
<h2 id="relevant-links-2"><a class="header" href="#relevant-links-2">Relevant Links</a></h2>
<ul>
<li><a href="https://en.cppreference.com/w/cpp/container/array">cppreference - array</a></li>
<li><a href="https://en.cppreference.com/w/cpp/string/byte/memcpy">cppreference - memcpy</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="strings"><a class="header" href="#strings">Strings</a></h1>
<p>A string is a one-dimensional array of type <code>char</code>. All strings must end with a
null character <code>\0</code> which is a byte used to represent the end of a string.</p>
<p>A character in a string can
be accessed either by an element in an array of by making use of a pointer.</p>
<p><strong>Example 1: Strings in practice</strong></p>
<pre><code class="language-c">char *first = &quot;john&quot;;
char last[6];

last[0] = 's';
last[1] = 'm';
last[2] = 'i';
last[3] = 't';
last[4] = 'h';
last[5] = '\0';

printf(&quot;Name: %s, len: %lu&quot;, first, strlen(first));
</code></pre>
<h2 id="relevant-links-3"><a class="header" href="#relevant-links-3">Relevant Links</a></h2>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Null-terminated_string">Wikipedia - Null-terminated string</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="structures"><a class="header" href="#structures">Structures</a></h1>
<p>Structures are named collections of data which are able to be of varying types.</p>
<p><strong>Example 1: Structures in practice</strong></p>
<pre><code class="language-c">struct student {
    char *last_name;
    int student_id;
    char grade;
};

// By using `typedef` we can avoid prefixing the type with `struct`
typedef struct unit {
    char *code;
    char *name;
} unit;

void update_student(struct student *student);
void update_grade(unit *unit);

int main(void) {
    struct student s1 = {
        .last_name = &quot;smith&quot;,
        .student_id = 119493029,
        .grade = 'B',
    };

    s1.grade = 'A';

    update_student(&amp;s1);


    unit new_unit;

    new_unit.name = &quot;Microprocessors and Digital Systems&quot;;

    update_unit(&amp;new_unit);
}

void update_student(struct student *student) {
    // `-&gt;` shorthand for dereference of struct
    student-&gt;last_name = &quot;doe&quot;;
    student-&gt;grade = 'C';
}

void update_unit(unit *unit) {
    // `-&gt;` shorthand for dereference of struct
    unit-&gt;code = &quot;CAB403&quot;;
    unit-&gt;name = &quot;Systems Programming&quot;;
}
</code></pre>
<h2 id="relevant-links-4"><a class="header" href="#relevant-links-4">Relevant Links</a></h2>
<ul>
<li><a href="https://en.cppreference.com/w/c/language/struct">cppreference - Struct declaration</a></li>
<li><a href="https://en.cppreference.com/w/cpp/language/typedef">cppreference - typedef specifier</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="dynamic-memory-management"><a class="header" href="#dynamic-memory-management">Dynamic Memory Management</a></h1>
<p>Memory in a C program can be divided into four categories:</p>
<ol>
<li>Code memory</li>
<li>Static data memory</li>
<li>Runtime stack memory</li>
<li>Heap memory</li>
</ol>
<h2 id="code-memory"><a class="header" href="#code-memory">Code Memory</a></h2>
<p>Code memory is used to store machine instructions. As a program runs,
machine instructions are read from memory and executed.</p>
<h2 id="static-data-memory"><a class="header" href="#static-data-memory">Static Data Memory</a></h2>
<p>Static data memory is used to store static data. There are two categories of
static data: global and static variables.</p>
<p>Global variables are variables defined outside the scope of any function as 
can be seen in example 1. Static variables on the other hand are defined with
the <code>static</code> modifier as seen in example 2.</p>
<p>Both global and static variables have one value attached to them; they are
assigned memory once; and they are initialised before <code>main</code> begins execution
and will continue to exist until the end of execution.</p>
<p><strong>Example 1: Global variables.</strong></p>
<pre><code class="language-c">int counter = 0;

int increment(void) {
    counter++;

    return counter;
}
</code></pre>
<p><strong>Example 2: Static variables.</strong></p>
<pre><code class="language-c">int increment(void) {
    // will be initialised once
    static int counter = 0;

    // increments every time the function is called
    counter++;

    return counter;
}
</code></pre>
<h2 id="runtime-stack-memory"><a class="header" href="#runtime-stack-memory">Runtime Stack Memory</a></h2>
<p>Runtime stack memory is used by function calls and is FILO (First in, Last out).
When a function is invoked, a block of memory is allocated by the runtime 
stack to store the information about the function call. This block of memory 
is termed as an <em>Activation Record</em>.</p>
<p>The information about the function call includes:</p>
<ul>
<li>Return address.</li>
<li>Internal registers and other machine-specific information.</li>
<li>Parameters.</li>
<li>Local variables.</li>
</ul>
<h2 id="heap-memory"><a class="header" href="#heap-memory">Heap Memory</a></h2>
<p>Heap memory is memory that is allocated during the runtime of the program.
On many systems, the heap is allocated in an opposite direction to the stack
and grows towards the stack as more is allocated. On simple systems without 
memory protection, this can cause the heap and stack to collide if too much
memory is allocated to either one.</p>
<p>To deal with this, C provides two functions in the standard library to handle
dynamic memory allocation; <code>calloc()</code> (contiguous allocation) and <code>malloc()</code>
(memory allocation).</p>
<p><code>void *calloc(size_t n, size_t s)</code> returns a pointer to enough space in memory
to store <code>n</code> objects, each of <code>s</code> bytes. The storage set aside is automatically
initialised to zero.</p>
<p><code>void *malloc(size_t s)</code> returns a pointer to a space of size <code>s</code> and leaves the
memory uninitialised.</p>
<p><strong>Example 3: <code>malloc()</code> and <code>calloc()</code></strong></p>
<pre><code class="language-c">#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;

int main() {
    int num_of_elements;
    int *ptr;
    int sum = 0;
  
    printf(&quot;Enter number of elements: &quot;);
    scanf(&quot;%d&quot;, &amp;num_of_elements);
  
    ptr = malloc(num_of_elements * sizeof(int));
    // or
    // ptr = calloc(num_of_elements, sizeof(int));
   
    if (ptr == NULL) {
        printf(&quot;[Error] - Memory was unable to be allocated.&quot;);

        exit(0);
    }
  
    printf(&quot;Enter elements: &quot;);

    for (int i = 0; i &lt; n; i++) {
        scanf(&quot;%d&quot;, ptr + i);

        sum += *(ptr + i);
    }
  
    printf(&quot;Sum = %d&quot;, sum);
    
    free(ptr);
  
    return 0;
}
</code></pre>
<h2 id="relevant-links-5"><a class="header" href="#relevant-links-5">Relevant Links</a></h2>
<ul>
<li><a href="https://en.cppreference.com/w/c/memory/malloc">cppreference - malloc</a></li>
<li><a href="https://en.cppreference.com/w/c/memory/calloc">cppreference - calloc</a></li>
<li><a href="https://en.cppreference.com/w/c/memory/realloc">cppreference - realloc</a></li>
<li><a href="https://en.cppreference.com/w/c/memory/free">cppreference - free</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="week-3"><a class="header" href="#week-3">Week 3</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="processes"><a class="header" href="#processes">Processes</a></h1>
<p>An operating system executes a variety of programes either via:</p>
<ul>
<li>Batch systems (jobs)</li>
<li>or Time-shared systems (user programs or tasks)</li>
</ul>
<p>A process, sometimes referred to as a job, is simply a program in execution.
The status of the current activity of a process is represented by the value 
of the program counter and the contents of the processor’s registers.</p>
<p>A process is made up of multiple parts:</p>
<ul>
<li><strong>Text section</strong>: The executable code</li>
<li><strong>Data section</strong>: Global variables</li>
<li><strong>Heap section</strong>: Memory that is dynamically allocated during program run
time</li>
<li><strong>Stack section</strong>: Temporary data storage when invoking functions 
(such as function parameters, return addresses, and local variables)</li>
</ul>
<p>It's important to note that a program itself is not a process but rather a 
passive entity. In contrast, a process is an active entity, with a program 
counter specifying the next instruction to execute and a set of associated 
resources.</p>
<p>As a process executes, it changes state. A process may be in one of the following 
states:</p>
<ul>
<li><strong>new</strong>: The process is being created.</li>
<li><strong>running</strong>: Instructions are being executed.</li>
<li><strong>waiting</strong>: The process is waiting for some event to occur.</li>
<li><strong>ready</strong>: The process is waiting to be assigned to a processor.</li>
<li><strong>terminated</strong>: The process has finished execution.</li>
</ul>
<p><img src="week_3/../media/process_state.png" alt="Figure: Diagram of process state." />
<strong>Figure: Diagram of process state.</strong></p>
<h2 id="process-control-block-pcb"><a class="header" href="#process-control-block-pcb">Process Control Block (PCB)</a></h2>
<p>Each process is represented in the OS by a process control block, also known as
a task control block. It contains information associated with a specific process
such as:</p>
<ul>
<li><strong>Process state</strong>: The state of the process.</li>
<li><strong>Program counter</strong>: The address of the next instruction to be executed for 
this process.</li>
<li><strong>CPU registers</strong>: The contents of all process-centric registers. Along with 
the program counter, this state information must be saved when an interrupt 
occurs, to allow the process to be continued correctly afterward when it is 
rescheduled to run.</li>
<li><strong>CPU scheduling information</strong>: Information about process priority, pointers 
to scheduling queues, and any other scheduling parameters.</li>
<li><strong>Memory-management information</strong>: This information may include such items 
as the value of the base and limit registers and the page tables, or the 
segment tables, depending on the memory system used by the operating system.</li>
<li><strong>Accounting information</strong>: This information includes the amount of CPU and 
real time used, time limits, account numbers, job or process numbers, etc..</li>
<li><strong>I/O status information</strong>: This information includes the list of I/O 
devices allocated to the process, a list of open files, etc..</li>
</ul>
<h2 id="threads"><a class="header" href="#threads">Threads</a></h2>
<p>In a single-threaded model, only a single thread of instructions can be executed.
This means only a single tasks can be completed at any given time. For example,
in a word document, the user cannot simultaneously type in characters and run 
the spell checker.</p>
<p>In most modern operating systems however, the use of multiple threads allows more
than one task to be performed at any given moment. A multithreaded word 
processor could, for example, assign one thread to manage user input while 
another thread runs the spell checker. </p>
<p>In a multi-threaded system, the PCB is expanded to include information for 
each thread.</p>
<h2 id="process-scheduling"><a class="header" href="#process-scheduling">Process Scheduling</a></h2>
<p>The objective of multi-programming is to have some process running at all times 
so as to maximize CPU utilization. A process scheduler is used to determine which
process should be executed. The number of processes currently in memory is known 
as the degree of multiprogramming</p>
<p>When a process enters the system, it's put into a <strong>ready queue</strong> where it then waits
to be executed. When a process is allocated a CPU core for execution it executes 
for a while and eventually terminates, is interrupted, or waits for the 
occurrence of a particular event. Any process waiting for an event to occur gets
placed into a <strong>wait queue</strong>.</p>
<p><img src="week_3/../media/process_queue.png" alt="Figure: Queueing-diagram representation of process scheduling." />
<strong>Figure: Queueing-diagram representation of process scheduling.</strong></p>
<p>Most processos can be described as either:</p>
<ul>
<li><strong>I/O bound</strong>: A I/O bound process that spends more of its time doing I/O operations.</li>
<li><strong>CPU bound</strong>: Spends more of its time doing more calculations with infrequent
I/O requests.</li>
</ul>
<h2 id="context-switch"><a class="header" href="#context-switch">Context Switch</a></h2>
<p>Interrupts cause the operating system to change a CPU core from its current task
and to run a kernel routine. These operations happen frequently so it's important
to ensure that when returning to the process, no information was lost.</p>
<p>Switching the CPU core to another process requires performing a state save of 
the current process and a state restore of a different process. This task is 
known as a context switch. When a context switch occurs, the kernel saves the 
context of the old process in its PCB and loads the saved context of the new 
process scheduled to run.</p>
<p>The time between a context switch is considered as overhead as no useful work is
done while switching. The more complex the OS and PCB, the longer it takes to
context switch.</p>
<h2 id="process-creation"><a class="header" href="#process-creation">Process Creation</a></h2>
<p>During execution, a process may need to create more processes. The creating 
process is called a parent process, and the new processes are called the 
children of that process. Each of these new processes may in turn create other 
processes, forming a tree of processes. Processes are identified by their process 
identifier (PID).</p>
<p>When a process is created, it will generally require some amount of resources to
accomplish its task. A child process may be able to obtain its resources directly 
from the operating system, or it may be constrained to a subset of the resources 
of the parent process.</p>
<p>When a process creates a new process, two possibilities for execution exist:</p>
<ol>
<li>The parent continues to execute concurrently with its children.</li>
<li>The parent waits until some or all of its children have terminated.</li>
</ol>
<p>There are also two address-space possibilities for the new process:</p>
<ol>
<li>The child process is a duplicate of the parent process (it has the same program 
and data as the parent).</li>
<li>The child process has a new program loaded into it.</li>
</ol>
<p>A new process is created by the <code>fork()</code> system call. The new process consists 
of a copy of the address space of the original process. The return code for the 
<code>fork()</code> is zero for the new (child) process, whereas the (nonzero) process identifier 
of the child is returned to the parent.</p>
<p>Once forked, it's typical for <code>exec()</code> to be called on one of the two processes.
The <code>exec()</code> system call loads a binary file into memory (destroying the memory 
image of the program containing the exec() system call) and starts its execution.</p>
<p>For example, this code forks a new process and, using <code>execlp()</code>, a version of 
the <code>exec(</code>) system call, overlays the process address space with the UNIX command 
<code>/bin/ls</code> (used to get a directory listing).</p>
<pre><code class="language-c">#include &lt;sys/types.h&gt;
#include &lt;sys/wait.h&gt;
#include &lt;stdio.h&gt;
#include &lt;unistd.h&gt;

int main() {
    pid_t pid;
    /* fork a child process */
    pid = fork();

    if (pid &lt; 0) { /* error occurred */
        fprintf(stderr, &quot;Fork failed\n&quot;);
    
        return 1;
    } else if (pid == 0) { /* child process */
        execlp(&quot;/bin/ls&quot;, &quot;ls&quot;, NULL);
    } else { /* parent process */
        /* parent will wait for the child to complete */
        wait(NULL);

        printf(&quot;Child complete\n&quot;);
    }

    return 0;
}
</code></pre>
<p><img src="week_3/../media/fork_system_call.png" alt="Figure: Process creation using the fork() system call." />
<strong>Figure: Process creation using the fork() system call.</strong></p>
<h2 id="process-termination"><a class="header" href="#process-termination">Process Termination</a></h2>
<p>A process terminates when it finishes executing its final statement and asks the 
operating system to delete it by using the <code>exit()</code> system call. At that point, 
the process may return a status value (typically an integer) to its waiting parent 
process (via the <code>wait()</code> system call).</p>
<p>A parent may terminate the execution of one of its children for a variety of 
reasons, such as:</p>
<ul>
<li>The child has exceeded its usage of some of the resources that it has been 
allocated.</li>
<li>The task assigned to the child is no longer required.</li>
<li>The parent is exiting, and the operating system does not allow a child to
continue if its parent terminates.</li>
</ul>
<p>A parent process may wait for the termination of a child process by using the 
<code>wait()</code> system call. The <code>wait()</code> system call is passed a parameter that allows 
the parent to obtain the exit status of the child. This system call also returns 
the process identifier of the terminated child so that the parent can tell which 
of its children has terminated:</p>
<pre><code class="language-c">pid t pid; 
int status;

pid = wait(&amp;status);
</code></pre>
<p>When a process terminates, its resources are deallocated by the operating system. 
However, its entry in the process table must remain there until the parent calls 
<code>wait()</code>, because the process table contains the process’s exit status. </p>
<p>If a child process is terminated but the parent has not called <code>wait()</code>, the process
is known as a zombie process. If a parent is terminated before calling <code>wait()</code>, the process
is know as an orphan.</p>
<h2 id="interprocess-communication"><a class="header" href="#interprocess-communication">Interprocess Communication</a></h2>
<p>Processes within a system may be independent or cooperating. A process is cooperating 
if it can affect or be affected by the other processes executing in the system.</p>
<p>There are a variety of reasons for providing an environment that allows process 
cooperation:</p>
<ul>
<li>Information sharing</li>
<li>Computational speedup</li>
<li>Modularity</li>
<li>Convenience</li>
</ul>
<p>Cooperating processes require an interprocess communication (IPC) mechanism that 
will allow them to exchange data. There are two fundamental models of interprocess 
communication: shared memory and message passing.</p>
<p><img src="week_3/../media/communication_models.png" alt="Figure: Communications models. (a) Shared memory. (b) Message passing." />
<strong>Figure: Communications models. (a) Shared memory. (b) Message passing.</strong></p>
<p>In the shared-memory model, a region of memory that is shared by the cooperating 
processes is established. Processes can then exchange information by reading and 
writing data to the shared region. In the message-passing model, communication 
takes place by means of messages exchanged between the cooperating processes.</p>
<h2 id="producer-consumer-problem"><a class="header" href="#producer-consumer-problem">Producer-Consumer Problem</a></h2>
<p>The Producer-Consumer problem is a common paradigm for cooperating processes.
A producer process produces information that is consumed by a consumer process.</p>
<p>One solution to the producer–consumer problem uses shared memory. To allow producer 
and consumer processes to run concurrently, we must have available a buffer of 
items that can be filled by the producer and emptied by the consumer. This buffer 
will reside in a region of memory that is shared by the producer and consumer 
processes.</p>
<p>Two types of buffers can be used. The <strong>unbounded buffer</strong> places no practical limit 
on the size of the buffer. The consumer may have to wait for new items, but the 
producer can always produce new items. The <strong>bounded buffer</strong> assumes a fixed buffer 
size. In this case, the consumer must wait if the buffer is empty, and the producer 
must wait if the buffer is full.</p>
<h2 id="message-passing"><a class="header" href="#message-passing">Message Passing</a></h2>
<p>Message passing provides a mechanism to allow processes to communicate and to 
synchronize their actions without sharing the same address space.</p>
<p>A message-passing facility provides at least two operations:</p>
<ol>
<li><code>send(message)</code></li>
<li><code>receive(message)</code></li>
</ol>
<p>Before two processes can communicate, they first need to establish a communication
link. </p>
<p>This could be via physical hardware:</p>
<ul>
<li>Shared memory.</li>
<li>Hardware bus.</li>
</ul>
<p>or logical:</p>
<ul>
<li>Direct or indirect communication.</li>
<li>Synchronous or asynchronous communication.</li>
<li>Automatic or explicit buffering.</li>
</ul>
<h2 id="direct-communication"><a class="header" href="#direct-communication">Direct Communication</a></h2>
<p>Under direct communication, each process that wants to communicate must explicitly 
name the recipient or sender of the communication.</p>
<ul>
<li><code>send(P, message)</code> - send a message to process P.</li>
<li><code>receive(Q, message)</code> - receive a message from process Q.</li>
</ul>
<p>A communication link in this scheme has the following properties:</p>
<ul>
<li>A link is established automatically. </li>
<li>The processes need to know only each other’s identity to communicate.</li>
<li>A link is associated with exactly two processes.</li>
<li>Between each pair of processes, there exists exactly one link.</li>
</ul>
<h2 id="indirect-communication"><a class="header" href="#indirect-communication">Indirect Communication</a></h2>
<p>With indirect communication, the messages are sent to and received from mailboxes, 
or ports. A mailbox can be viewed abstractly as an object into which messages can 
be placed by processes and from which messages can be removed. Each mailbox has 
a unique identification.</p>
<ul>
<li><code>send(A, message)</code> — Send a message to mailbox A.</li>
<li><code>receive(A, message)</code> — Receive a message from mailbox A.</li>
</ul>
<p>The operating system then must provide a mechanism that allows a process to do 
the following:</p>
<ul>
<li>Create a new mail box.</li>
<li>Send and receive messages through the mailbox.</li>
<li>Delete a mail box.</li>
</ul>
<p>In this scheme, a communication link has the following properties:</p>
<ul>
<li>A link is established between a pair of processes only if both members of the 
pair have a shared mailbox.</li>
<li>A link may be associated with more than two processes.</li>
<li>Between each pair of communicating processes, a number of different links
may exist, with each link corresponding to one mailbox.</li>
</ul>
<p>Now suppose that processes \(P_1\), \(P_2\), and \(P_3\) all share mailbox A. 
Process \(P_1\) sends a message to A, while both \(P_2\) and \(P_3\) execute 
a <code>receive()</code> from A. Which process will receive the message sent by \(P_3\)? 
The answer depends on which of the following methods we choose:</p>
<ul>
<li>Allow a link to be associated with at most two processes</li>
<li>Allow only one process at a time to execute a receive operation</li>
<li>Allow the system to select arbitrarily the receiver. Sender is notified who
the receiver was.</li>
</ul>
<h2 id="synchronisation"><a class="header" href="#synchronisation">Synchronisation</a></h2>
<p>Communication between processes takes place through calls to <code>send()</code> and <code>receive()</code> 
primitives. Message passing may be either blocking or nonblocking also known as 
synchronous and asynchronous.</p>
<ul>
<li><strong>Blocking send</strong>: The sending process is blocked until the message is received 
by the receiving process or by the mailbox.</li>
<li><strong>Nonblocking send</strong>: The sending process sends the message and resumes operation.</li>
<li><strong>Blocking receive</strong>: The receiver blocks until a message is available.</li>
<li><strong>Nonblocking receive</strong>: The receiver retrieves either a valid message or a null.</li>
</ul>
<p>Different combinations of <code>send()</code> and <code>receive()</code> are possible. When both <code>send()</code> 
and <code>receive()</code> are blocking, we have a rendezvous between the sender and the 
receiver.</p>
<h2 id="buffering"><a class="header" href="#buffering">Buffering</a></h2>
<p>Whether communication is direct or indirect, messages exchanged by communicating 
processes reside in a temporary queue. These queues can be implemented in three 
ways:</p>
<ol>
<li><strong>Zero capacity</strong>: The queue has a maximum length of zero; thus, the link cannot 
have any messages waiting in it. In this case, the sender must block until the 
recipient receives the message.</li>
<li><strong>Bounded capacity</strong>: The queue has finite length \(n\); thus, at most \(n\) 
messages can reside in it. If the queue is not full when a new message is sent, 
the message is placed in the queue (either the message is copied or a pointer to 
the message is kept), and the sender can continue execution without waiting. The 
link’s capacity is finite, however. If the link is full, the sender must block 
until space is available in the queue.</li>
<li><strong>Unbounded capacity</strong>: The queue’s length is potentially infinite; thus, any 
number of messages can wait in it. The sender never blocks.</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="week-4"><a class="header" href="#week-4">Week 4</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="threads-1"><a class="header" href="#threads-1">Threads</a></h1>
<p>A thread is a basic unit of CPU utilisation. A thread consists of:</p>
<ul>
<li>A thread ID</li>
<li>A program counter PC</li>
<li>A register set </li>
<li>A stack</li>
</ul>
<p>A thread shares it's code section, data section, and other operating-system resources
with others threads within the same process. A traditional process usually consists
of a single thread of control, this is called a single-threaded process. A process
with multiple threads of control can therefore perform more than one task at
any given moment, this is called a multi-threaded process.</p>
<p><img src="week_4/../media/single-vs-multi-threaded.png" alt="Figure: Single-threaded and multithreaded processes." />
<strong>Figure: Single-threaded and multithreaded processes.</strong></p>
<p>Most programs that run on modern computers and mobile devices are multithreaded.
For example, A word processor may have a thread for displaying graphics, another 
thread for responding to keystrokes from the user, and a third thread for performing 
spelling and grammar checking in the background.</p>
<p>In certain situations, a single application may be required to perform several 
tasks at any one time. For example, a web server needs to accept many client requests
concurently. A solution to this is to have the server run a single process that 
accepts requests. When a request is recieved, a new process is created to service
the request. Before threads became popular, this was the most common way to handle
such situation.</p>
<p>The problem with this however is that processes are expensive to create. If the
new process will perform the same tasks as the existing process, why incur the
overhead of creating another. If a web server is multithreaded, the server will
create a separate thread that listens for client requests. When a new requests
comes in, the server will create a new thread to service the requests and resume
listening for more requests.</p>
<p><img src="week_4/../media/multi-threaded-web-server.png" alt="Figure: Multithreaded server architecture." />
<strong>Figure: Multithreaded server architecture.</strong></p>
<p>Most operating system kernels are also typically multithreaded. During system
boot time on Linux systems, several kernel threads are created to handle tasks
such as managing devices, memory management, and interrupt handling.</p>
<p>There are many benefits to using a multithreaded programming approach:</p>
<ol>
<li><strong>Responsiveness</strong>: Multithreading an interactive application may allow a 
program to continue running even if part of it is blocked or is performing a 
lengthy operation.</li>
<li><strong>Resouce sharing</strong>: Processes can share resources only through techniques 
such as shared memory and message passing. However, threads share the memory and 
the resources of the process to which they belong by default.</li>
<li><strong>Economy</strong>: Allocating memory and resources for process creation is costly. 
Because threads share the resources of the process to which they belong, it is 
more economical to create and context-switch threads.</li>
<li><strong>Scalability</strong>: The benefits of multithreading can be even greater in a 
multiprocessor architecture, where threads may be running in parallel on different 
processing cores.</li>
</ol>
<h2 id="multicore-programming"><a class="header" href="#multicore-programming">Multicore Programming</a></h2>
<p>Due to the need for more computing performance, single-CPU systems evolved into
multi-CPU systems. A trend in system design was to place multiple computing cores
on a single processing chip where each core would then appear as a separate CPU 
to the operating system, such systems are referred to as multicore systems.</p>
<p>Imagine an application with four threads. On a system with a single computing core,
concurrency merely means that the execution of the threads will be interleaved
over time due to the processing core only being capable of executing a single
thread at a time.</p>
<p><img src="week_4/../media/concurrent-single-core.png" alt="Figure: Concurrent execution on a single-core system." />
<strong>Figure: Concurrent execution on a single-core system.</strong></p>
<p>On a system with multiple cores, concurrency means that some threads can run in
parallel due to the system being capable of assigning a separate thread to each
core.</p>
<p><img src="week_4/../media/parallel-multi-core.png" alt="Figure: Parallel execution on a multicore system." /><br />
<strong>Figure: Parallel execution on a multicore system.</strong></p>
<h2 id="types-of-parallelism"><a class="header" href="#types-of-parallelism">Types of Parallelism</a></h2>
<p>There are two types of parallelism:</p>
<ol>
<li><strong>Data parallelism</strong>: Focuses on distributing subsets of the same data across 
multiple computing cores and performing the same operation on each core.</li>
<li><strong>Task parallelism</strong>: Involves distributing not data but tasks (threads) 
across multiple computing cores. Each thread is performing a unique operation. 
Diferent threads may be operating on the same data, or they may be operating on 
different data.</li>
</ol>
<p>It's important to note that these two methods are not mutually exclusive and an
application may use a hybrid method of both strategies.</p>
<h2 id="multithreading-models"><a class="header" href="#multithreading-models">Multithreading Models</a></h2>
<p>Support for threads may be provided either at the user level (user threads) or
by the kernel (kernel threads). User threads are supported above the kernel and
are managed without kenel support. Kernel threads on the other hand are supported
and managed directly by the operating system.</p>
<p>There are three common relationships between user threads and kernel threads.</p>
<ol>
<li><strong>Many-to-One Model</strong>: The many-to-one model maps many user-level threads to 
one kernel thread. Thread management is done by the thread library in user space, 
so it is efficient. However, the entire process will block if a thread makes a 
blocking system call. Also, because only one thread can access the kernel at a time, 
multiple threads are unable to run in parallel on multicore systems.</li>
</ol>
<p><img src="week_4/../media/threads-many-one.png" alt="Figure: Many-to-one model." /><br />
<strong>Figure: Many-to-one model.</strong></p>
<ol start="2">
<li><strong>One-to-One Model</strong>: The one-to-one model maps each user thread to a kernel 
thread. It provides more concurrency than the many-to-one model by allowing another 
thread to run when a thread makes a blocking system call. It also allows multiple 
threads to run in parallel on multiprocessors. The only drawback to this model 
is that creating a user thread requires creating the corresponding kernel thread, 
and a large number of kernel threads may burden the performance of a system.</li>
</ol>
<p><img src="week_4/../media/threads-one-one.png" alt="Figure: One-to-one model." /><br />
<strong>Figure: One-to-one model.</strong></p>
<ol start="3">
<li><strong>Many-to-Many Model</strong>: The many-to-many model (Figure 4.9) multiplexes many 
user-level threads to a smaller or equal number of kernel threads. Although the 
many-to-many model appears to be the most flexible of the models discussed, in 
practice it is difficult to implement.</li>
</ol>
<p><img src="week_4/../media/threads-many-many.png" alt="Figure: Many-to-many model." /><br />
<strong>Figure: Many-to-many model.</strong></p>
<h2 id="creating-threads"><a class="header" href="#creating-threads">Creating Threads</a></h2>
<p>There are two general strategies forr creating multiple threads:</p>
<ol>
<li><strong>Asynchronous threading</strong>: Once the parent creates a child thread, the parent 
resumes its execution, so that the parent and child execute concurrently and 
independently of one another.</li>
<li><strong>Synchronous threading</strong>: The parent thread creates one or more children and 
then must wait for all of its children to terminate before it resumes. Here, the 
threads created by the parent perform work concurrently, but the parent cannot 
continue until this work has been completed. Once each thread has finished its 
work, it terminates and joins with its parent. Only after all of the children 
have joined can the parent resume execution.</li>
</ol>
<h2 id="pthreads"><a class="header" href="#pthreads">Pthreads</a></h2>
<p>Pthreads refers to the POSIX standard (IEEE 1003.1c) defining an API fo thread
creation and synchronisation. It's important to know that Pthreads is simply a
specification for thread behaviour and not an implementation, that is left up to
the operating-system designers.</p>
<p>Below is an example application using Ptheads to calculate the summation of a 
non-negative integer in a separate thread.</p>
<pre><code class="language-c">#include &lt;pthread.h&gt;
#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;

int sum; // The data shared among the threads.
void *runner(void *param); // The function called by each thread.

int main(int argc, char *argv[]) {
    pthread_t tid; // The thread identifier.
    pthread_attr_t attr; // Set of thread attributes.

    // Set the default attributes of the thread.
    pthread_attr_init(&amp;attr);

    // Create the thread.
    pthread_create(&amp;tid, &amp;attr, runner, argv[1]);

    // Wait for the thead to finish executing.
    pthead_join(tid, NULL);

    printf(&quot;Sum: %d\n&quot;, sum);
}

void *runner(void *param) {
    int upper = atoi(param);
    int sum = 0;

    for (int i = 1; i &lt;= upper; i++) {
        sum += 1;
    }

    pthread_exit(0);
}
</code></pre>
<p>This example program creates only a single thread. With the growing dominance of 
multicore systems, writing programs containing several threads has become increasingly 
common. A simple method for waiting on several threads using the <code>pthread_join()</code> 
function is to enclose the operation within a simple for loop.</p>
<pre><code class="language-c">#define NUM_THREADS 10

pthread_t workers[NUM_THREADS];

for (int i = 0; i &lt; NUM_THREADS; i++) {
    pthread_join(workers[i], NULL);
}
</code></pre>
<h2 id="thread-pools"><a class="header" href="#thread-pools">Thread Pools</a></h2>
<p>The idea behing a thread pool is to create a number of threads at start-up and
place them into a pool where they sit and wait for work. In the context of a web
server, when a request is recieved, rather than creating a new thread, it instead
submits the request to the thread pool and resumes waiting for additional requests.
Once the thread completes its service, it returns to the pool and awaits more 
work.</p>
<p>A thread pool has many benefits such as:</p>
<ul>
<li>Servicing a equest within an existing thead is often faster than waiting to 
create a new thread.</li>
<li>A thread pool limits the number of threads that exist at any one point. This
ensures that the system does not get overwhelmed when creating more threads than
it can handle.</li>
<li>Separating the task to be performed from the mechanics of creating the task 
allows us to use different strategies for running the task. For example, the task 
could be scheduled to execute after a time delay or to execute periodically.</li>
</ul>
<p>The number of threads in the pool can be set heuristically based on factors such
as the number of CPUs in the system, amount of physical memory, and the expected
number of concurrent client requests. More sophisticated thread pool architectures
are able to dynamically adjust the number of threads in the pool based off usage
patterns.</p>
<h2 id="fork-join"><a class="header" href="#fork-join">Fork Join</a></h2>
<p>The fork-join method is one in which when the main parent thread creates one or 
more child threads and then waits for the children to terminate and join with it.</p>
<p>This synchronous model is often characterised as explicit thread creation, but 
it is also an excellent candidate for implicit threading. In the latter situation, 
threads are not constructed directly during the fork stage; rather, parallel 
tasks are designated. A library manages the number of threads that are created 
and is also responsible for assigning tasks to threads.</p>
<h2 id="threading-issues"><a class="header" href="#threading-issues">Threading Issues</a></h2>
<ul>
<li><strong>The <code>fork()</code> and <code>exec()</code> system calls</strong></li>
<li><strong>Signal handling</strong></li>
<li><strong>Thread cancellation</strong></li>
<li><strong>Thread-local storage</strong></li>
<li><strong>Scheduler activations</strong></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="week-5"><a class="header" href="#week-5">Week 5</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="synchronisation-1"><a class="header" href="#synchronisation-1">Synchronisation</a></h1>
<p>A cooperating process is one that can affect or be affected by other processes 
executing in the system. Cooperating processes can either directly share a logical
address space or be allowed to share data through shared memory or message passing.
Concurrent access to shared data may result in data inconsistency.</p>
<p>A race condition occurs when several processes access and manipulate the same 
data concurrently and the outcome of the execution depends on the particular order 
in which the access takes place.</p>
<h2 id="the-critical-section-problem"><a class="header" href="#the-critical-section-problem">The Critical-Section Problem</a></h2>
<p>Consider a system of \(n\) processes. Each proces has a segment of code, called
the critical section, in which the process may be accessing - and updating - data
that is shared with at least one other process. When one process is executing in 
its critical section, no other process is allowed  to execute in its critical 
section. The critical-section problem is to design a protocol that the processes 
can use to synchronise their activity so as to cooperatively share data.</p>
<p>Each process must request permission to enter its critical section. The code
implementing this request is the entry section. The critical section may be followed 
by an exit section. The remaining code is the remainder section.</p>
<p><img src="week_5/../media/process-structure.png" alt="Figure: General structure of a typical process." /><br />
<strong>Figure: General structure of a typical process.</strong></p>
<p>A solution to the critical-section problem must satisfy the following three
requirements:</p>
<ol>
<li><strong>Mutual exclusion</strong>: If process \(P_i\) is executing in its critical section,
then no other process can be executing in their critical sections.</li>
<li><strong>Progress</strong>: If no process is executing in its critical section and some processes
wish to enter their critical sections, then only those processes that are not executing
in their remainder sections can participate in deciding which will enter its critical 
section next, and this selection cannot be postponed indefinitely.</li>
<li><strong>Bounded waiting</strong>: There exists a bound, or limit, on the number of times 
that other processes are allowed to enter their critical sections after a process 
has made a request to enter its critical section and before that request is granted.</li>
</ol>
<p>There are two general approaches used to handle critical sections in operating
systems:</p>
<ol>
<li><strong>Preemptive kernels</strong>: A preemptive kernel allows a process to be preemped while 
it's running in kernel mode.</li>
<li><strong>Non-preemptive kernels</strong>: A non-preemptive kernel does not allow a process running
in kernel mode to be preempted; A kernel-mode process will run until it exits kernel
mode, blocks, or voluntarily yields control of the CPU.</li>
</ol>
<p>A non-preemptive kernel is essentially free from race conditions on kernel data
structures as only one process is active in the kernel at a time. Preemptive
kernels on the other hand are not and must be carefully designed to ensure that 
shared kernel data is free from race conditions.</p>
<p>Despite this, preemptive kernels are still preferred as:</p>
<ul>
<li>They allow a real-time process to preempt a process currently running in kernel 
mode</li>
<li>They are more responsive since there is less risk that a kernel-mode process 
will run for an arbitrarily long period before relinquishing the processor to 
waiting processes.</li>
</ul>
<h2 id="petersons-solution"><a class="header" href="#petersons-solution">Peterson's Solution</a></h2>
<p>Peterson's solution is a software-based solution to the critical-section problem.
Due to how modern computer architectures perform basic machine-language instructions,
there are no guarantees that Peterson's solution will work correctly on such
architectures.</p>
<pre><code>int turn;
boolean flag[2];

while (true) {
    flag[i] = true;
    turn = j;

    while (flag[j] &amp;&amp; turn == j);

    // Critical section

    flag[i] = false;

    // Remainder section
}
</code></pre>
<h2 id="hardware-support-for-synchronisation"><a class="header" href="#hardware-support-for-synchronisation">Hardware Support for Synchronisation</a></h2>
<h3 id="memory-barriers"><a class="header" href="#memory-barriers">Memory Barriers</a></h3>
<p>How a computer architecture determines what memory guarantees it will provide
to an application program is known as its memory model. A memory model falls into
one of two categories:</p>
<ol>
<li><strong>Strongly ordered</strong>: Where a memory modification on one processor is immediately
visible to all other processors.</li>
<li><strong>Weakly ordered</strong>: Where modifications to memory on one processor may not be.
immediately visible to other processors.</li>
</ol>
<p>Memory models vary by processor type, so kernel developers cannot make assumptions
regarding the visibility of modifications to memory on a shared-memory multiprocessor.
To address this issue, computer architectures provide instructions that can force
any changes in memory to be propagated to all other processors. Such instructions
are know as memory barriers or memory fences.</p>
<p>When a memory barrier instruction is performed, the system ensures that all loads
and stores are completed before any subsequent load or store operations are performed.
This ensures that even if instructions were re-ordered, the store operations are
completed in memory and visible to other processors before future load or store
operations are performed.</p>
<p>Memory barriers are considered very low-level operations are are typically only
used by kernel developers when writing specialised code that ensures mutual
exclusion.</p>
<h3 id="hardware-instructions"><a class="header" href="#hardware-instructions">Hardware Instructions</a></h3>
<p>Many computer systems provide special hardware instructions that allow us either 
to test and modify the content of a word or to swap the contents of two words
atomically - that is, as one uninterruptible unit. These special instructions 
can be used to solve the critical-section problem. Such examples of these instructions
are <a href="https://en.wikipedia.org/wiki/Test-and-set"><code>test_and_set()</code></a> and 
<a href="https://en.wikipedia.org/wiki/Compare-and-swap"><code>compare_and_swap</code></a>.</p>
<pre><code class="language-c">boolean test_and_set(boolean *target) {
    boolean rv = *target;
    *target = true;

    return rv;
}
</code></pre>
<p><strong>Figure: The definition of the atomic <code>test_and_set()</code> instruction.</strong></p>
<pre><code class="language-c">int compare_and_swap(int *value, int expected, int new_value) {
    int temp = *value;

    if (*value == expected) {
        *value = new_value;
    }

    return temp;
}
</code></pre>
<p><strong>Figure: The definition of the atomic <code>compare_and_swap()</code> instruction.</strong></p>
<h3 id="atomic-variables"><a class="header" href="#atomic-variables">Atomic Variables</a></h3>
<p>An atomic variable provides atomic operations on basic data types such as integers
and booleans. Most systems that support atomic variables provide special atomic 
data types as well as functions for acessing and manipulating atomic variables.
These functions are often implemented using <code>compare_and_swap()</code> operations.</p>
<p>For example, the following increments the atomic integer sequence:</p>
<pre><code class="language-c">increment(&amp;sequence);
</code></pre>
<p>where the <code>increment()</code> function is implemented using the CAS instruction:</p>
<pre><code class="language-c">void increment(atomic_int *v) {
    int temp;

    do {
        temp = *v;
    } while (temp != compare_and_swap(v, temp, temp + 1));
}
</code></pre>
<p>It's important to note however that although atomic variables provide atomic 
updates, they do not entirely solve race conditions in all circumstances.</p>
<h2 id="mutex-locks"><a class="header" href="#mutex-locks">Mutex Locks</a></h2>
<p>Mutex, short for mutual exclusion, locks are used to protect critical sections 
and thus prevent race conditions. They act as high-level software tools to solve
critical-section problems.</p>
<p>A process must first acquire a lock before entering a critical section; it then
releases the lock when it exits the critical section. The <code>acquire()</code> function
acquires the lock, and the <code>release()</code> function releases the lock. A mutex lock 
has a boolean variable <code>available</code> whose value indicates if the lock is available 
or not. Calls to either <code>acquire()</code> or <code>release()</code> must be performed atomically.</p>
<pre><code>acquire() {
    while (!available); /* busy wait */
    available = false;;
}

release() {
    available = true;
}
</code></pre>
<p>The type of mutex lock described above is also called a spin-lock due to the 
process &quot;spinning&quot; while waiting for the lock to become available. The main 
disadvantage with spin locks is that they require busy waiting. While a process 
is in its critical section, any other process that tries to entir its critical 
section must loop continuously in the call to <code>acquire()</code>. This wastes CPU cycles 
that some other process might be able to use productively. On the other hand, 
spinlocks do have an advantage in that no context switch is required when a process
must wait on a lock.</p>
<h2 id="semaphores"><a class="header" href="#semaphores">Semaphores</a></h2>
<p>A semaphore \(S\) is an integer variable that, apart from initialisation, is
accessed only through two standard atomic operations: <code>wait()</code> and <code>signal()</code>.</p>
<p>Operating systems often distinguish between counting and binary semaphores. The
value of a counting semaphore can range over an unrestricted domain. The value
of a binary semaphore can range only between 0 and 1. </p>
<p>Counting semaphores can be used to control access to a given resource consisting
of a finite number of instances. The semaphore is initialised to a number of 
resources available. Each process that wishes to use a resource performs a <code>wait()</code>
operation on the semaphore (decrementing the count). When a process releases 
resource, it performs a <code>signal()</code> operation (incrementing the count). When the 
count for the semaphore goes to 0, all resources are being used. Processes wishing 
to use a resource will block until the count becomes greater than 0.</p>
<pre><code>wait(S) {
    while (S &lt;= 0); // busy wait
    S--;
}

signal (S) {
    S++;
}
</code></pre>
<p><strong>Figure: Semaphore with busy waiting.</strong></p>
<p>It's important to note that some definitions of the <code>wait()</code> and <code>signal()</code> 
semaphore operations, like the example above, present the same problem that 
spinlocks do, busy waiting. To overcome this, other definition of these functions 
are modified as to when a process executes <code>wait()</code>, it suspends itself rather 
than busy waiting. Suspending the process puts it back a waiting queue associated 
with the semaphore. Control is then transferred to the CPU scheduler, which selects 
another process to execute. A process that is suspended, waiting on a semaphore 
\(S\), should be restarted when some other process executes a <code>signal()</code> operation. 
A process that is suspended can be restarted by a <code>wakeup()</code> operation which 
changes the process from the waiting state to the ready state subsequently placing 
it into the ready queue.</p>
<pre><code>typedef struct{
    int value;
    struct process *list;
} semaphore;

wait(semaphore *S) {
    S-&gt;value--;

    if (S-&gt;value &lt; 0) {
        add this process to S-&gt;list;
        block();
    }
}

signal(semaphore *S) {
    S-&gt;value++;

    if (S-&gt;value &lt;= 0) {
        remove a process P from S-&gt;list;
        wakeup(P);
    }
}
</code></pre>
<p><strong>Figure: Semaphore without busy waiting.</strong></p>
<h2 id="monitors"><a class="header" href="#monitors">Monitors</a></h2>
<p>An abstract data type - or ADT - encapsulates data with a set of functions to
operate on that data that are independent of any specific implementation of the
ADT. A monitor type is an ADT that includes a set of programmer-defined operations
that are provided with mutual exclusion within the monitor. The monitor type
also declares the variables whose values define the state of an instance of that
type, along with the bodies of functions that operate on those variables.</p>
<p>The representation of a monitor type cannot be used directly by the various 
processes. Thus, a function defined within a monitor can access only those variables 
declared locally within the monitor and its formal parameters. Similarly, the 
local variables of a monitor can be accessed by only the local functions.</p>
<p>The monitor construct ensures that only one process at a time is active within 
the monitor. Consequently, the programmer does not need to code this synchronization 
constraint explicitly. In some instances however, we need to define additional 
synchronization mechanisms. These mechanisms are provided by the <code>condition</code> 
construct. A programmer who needs to write a tailor-made synchronization scheme 
can define one or more variables of type condition. The only operations that can 
be invoked on a condition variable are <code>wait()</code> and <code>signal()</code>.</p>
<p>The <code>wait()</code> means that the process invoking this operation is suspended until 
another process invokes whereas the <code>signal()</code> operation resumes exactly one 
suspended process. If no process is suspended, then the <code>signal()</code> operation has 
no effect. Contrast this operation with the <code>signal()</code> operation associated with 
semaphores, which always affects the state of the semaphore.</p>
<p>Now suppose that, when the <code>x.signal()</code> operation is invoked by a process \(P\), 
there exists a suspended process \(Q\) associated with condition <code>x</code>. Clearly, 
if the suspended process \(Q\) is allowed to resume its execution, the signaling 
process \(P\) must wait. Otherwise, both \(P\) and \(Q\) would be active 
simultaneously within the monitor. Two possibilities exist:</p>
<ol>
<li><strong>Signal and wait</strong>: \(P\) either waits until \(Q\) leaves the monitor 
or waits for another condition.</li>
<li><strong>Signal and continue</strong>: \(Q\) either waits until \(P\) leaves the monitor 
or waits for another condition.</li>
</ol>
<h2 id="liveness"><a class="header" href="#liveness">Liveness</a></h2>
<p>Liveness refers to a set of properties that a system must satisfy to ensure that 
processes make progress during their execution life cycle. A process waiting
indefinitely is an example of a &quot;liveness failure&quot;. There are many different
forms of liveness failure; however, all are generally characterised by poor
performance and responsiveness.</p>
<h3 id="deadlock"><a class="header" href="#deadlock">Deadlock</a></h3>
<p>The implementation of a semaphore with a waiting queue may result in a situation 
where two or more processes are waiting indefinitely for an event that can be
caused only by one of the waiting processes. When such a state is reached, these
processes are said to be deadlocked.</p>
<h3 id="priority-inversion"><a class="header" href="#priority-inversion">Priority Inversion</a></h3>
<p>A scheduling challenge arises when a higher-priority process needs to read or 
modify kernel data that are currently being accessed by a lower-priority process 
— or a chain of lower-priority processes. Since kernel data are typically protected 
with a lock, the higher-priority process will have to wait for a lower-priority 
one to finish with the resource. The situation becomes more complicated if the 
lower-priority process is preempted in favor of another process with a higher 
priority.</p>
<p>This liveness problem is known as priority inversion, and it can occur only in 
systems with more than two priorities. Typically, priority inversion is avoided 
by implementing a priority-inheritance protocol. According to this protocol, 
all processes that are accessing resources needed by a higher-priority process 
inherit the higher priority until they are finished with the resources in question. 
When they are finished, their priorities revert to their original values.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="synchronisation-examples"><a class="header" href="#synchronisation-examples">Synchronisation Examples</a></h1>
<h2 id="bounded-buffer-problem"><a class="header" href="#bounded-buffer-problem">Bounded-Buffer Problem</a></h2>
<p>In this problem, the producer and consumer processes share the following data
structures.</p>
<pre><code>int n;
semaphore mutex = 1;
semaphore empty = n;
semaphore full = 0;
</code></pre>
<p>We assume that the pool consits of <code>n</code> buffers, each capable of holding one item.
The <code>mutex</code> binary semaphore provides mutual exclusion for accesses to the buffer 
pool and is initialised to the value 1. The <code>empty</code> and <code>full</code> semaphores count
the number of empty and full buffers. </p>
<pre><code>while (true) {
    // ...
    // Produce an item in next_produced
    // ...

    wait(empty);
    wait(mutex);

    // ...
    // Add next_produced to the buffer
    // ...

    signal(mutex);
    signal(full);
}
</code></pre>
<p><strong>Figure: The structure of the producer process.</strong></p>
<pre><code>while (true) {
    wait(full);
    wait(mutex);

    // ...
    // Remove an item from the buffer to next_consumed
    // ...

    signal(mutex);
    signal(empty);

    // ...
    // consume the item in next_consumed
    // ...
}
</code></pre>
<p><strong>Figure: The structure of the consumer process.</strong></p>
<p>We can interpret this code as the producer producing full buffers for the consumer 
or as the consumer producing empty buffers for the producer.</p>
<h2 id="readers-writers-problem"><a class="header" href="#readers-writers-problem">Readers-Writers Problem</a></h2>
<p>Suppose that the database is to be shared among several concurrent processes.
Some of these processes may want only to read the database (readers), whereas 
others may want to update the database (writers). Two readers can access the 
shared data simultaneously with no adverse effects however, if a writer and some 
other process (either reader or writer) access the data simultaneously, chaos 
may ensure.</p>
<p>To avoid these situations from arising, it's required that the writers have
exclusive access to the shared database while writing to the database. This 
synchronisation problem is referred to as the readers-writers problem. This 
problem has several variations, all involving priorities.</p>
<p>The first readers-writers problem requires that no reader be kept waiting unless 
a writer has already obtained permission to use the shared object. No reader should
wait for other readers to finish simply because a writer is waiting. The second 
readers-writers problem requires that once a writer is ready, that writer peform 
its write as soon as possible. If a writer is waiting to access the object, no 
new readers may start reading. </p>
<p>A solution to either may result in starvation. In the first case, writers may
starve, in the second case, readers may starve. It's because of this that other
variants of the problem have been proposed.</p>
<p>In the following solution to the first readers-writers problem, the reader processes
share the following data structures:</p>
<pre><code>semaphore rw_mutex = 1;
semaphore mutex = 1;
int read_count = 0;
</code></pre>
<p>The semaphore <code>rw_mutex</code> is common to both reader and writer processes. The <code>mutex</code>
semaphore is used to ensure mutual exclusion when the variable <code>read_count</code> is
updated. The <code>read_count</code> variable keeps track of how many process are currently
reading the object. The semaphore <code>rw_mutex</code> functions as a mutual exclusion
semaphore for the writers. It is also used by the first or last reader that
enters or exits the critical section. It is not used by readers that enter or 
exit while other readers are in their critical sections.</p>
<pre><code>while (true) {
    wait(rw_mutex);

    // ...
    // writing is performed
    // ...

    signal(rw_mutex);
}
</code></pre>
<p><strong>Figure: The structure of a writer process.</strong></p>
<pre><code>while (true) {
    wait(mutex);
    read_count++;

    if (read_count == 1) {
        wait(rw_mutex);
    }

    signal(mutex);

    // ...
    // reading is performed
    // ...

    wait(mutex);
    read_count--;

    if (read_count == 0) {
        signal(rw_mutex);
    }

    signal(mutex);
}
</code></pre>
<p><strong>Figure:The structure of a reader process.</strong></p>
<h2 id="dining-philosophers-problem"><a class="header" href="#dining-philosophers-problem">Dining-Philosophers Problem</a></h2>
<p>Consider five philosophers who spend their lives thinking and eating. The philosophers
share a circular table surrounded by five chairs. In the center of the table is
a bowl of rice, and the table is laid with five single chopsticks. When a philosopher
thinks, she does not interact with her colleagues. From time to time, a philosopher 
gets hungry and tries to pick up the two chopsticks that are closest to her 
(the chopsticks that are between her and her left and right neighbors). A philosopher 
may pick up only one chopstick at a time. Obviously, she cannot pick up a chopstick 
that is already in the hand of a neighbor. When a hungry philosopher has both 
her chopsticks at the same time, she eats without releasing the chopsticks. 
When she is finished eating, she puts down both chopsticks and starts thinking 
again. </p>
<p>This is known as the dining-philosophers problem and is a classic synchronisation
problem because it is an example of a large class of concurrency-control problems.
It is a simple representation of the need to allocate several resources among
several processes in a deadlock-free and starvation-free manner.</p>
<pre><code>while (true) {
    wait(chopstick[i]);
    wait(chopstick[(i + 1) % 5]);

    // ...
    // eat for a while
    // ...

    signal(chopstick[i]);
    signal(chopstick[(i + 1) % 5]);

    // ...
    // think for a while
    // ...
}
</code></pre>
<p><strong>Figure: The structure of philosopher \(i\).</strong></p>
<p>One simple solution is to represent each chopstick with a semaphore. A philosopher 
tries to grab a chopstick by executing a <code>wait()</code> operation on that semaphore. 
She releases her chopsticks by executing the <code>signal()</code> operation on the appropriate 
semaphores. Thus, the shared data are</p>
<pre><code>semaphore chopstick[5];
</code></pre>
<p>where all the elements of chopstick are initialized to 1. Although this solution 
guarantees that no two neighbors are eating simultaneously, it could create a 
deadlock. Suppose that all five philosophers become hungry at the same time and 
each grabs her left chopstick. All the elements of chopstick will now be equal 
to 0. When each philosopher tries to grab her right chopstick, she will be delayed 
forever.</p>
<p>Here we presenting a deadlock-free solution to the dining-philosophers problem. 
This solution imposes the restriction that a philosopher may pick up her chopsticks 
only if both of them are available.</p>
<pre><code>monitor DiningPhilosophers {
    enum {
        THINKING, 
        HUNGRY, 
        EATING
    } state[5]; 
    condition self[5];

    void pickup(int i) { 
        state[i] = HUNGRY; 
        test(i);

        if (state[i] != EATING) {
            self[i].wait();
        }
    }

    void putdown(int i) { 
        state[i] = THINKING; 
        test((i + 4) % 5); 
        test((i + 1) % 5);
    }

    void test(int i) {
        if ((state[(i + 4) % 5] != EATING) &amp;&amp;
                (state[i] == HUNGRY) &amp;&amp;
                (state[(i + 1) % 5] != EATING) ) {
            state[i] = EATING;
            self[i].signal();
        } 
    }

    initialization code() {
        for (int i = 0; i &lt; 5; i++) {
            state[i] = THINKING;
        }
    } 
}
</code></pre>
<p><strong>Figure: A monitor solution to the dining-philosophers problem.</strong></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="week-6"><a class="header" href="#week-6">Week 6</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="safety-critical-systems"><a class="header" href="#safety-critical-systems">Safety Critical Systems</a></h1>
<p>A safety-critical system is a system whose failure or malfunction may result in
one (or more) of the following outcomes:</p>
<ul>
<li>Death or series injury</li>
<li>Loss or severe damage to equipment/property</li>
<li>Environmental harm</li>
</ul>
<p>Safety-critical systems are increasingly becoming computer based. A safety-related
system comprises everything (hardware, software, and human aspects) needed to 
perform one or more safety functions.</p>
<h2 id="safety-critical-software"><a class="header" href="#safety-critical-software">Safety Critical Software</a></h2>
<p>Software by itself is neither safe nor unsafe; however, when it is part of a
safety-critical system, it can cause or contribute to unsafe conditions. Such
software is considered safety critical.</p>
<p>According to IEEE, safety-critical software is </p>
<blockquote>
<p>Software whose use in a system can result in unacceptable risk. Safety-critical 
software includes software whose operation or failure to operate can lead to a 
hazardous state, software intended to recover from hazardous states, and software 
intended to mitigate the severity of an accident.</p>
</blockquote>
<p>Software based systems are used in many applications where a failure could increase
the risk of injury or even death. The lower risk systems such as an oven temperature
controller are safety related, whereas the higher risk systems such as the interlocking
between railway points and signals are safety critical.</p>
<p>Although software failures can be safety-critical, the use of software control 
systems contributes to increased system safety. Software monitoring and control
allows a wider range of conditions to be monitored and controlled than is possible
using electro-mechanical safety systems. Software can also detect and correct
safety-critical operator errors.</p>
<h2 id="system-dependability"><a class="header" href="#system-dependability">System Dependability</a></h2>
<p>For many computer-based systems, the most important system property is the dependability
of the system. The dependability of a system reflects the users degree of trust
in that system. It reflects the extent of the users confidence that it will operate
as users expect and that it will not fail in normal use.</p>
<p>System failures may have widespread effects with large numbers of people affected
by the failure. The costs of a system failure may be very high if the failure
leads to economic losses or physical damage. Dependability covers the related 
systems attributes of reliablity, availability, safety, and security. These are
inter-dependent.</p>
<ul>
<li><strong>Availability</strong>: The ability of the system to deliver services when requested.
Availability is expressed as probability: a percentage of the time that the system
is available to deliver services.</li>
<li><strong>Reliablity</strong>: The ability of the system to deliver services as specified.
Reliablity is also expressed as probability.</li>
<li><strong>Safety</strong>: The ability of the system to operate without catastrophic failure
threating people or the environment. Reliablity and availability are necessary
but not sufficient conditions for system safety.</li>
<li><strong>Security</strong>: The ability of the system to protect itself against accidental
or deliberate intrusion.</li>
</ul>
<h2 id="how-to-achieve-safety"><a class="header" href="#how-to-achieve-safety">How to Achieve Safety?</a></h2>
<ul>
<li><strong>Hazard avoidance</strong>: The system is designed so that some classes of hazard 
simply cannot arise.</li>
<li><strong>Hazard detection and removal</strong>: The system is designed so that hazards are
detected and removed before they result in an accident.</li>
<li><strong>Damage limitation</strong>: The system includes protection features that minimise
the damage that may result from an accident.</li>
</ul>
<h2 id="how-safe-is-safe-enough"><a class="header" href="#how-safe-is-safe-enough">How Safe is Safe Enough?</a></h2>
<p>Accidents are inevitable, achieving complete safety is impossible in complex systems.
Accidents in complex systems rarely have a single cause as these systems are designed
to be resilient to a single point of failure. This means that almost all accidents
are a result of combinations of malfunctions rather than single failures. It is
probably the case that anticipating all problem combinations, especially, in software
controlled systems is impossible so achieving complete safety is impossible.</p>
<p>The answer depends greatly on the different industries:</p>
<ul>
<li>&quot;How much should we spend to avoid fatal accidents on the roads or railways?&quot;</li>
<li>&quot;What probability of failure should we permit for the protection system of this
nuclear reactor?&quot;</li>
<li>&quot;What probability of failure should we permit for safety-critical aircraft
components?&quot;</li>
</ul>
<h2 id="dependability-costs"><a class="header" href="#dependability-costs">Dependability Costs</a></h2>
<p>Dependability costs tend to increase exponentially as increasing levels of dependability
are required. There are two main reasons behind this:</p>
<ol>
<li>The use of more expensive development techniques and hardware that are required
to achieve the higher levels of dependability.</li>
<li>The increased testing and system validation that is required to convince the 
system client and regulators that the required levels of dependability have been
achieved.</li>
</ol>
<p>Due to the very high costs of dependability achievement, it may be more cost effective
to accept untrustworthy systems and pay for failure costs.</p>
<h2 id="causes-of-failure"><a class="header" href="#causes-of-failure">Causes of Failure</a></h2>
<ul>
<li><strong>Hardware failure</strong>: Hardware fails because of design and manufacturing errors 
or because components have reached the end of their natural life.</li>
<li><strong>Software failure</strong>: Software fails due to errors in its specification, design
or implementation.</li>
<li><strong>Operational failure</strong>: Human operators make mistakes. They are currently perhaps 
the largest single cause of system failures in socio-technical systems.</li>
</ul>
<h2 id="hazards-and-risks"><a class="header" href="#hazards-and-risks">Hazards and Risks</a></h2>
<p>A hazard is anything that may cause harm. Hazard analysis attempts to identify
all the dangerous states. A risk is the combination of the probability that the
hazard will lead to an accident and the likely severity of the accident if it occurs.
For each hazard, the risk is assessed and if the risk is not acceptable but can
be made tolerable, measures must be introduces to reduce it.</p>
<h2 id="faults-and-failures"><a class="header" href="#faults-and-failures">Faults and Failures</a></h2>
<p>A fault is an abnormal condition/defect that may lead to failure. A failure is 
the inability of the component, subsystem, or system to perform its intended function
as designed. A failure may be the resut of one or more faults.</p>
<p>Fault Tree Analysis (FTA) considers how a failure may arise. Failure Modes and
Effects Analysis (FMEA) analyses the ways in which each component could fail, and
considers the effect this will have on the system.</p>
<h2 id="safety-standards"><a class="header" href="#safety-standards">Safety Standards</a></h2>
<p>Below are a few commonly used standards. All standards are process based. Process
alone does not guarantee quality however, they can only help reduce the risk.</p>
<div class="table-wrapper"><table><thead><tr><th>Standard</th><th>Purpose</th><th>Sector</th></tr></thead><tbody>
<tr><td>ISO9001</td><td>General quality management system.</td><td>All</td></tr>
<tr><td>ISO27001</td><td>Information security standard.</td><td>All</td></tr>
<tr><td>ISO13485</td><td>Quality management system.</td><td>Medical</td></tr>
<tr><td>IEC61508</td><td>Functional safety.</td><td>All</td></tr>
<tr><td>IEC62304</td><td>Software lifecycle.</td><td>Medical</td></tr>
<tr><td>ISO14971</td><td>Risk management.</td><td>Medical</td></tr>
<tr><td>FDA GMP</td><td>Quality system regulation.</td><td>Medical</td></tr>
<tr><td>ISO/TR80002</td><td>Application of 14971 to medical device software.</td><td>Medical</td></tr>
<tr><td>Def-Stan 55/56</td><td>Procurement of safety critical software.</td><td>Defence</td></tr>
<tr><td>IEC80001</td><td>Risk management - IT networks.</td><td>Medical</td></tr>
<tr><td>IEC60601</td><td>Requirements for safety.</td><td>Medical</td></tr>
<tr><td>ISO26262</td><td>Automative software safety.</td><td>Automotive</td></tr>
</tbody></table>
</div>
<p>IEC61508 is an umbrella standard for functional safety across all industries.
Compliance to IEC61508 ensures compliance with industry specific safety standards.
IEC61508 has the following views on risks:</p>
<ul>
<li>Zero risk can never be reached, only probabilities can be reduced.</li>
<li>Non-tolerable risks must be reduced (ALARP - as low as reasonably possible).</li>
<li>Optimal, cost effective safety is achieved when addressed in the entire safety
lifecycle.</li>
</ul>
<p>The IEC61508 standard defines three successive tiers of safety assessment:</p>
<ul>
<li>Safety Instrumented System (SIS): The entire system.</li>
<li>Safety Instrumented Functions (SIF): A singular component.</li>
<li>Safety Integrity Level (SIL): The safety integrity level of a specific SIF which
is being implemented by an SIS.</li>
</ul>
<h2 id="functional-safety"><a class="header" href="#functional-safety">Functional Safety</a></h2>
<p>The functional safety goal is the goal that an automatic safety function will perform
the intended function correctly or the system will fail in a predictable (safe)
manner. </p>
<p>It will either:</p>
<ul>
<li>perform the intended function correctly (reliable)</li>
<li>or, fail in a predictable manner (safe)</li>
</ul>
<h2 id="real-time-operating-system-rtos-areas-of-concern"><a class="header" href="#real-time-operating-system-rtos-areas-of-concern">Real-time Operating System (RTOS) Areas of Concern</a></h2>
<ul>
<li>Tasking:
<ul>
<li>Task terminates or is deleted.</li>
<li>Overflow of Kernel's storage area for task control blocks.</li>
<li>Task stack size is exceeded.</li>
</ul>
</li>
<li>Scheduling:
<ul>
<li>Deadlocks.</li>
<li>Tasks spawn additional tasks that starve CPU resources.</li>
<li>Service calls with unbounded execution times.</li>
</ul>
</li>
<li>Memory and I/O device access:
<ul>
<li>An incorrect pointer referencing/dereferncing.</li>
<li>Data overwrite.</li>
<li>Unauthorised access to critical system devices.</li>
</ul>
</li>
<li>Queueing:
<ul>
<li>Overflow of Kernel work queue.</li>
<li>Task queue.</li>
<li>Message queue.</li>
</ul>
</li>
<li>Interrupts and exceptions:
<ul>
<li>No interrupt handler.</li>
<li>No exception handler.</li>
<li>Improper protection of supervisor task.</li>
</ul>
</li>
</ul>
<h2 id="software-planning-process"><a class="header" href="#software-planning-process">Software Planning Process</a></h2>
<p>The purpose of software planning is to determine what will be done to produce
safe, requirements-based software.</p>
<p>The expected outputs are:</p>
<ul>
<li>A plan for Software Aspects of Certification (PSAC).</li>
<li>Software development plan.</li>
<li>Software verification plan.</li>
<li>Software configuration management plan.</li>
<li>Software quality assurance plan.</li>
</ul>
<p>The software development process is broken down into four sub-processes:</p>
<ul>
<li><strong>Software requirement process</strong>: High-level requirements in relation to function,
performance, interface, and safety.</li>
<li><strong>Software design process</strong>: Low-level requirements used to implement the source
code.</li>
<li><strong>Software coding process</strong>: Production of source-code from the design process.</li>
<li><strong>Integration process</strong>: Integration of code into a real-time environment.</li>
</ul>
<p>The following tangible outputs are the result of the combined four sub-processes:</p>
<ul>
<li>Software requirements data.</li>
<li>Software design description.</li>
<li>Source code.</li>
<li>Executable object code.</li>
</ul>
<h2 id="c-coding-standards"><a class="header" href="#c-coding-standards">C Coding Standards</a></h2>
<div class="table-wrapper"><table><thead><tr><th>Coding Standard</th><th>C Standard</th><th>Security Standard</th><th>Safety Standard</th><th>International Standard</th><th>Whole Language</th></tr></thead><tbody>
<tr><td>CWE</td><td>None/All</td><td>Yes</td><td>No</td><td>No</td><td>N/A</td></tr>
<tr><td>MISRA 2012 Amendment 2</td><td>C99/C11/C18</td><td>No</td><td>Yes</td><td>No</td><td>No</td></tr>
<tr><td>CERT C</td><td>C99/C11</td><td>Yes</td><td>No</td><td>No</td><td>Yes</td></tr>
<tr><td>ISO/IEC TS 17961</td><td>C11</td><td>Yes</td><td>No</td><td>Yes</td><td>Yes</td></tr>
</tbody></table>
</div>
<h2 id="misra-c"><a class="header" href="#misra-c">MISRA C</a></h2>
<p>MISRA - The Motor Industry Software Reliability Association - provides coding
standards for developing safety-critical systems. MISRA C is a set of software
development guidelines for the C programming language developed by The MISRA Consortium.
It is not for finding bugs, rather for preventing unsafe coding habits.</p>
<p>Although originating from the automotive industry, it has evolved as a widely 
accepted model for best practices by leading developers in sectors including
automotive, aerospace, telecom, medical devices, defense, railway, and more.</p>
<p>MISRA C has trhee categories of guidelines:</p>
<ol>
<li><strong>Mandatory</strong>: You must follow these, no exceptions permitted.</li>
<li><strong>Required</strong>: You must follow these but there can be execptions in certain cases.</li>
<li><strong>Advisory</strong>: You must try to follow these but they are not mandatory.</li>
</ol>
<p>The guidelines provided by MISRA C are not &quot;you should not do that&quot; but &quot;this is
dangerous, you may only do that if it is needed and is safe to do so&quot;. Therefore,
the deviation process is an essential part of MISRA C. Violation of a guideline
does not necessarily mean a software error. For example, there is nothing wrong
about converting an integer constant to a pointer when it is necessary to address 
memory mapped registers or other hardware features. However, such conversions
are implementation-defined and have undefined behaviours, so Rule 11.4 suggests
avoiding them everywhere apart from the very specific instances where they are
both required and safe.</p>
<p>For example, here are some safe coding practices in ISO 26262-6:2018</p>
<ul>
<li>One entry and one exit point in sub-programs and functions.</li>
<li>No dynamic objects or variables, or else online test during their creation.</li>
<li>Initialisation of variables.</li>
<li>No multiple use of variable names.</li>
<li>Avoid global variables or else justify their usage.</li>
<li>Restricted use of pointers.</li>
<li>No implicit type conversions.</li>
<li>No hidden data flow or control flow.</li>
<li>No unconditional jumps.</li>
<li>No recursions.</li>
</ul>
<h2 id="nasa---the-power-of-10-rules-for-developing-safety-critical-code"><a class="header" href="#nasa---the-power-of-10-rules-for-developing-safety-critical-code">NASA - The power of 10: Rules for developing safety-critical code</a></h2>
<ol>
<li>Avoid complex flow constructs such as <code>goto</code> and recursion.</li>
<li>All loops must have fixed bounds. This prevents runaway code.</li>
<li>Avoid heap memory allocation, e.g. do not use <code>malloc</code>.</li>
<li>Restrict functions to a single printed page.</li>
<li>Use a minimum of two runtime assertions per function.</li>
<li>Restrict the scope of data to the smallest possible.</li>
<li>Check the return value of all non-void functions, or cast to void to indicate
the return value is useless.</li>
<li>Use the pre-processor sparingly, e.g. do not use <code>stdio.h</code>, <code>local.h</code>, 
<code>abort()</code>/<code>exit()</code>/<code>system()</code> from <code>stdlib.h</code>, time handling from <code>time.h</code>, etc.</li>
<li>Limit pointer use to single dereference, and do not use function pointers.</li>
<li>Compile with all possible warnings active; all warnings should then be addressed
before release of the software.</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="week-7"><a class="header" href="#week-7">Week 7</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="distributed-systems"><a class="header" href="#distributed-systems">Distributed Systems</a></h1>
<p>A distributed system is a collection of processors that do not share memory or
a clock. Instead, each node has its own local memory. The nodes communicate
with one another through various networks, such as high-speed buses.</p>
<h2 id="advantages-of-distributed-systems"><a class="header" href="#advantages-of-distributed-systems">Advantages of Distributed Systems</a></h2>
<p>A distributed system is a collection of loosely coupled nodes interconnected by
a communication network. From the point of view of a specific node in a distributed
system, the rest of the nodes and their respective resources are remote, whereas
its own resources are local.</p>
<p>Nodes can exist in a client-server configuration, a peer-to-peer configuration,
or a hybrid of these. In the common client-server configuration, one node at one
site, the server, has a resource that another node, the client (or user), would 
like to use. In a peer-to-peer configuration,  there are no servers or clients.
Instead, the nodes share equal responsibilities and can act as both clients and
servers.</p>
<h3 id="resource-sharing"><a class="header" href="#resource-sharing">Resource Sharing</a></h3>
<p>If a number of different sites are connected to one another, then a user at one
site may be able to use the resources available at another. For example, a user
at site A may query a database located at site B. Much like a user at site B may
access a file that resides at site A.</p>
<h3 id="computation-speedup"><a class="header" href="#computation-speedup">Computation Speedup</a></h3>
<p>If a particular computation can be partitioned into sub-computations that can 
run concurrently, then a distributed system allows us to distribute the sub-computations
among the various sites. The sub-computations can be run concurrently and thus 
provide computation speedup.</p>
<p>In addition, if a particular site is currently overloaded with requests, some
of them can be moved or re-routed to other, more lighly loaded sites. This movement
of jobs is called load balancing and is common among distributed systems and other
services provided on the internet.</p>
<h3 id="reliability"><a class="header" href="#reliability">Reliability</a></h3>
<p>If one site fails in a distributed system, the remaining sites can continue operating,
giving the system better reliability. If the system is composed of multiple large
autonomous installations, the failure of one of them should not affect the rest. 
If, however, the system is composed of diversified machines, each of which is
responsible for some crucial system function, then a single failure may halt the
operation of the whole system.</p>
<p>The failure of a node or site must be detected by the system, and appropriate
action may be needed to recover from the failure. The system must no longer use
the services of that site. In addition, if the function of the failed site can 
be taken over by another site, the system must ensure that the transfer of function
occurs correctly. Finally, when the failed site recovers or is repaired, mechanisms
must be available to integrate it back into the system smoothly.</p>
<h2 id="network-structure"><a class="header" href="#network-structure">Network Structure</a></h2>
<p>There are two types of networks: local-area networks (LAN) and wide-area networks 
(WAN). The main difference between the two is the way in which they are geographically
distributed. Local-area networks are composed of hosts distributed over small areas,
whereas wide-area networks are composed of systems distributed over a large arrea.</p>
<h3 id="local-area-networks"><a class="header" href="#local-area-networks">Local-Area Networks</a></h3>
<p>LANs are usually designed to cover a small geographical area, and they are generally
used in an office or home environment. All the sites in such systems are close
to one another, so the communication links tend to have higher speed and lower
error rate than their counterparts in wide-area networks.</p>
<p>A typical LAN may consist of a number of different computers, various shared
peripheral devices, and one or more routers that provide access to other networks.
Ethernet and WiFi are commonly used to construct LANs. Wireless access points connect
devices to the LAN wirelessly, and they may or may not be routers themselves.</p>
<p>Ethernet networks use coaxial, twisted pair, and/or fiber optic cables to send
signals. An Ethernet network has no central controller, because it is a multiaccess
bus, so new hosts can be added easily into the network. The Ethernet protocol is
defined by the IEEE 802.3 standard. Typical Ethernet speeds using common twisted-pair
cabling can vary from 10Mbps to over 10Gbps, with other types of cabling reaching
speeds of 100Gbps.</p>
<p>WiFi is now ubiquitous and either supplements traditional Ethernet networks or
exist by itself. Specifically, WiFi allows us to construct a network without
using physical cables. Each host has a wireless transmitter and receiver that
it uses to participate in the network. WiFi is defined by the IEEE 802.11 standard.
WiFi speeds can vary from 11Mbps to over 400Mbps.</p>
<h3 id="wide-area-networks"><a class="header" href="#wide-area-networks">Wide-Area Networks</a></h3>
<p>Sites in WAN are physically distributed over a large geographical area. Typical
links are telephone lines, leased lines, optical cable, microwave links, radio
waves, and satellite channels. These communication links are controlled by routers
that are responisble for directing traffic to other routers and networks and transferring
information among the various sites. </p>
<p>The first WAN to be designed and developed was the ARPANET. The ARPANET has grown 
from a four-site experimental network to a worldwide network of networks, the Internet, 
comprising millions of computer systems. There are, of course, other WANs besides 
the Internet. A company may, for example, create its own private WAN for increased 
security, performance, or reliability.</p>
<p>WANs are generally slower than LANs, although backbone WAN connections that link
major cities may have very fast transfer rates through fiber optic cables.</p>
<p>Frequently, WANs and LANs interconnect, and it is difficult to tell where one
ends and the other start. Consider the cellular phone data network. Cell phones
are used for both voice and data communications. Cell phones in a given area
connect via radio waves to a cell tower that contains receivers and transmitters.
This part of the network is similar to a LAN except that the cell phones do not
communicate with each other. Rather, the towers are connected to other towers
and to hubs that connect the tower communications to land lines or other communication
media and route the packets towards their destination. This part of the network
is more WAN-like.</p>
<h2 id="communication-structure"><a class="header" href="#communication-structure">Communication Structure</a></h2>
<h3 id="naming-and-name-resolution"><a class="header" href="#naming-and-name-resolution">Naming and Name Resolution</a></h3>
<p>The first issue in network communication involves the naming of the systems in
the network. For a process at site A to exchange information with a process at
site B, each must be able to specify the other. Within a computer system, each
process has a process identifier, and messages may be addressed with the process
identifier. Because networked systems share no memory, however, a host within
the system initially has no knowledge about the processes on other hosts.</p>
<p>To solve this problem, processes on remote systems are generally identified by
the pair &lt;host name, identifier&gt;, where host name is a name unique within the
network and identifier is a process idntifier or other unique number within that
host. A host name is usually an alphanumeric identifier, rather than a number,
to make it easier for users to specify. For instance, site A might have hosts 
named &quot;program&quot;, &quot;student&quot;, &quot;faculty&quot;, and &quot;cs&quot;. The host name &quot;program&quot; is certianly
easier to remember than the numeric host address 128.148.31.100.</p>
<p>Names are convenient for humans to use, but computers prefer numbers for speed
and simplicity. For this reason, there must be a mechanism to resolve the host
name into a host-id that describes the destination system to the networking 
hardware. The internet uses a domain-name system (DNS) for host-name resolution.</p>
<p>DNS specifies the naming structure of the hosts, as well as name-to-address resolution.
Hosts on the Internet are logically addressed with multipart names known as IP 
addresses. The parts of the IP address progress form most specific to the most
general, with periods separating the fields. For instance, <em>eric.cs.yale.edu</em> refers
to the host <em>eric</em> in the Department of Computer Science at Yale University within
the top-level domain <em>edu</em>. Each component has a name server - simply a process
on a system - that accepts a name and returns the address of the name server responsible
for that name.</p>
<h3 id="routing-stategies"><a class="header" href="#routing-stategies">Routing stategies</a></h3>
<ul>
<li><strong>Fixed routing</strong>: A path from A to B is specified in advance; The path then
only changes if a hardware failure disables it. Since the shortest path is usually 
chosen, communication costs are minimized. Fixed routing cannot adapt to load changes
however. Fixed routing ensures that messages will be delivered in the order in 
which they were sent.</li>
<li><strong>Virtual routing</strong>: A path from A to B is fixed for the duration of one session. 
Different sessions involving messages from A to B may have different paths. This 
is a partial remedy to adapting to load changes. Virtual routing ensures that 
messages will be delivered in the order in which they were sent.</li>
<li><strong>Dynamic routing</strong>: The path used to send a message from site A to site B is 
chosen only when a message is sent. Usually a site sends a message to another 
site on the link least used at that particular time. This method adapts to load 
changes by avoiding routing messages on heavily used path. One downside to dynamic
routing is that messages may arrive out of order. This problem can be remedied 
by appending a sequence number to each message. Dynamic routing is also the most
complex of the three to setup.</li>
</ul>
<h3 id="connection-stategies"><a class="header" href="#connection-stategies">Connection stategies</a></h3>
<ul>
<li><strong>Circuit switching</strong>: A permanent physical link is established for the duration 
of the communication (i.e., telephone system).</li>
<li><strong>Message switching</strong>: A temporary link is established for the duration of one 
message transfer (i.e., post-office mailing system).</li>
<li><strong>Packet switching</strong>: Messages of variable length are divided into fixed-length 
packets which are sent to the destination. Each packet may take a different path 
through the network. The packets must be reassembled into messages as they arrive.</li>
</ul>
<p>Circuit switching requires setup time, but incurs less overhead for shipping each 
message. Circuit switching may waste network bandwidth however. Message and packet 
switching require less setup time, but incur more overhead per message.</p>
<h2 id="network-and-distributed-operating-systems"><a class="header" href="#network-and-distributed-operating-systems">Network and Distributed Operating Systems</a></h2>
<h3 id="network-operating-system"><a class="header" href="#network-operating-system">Network Operating System</a></h3>
<p>A network operating system provides an environment in which users can access remote
resources by either logging in to the appropriate remote machine or transferring
data form the remote machine to their own machines.</p>
<p>There are two major functions of a network operating system:</p>
<ol>
<li><strong>Remote login</strong>: Users can remotely login to a machine. This can be done via the
<code>ssh</code> facility. for example, suppose that a user at Westminster College wishes
to compute on <code>kristen.cs.yale.edu</code>, a computer located at Yale University. To
do so, the user must have a valid account on that machine. To log in remotely, 
the user can issue the command <code>ssh kristen.cs.yale.edu</code>.</li>
<li><strong>Remote file transfer</strong>: A way for users to transfer files from one machine
to another remotely. The Internet provides a mechanism for such a transfer with
the file transfer protocol (FTP) and the more private secure file transfer protocol
(SFTP).</li>
</ol>
<h3 id="distributed-operating-system"><a class="header" href="#distributed-operating-system">Distributed Operating System</a></h3>
<p>In a distributed operating system, users access remote resources in the same way
they access local resources. Data and process migration from one site to another
is under the control of the distributed operating system.</p>
<h4 id="data-migration"><a class="header" href="#data-migration">Data Migration</a></h4>
<p>The system can transfer data by one of two basic methods. The first approach to
data migration is ot transfer the entire file to site A. From that point on, 
all access to the file is local. When the user no longer needs access to the file, 
a copy of the file is sent back to site B.</p>
<h4 id="computation-migration"><a class="header" href="#computation-migration">Computation Migration</a></h4>
<p>In some circumstances, we may want to transfer the computation, rather than the 
data, across the system; this process is called computation migation. For example,
consider a job that needs to access various large files that reside at different
sites, to obtain a summary of those files. It would be more efficient to access
the files at the sites where they reside and return the desired results to the
site that initated the computation. This can be achieved via remote procedure 
calls (RPCs) or via a messaging system.</p>
<h4 id="process-migration"><a class="header" href="#process-migration">Process Migration</a></h4>
<p>When a process is submitted for execution, it is not always executed at the site
at which it is initiated. The entire process, or parts of it, may be executed at
different sites. This scheme may be used for several reasons:</p>
<ul>
<li><strong>Load balancing</strong>: The processes (or subprocesses) may be distributed across
the sites to even the workload.</li>
<li><strong>Computation speedup</strong>: If a single process can be divided into a number of 
subprocesses that can run concurrently on different sites or nodes, then the total
process turnaround time can be reduced.</li>
<li><strong>Hardware preference</strong>: The process may have characteristics that make it more 
suitable for execution on some specialized processor (such as matrix inversion 
on a GPU) than on a microprocessor.</li>
<li><strong>Software preference</strong>: Th eprocess may require software that is availableat 
only a particular site, and either the software cannot be moved, or it is less 
expensive to move the process.</li>
<li><strong>Data access</strong>: Just as in computation migration, if the data being used in 
the computation are numerous, it may be more efficient to have a process run 
remotely (say, on a server that hosts a large database) than to transfer all the 
data and run the process locally.</li>
</ul>
<h2 id="design-issues-in-distributed-systems"><a class="header" href="#design-issues-in-distributed-systems">Design Issues in Distributed Systems</a></h2>
<p>The designers of distributed systems must take a number of design challenges into
account. The system should be robust so that it can withstand failures. The system
should also be transparent to users in terms of both file location and user mobility.
Finally, the system should be scalable to allow the addition of more computation
power, more storage, more users.</p>
<ul>
<li><strong>Transparency</strong>: The distributed system should appear as a conventional, 
centralized system to the user.</li>
<li><strong>Fault tolerance</strong>: The distributed system should continue to function in
the face of failure.</li>
<li><strong>Scalability</strong>: As demands increase, the system should easily accept
the addition of new resources to accommodate the increased demand.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="sockets"><a class="header" href="#sockets">Sockets</a></h1>
<p>A socket is an abstract representation of an &quot;endpoint for communication&quot;. They
can be either:</p>
<ul>
<li>Connection based or connectionless.</li>
<li>Packet based or stream based.</li>
<li>Reliable or unreliable.</li>
</ul>
<p>Sockets are characterised by their domain, type, and transport protocol. 
There are two types of sockets:</p>
<ol>
<li>Connection-based sockets communicate client-server: The server waits for a
connection from the client.</li>
<li>Connectionless sockets are peer-to-peer: Each process is symmetric.</li>
</ol>
<p>Common domains consist of:</p>
<ul>
<li><strong>AF_UNIX</strong>: Address format is UNIX path-name.</li>
<li><strong>AF_INET</strong>: Address format is host and port number.</li>
</ul>
<p>Common types consist of:</p>
<ul>
<li><strong>Virtual circuit</strong>: Received in order transmitted and reliably.</li>
<li><strong>Datagram</strong>: Arbitrary order and unreliable.</li>
</ul>
<p>Common transport protocols consist of:</p>
<ul>
<li>TCP/IP (virtual circuit)</li>
<li>UDP (datagram)</li>
</ul>
<h2 id="bsd-socket-apis"><a class="header" href="#bsd-socket-apis">BSD Socket APIs</a></h2>
<ul>
<li><code>socket()</code>: Creates a socket of a given domain, type, and protocol.</li>
<li><code>bind()</code>: Assigns a name to the socket.</li>
<li><code>listen()</code>: Specifies the number of pending connections that can be queued for
a server socket.</li>
<li><code>accept()</code>: Server accepts a connection request from a client.</li>
<li><code>connect()</code>: Client requests a connection to a server.</li>
<li><code>send(), sendto</code>: Write to connection.</li>
<li><code>recv(), recvfrom</code>: Read from connection.</li>
<li><code>shutdown()</code>: End sending or receiving.</li>
<li><code>close()</code>: Close a socket and terminate a TCP connection.</li>
</ul>
<h2 id="connection-based-sockets"><a class="header" href="#connection-based-sockets">Connection-based Sockets</a></h2>
<p>With connection-based sockets, the server performs the following actions:</p>
<ul>
<li><code>socket()</code></li>
<li><code>bind()</code></li>
<li><code>listen()</code></li>
<li><code>accept()</code></li>
<li><code>send()</code></li>
<li><code>recv()</code></li>
<li><code>shutdown()</code></li>
<li><code>close()</code></li>
</ul>
<p>while the client performs the following actions:</p>
<ul>
<li><code>socket()</code></li>
<li><code>connect()</code></li>
<li><code>send()</code></li>
<li><code>recv()</code></li>
<li><code>shutdown()</code></li>
<li><code>close()</code></li>
</ul>
<h2 id="connectionless-sockets"><a class="header" href="#connectionless-sockets">Connectionless Sockets</a></h2>
<p>Due to communication being symmetric, all devices perform the following actions:</p>
<ul>
<li><code>socket()</code></li>
<li><code>bind()</code></li>
<li><code>sendto()</code></li>
<li><code>recvfrom()</code></li>
<li><code>shutdown()</code></li>
<li><code>close()</code></li>
</ul>
<h2 id="example"><a class="header" href="#example">Example</a></h2>
<pre><code class="language-c">// Server
#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
#include &lt;string.h&gt;
#include &lt;unistd.h&gt;
#include &lt;sys/types.h&gt;
#include &lt;sys/socket.h&gt;
#include &lt;netinet/in.h&gt;

const int NUM_OF_CONNECTIONS = 10;

int main(void) {
    struct sockaddr_in server_addr;
    struct sockaddr client_addr;
    socklen_t client_addr_len;
    char buf[1024];

    int fd = socket(AF_INET, SOCK_STREAM, 0);

    if (fd == -1) {
        fprintf(stderr, &quot;[Error] - Failed to create socket.\n&quot;);
        return 1;
    }

    server_addr.sin_family = AF_INET;
    // Bind socket so it's able to be connected to from anywhere
    server_addr.sin_addr.s_addr = htonl(INADDR_ANY);
    // Bind socket to port 3000
    server_addr.sin_port = htons(3000);

    if (bind(fd, (struct sockaddr *) &amp;server_addr, sizeof(server_addr)) == -1) {
        fprintf(stderr, &quot;[Error] - Failed to bind socket.\n&quot;);
        return 1;
    }

    if (listen(fd, NUM_OF_CONNECTIONS) == -1) {
        fprintf(stderr, &quot;[Error] - Failed to listen for connections.\n&quot;);
        return 1;
    }

    int client_fd = accept(fd, &amp;client_addr, &amp;client_addr_len);

    if (client_fd == -1) {
        fprintf(stderr, &quot;[Error] - Failed to accept connection.\n&quot;);
        return 1;
    }

    // Do something with socket
    int bytes_received = recv(client_fd, buf, 1023, 0);

    if (bytes_received == -1) {
        fprintf(stderr, &quot;[Error] - Failed to receive data.\n&quot;);
        return 1;
    }

    buf[bytes_received] = '\0';
    printf(&quot;Received from client: %s\n&quot;, buf);

    if (shutdown(client_fd, SHUT_RDWR) == -1) {
        fprintf(stderr, &quot;[Error] - Failed to shutdown connection.\n&quot;);
        return 1;
    }

    close(client_fd);

    close(fd);

    return 0;
}
</code></pre>
<pre><code class="language-c">// Client
#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
#include &lt;string.h&gt;
#include &lt;unistd.h&gt;
#include &lt;sys/types.h&gt;
#include &lt;sys/socket.h&gt;
#include &lt;netinet/in.h&gt;
#include &lt;arpa/inet.h&gt;

const int NUM_OF_CONNECTIONS = 10;

int main(void) {
    struct sockaddr_in server_addr;
    struct sockaddr client_addr;
    socklen_t client_addr_len;
    char buf[1024];
    char *message = &quot;Hello, world!\n&quot;;

    int fd = socket(AF_INET, SOCK_STREAM, 0);

    if (fd == -1) {
        fprintf(stderr, &quot;[Error] - Failed to create socket.\n&quot;);
        return 1;
    }

    server_addr.sin_family = AF_INET;
    if (inet_pton(AF_INET, &quot;127.0.0.1&quot;, &amp;server_addr.sin_addr) != 1) {
        fprintf(stderr, &quot;[Error] - Failed to convert presentation format address to network format.\n&quot;);
        return 1;
    }
    server_addr.sin_port = htons(3000);

    if (connect(fd, (struct sockaddr *) &amp;server_add, sizeof(server_addr)) == -1) {
        fprintf(stderr, &quot;[Error] - Failed to connect to server.\n&quot;);
        return 1;
    }

    send(fd, message, strlen(message), 0);

    if (shutdown(client_fd, SHUT_RDWR) == -1) {
        fprintf(stderr, &quot;[Error] - Failed to shutdown connection.\n&quot;);
        return 1;
    }

    close(fd);

    return 0;
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="week-8"><a class="header" href="#week-8">Week 8</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="cpu-scheduling"><a class="header" href="#cpu-scheduling">CPU Scheduling</a></h1>
<p>In a system with a single CPU core, only one process can run at a time. A process
is executed until it must wait. With multi-pogramming, multiple processes are
kept in memory at one time. When one process has to wait, the OS takes the CPU
away from that process and gives the CPU to another process. This selection process
is caried out by the CPU schedule. It's important to note that the queue of ready
items is not necessarily a FIFO queue. The records in the queue are typically
pocess control blocks (PCBs) of the processes.</p>
<p>CPU scheduling decisions may take place under the following four circumstances:</p>
<ol>
<li>When a process switches from the running state to the waiting state.</li>
<li>When a pocess switches fom the running state to the ready state.</li>
<li>When a process switches from the waiting state to the ready state.</li>
<li>When a process terminates.</li>
</ol>
<p>When scheduling takes place under circumstances 1 and 4, the scheduling scheme
is non-preemptive or cooperative, otherwise, it is preemptive.</p>
<p>Under non-preemptive scheduling, once the CPU has been allocated to a process,
the process keeps the CPU until it releases it either by termination or by switching 
to the waiting state. The majority of modern operating systems use non-preemptive
scheduling algorithms. Preemptive scheduling can however, result in race conditions 
when data is shared among several processes. </p>
<p>A non-preemptive kernel will wait for a system call to complete or for a process 
to block while waiting for I/O to complete to take place before doing a context 
switch. A preemptive kernel requires mechanisms such as mutex locks to prevent
race conditions when accessing shaed kernel data structues.</p>
<p>Due to interrupts being able to occur at any time, and becuase they cannot always
be ignored by the kernel, sections affected by interrupts must be guarded from
simultaneous use. So that these sections of code are not accessed concurrently 
by several processes, they disable interupts at entry and re-enable them at exit.</p>
<h2 id="dispatcher"><a class="header" href="#dispatcher">Dispatcher</a></h2>
<p>A dispatcher is a module that gives control of the CPUs core to a process selected
by the CPU scheduler. This involves tasks such as:</p>
<ul>
<li>Switching context from one process to another.</li>
<li>Switching to user mode.</li>
<li>Jumping to the proper location in the user program to resume that program.</li>
</ul>
<p>Due to the dispatcher being invoked during every context switch it must be fast.
The time it takes for the dispatcher to stop one process and start another is
known as dispatch latency.</p>
<p><img src="week_8/../media/dispatch_latency.png" alt="Figure: The role of the dispatcher" /><br />
<strong>Figure: The role of the dispatcher</strong></p>
<p>A voluntary context switch occurs when a process has given up contol of the CPU
because it requires a resource that is currently unavailable. A non-voluntary
context switch occurs when the CPU has been taken away from a process. This can
occur when its time slice has expired, its been preempted by a higher-priority
process, and more.</p>
<p>Using the <code>/proc</code> file system, the number of context switches for a given process
can be detemined. For example, the contents of the file <code>/proc/2166/status</code> provides
the following trimmed output:</p>
<pre><code>voluntary_ctxt_switches       150
nonvoluntary_ctxt_swtiches    8
</code></pre>
<p>The Linux command <code>vmstat</code> can also be used to see the number of context switches
on a system-wide level.</p>
<h2 id="scheduling-criteria"><a class="header" href="#scheduling-criteria">Scheduling Criteria</a></h2>
<p>Different CPU scheduling algorithms have diffeernt properties and the choice
of a particular algorithm may favour one class of process over another.</p>
<p>Many critera have been suggested for comparing CPU scheduling alogrithms:</p>
<ul>
<li><strong>CPU utilisation</strong>: We want to keep the CPU as busy as possible. Conceptually, 
CPU utilization can range from 0 to 100 percent. In a real system, it should range 
from 40 percent (for a lightly loaded system) to 90 percent (for a heavily loaded 
system).</li>
<li><strong>Throughput</strong>: If the CPU is busy executing processes, then work is being done. 
One measure of work is the number of processes that are completed per time unit, 
called throughput.</li>
<li><strong>Turn-around time</strong>: From the point of view of a particular process, the 
important criterion is how long it takes to execute that process. The interval 
from the time of submission of a process to the time of completion is the turnaround 
time. Turnaround time is the sum of the periods spent waiting in the ready queue, 
executing on the CPU, and doing I/O.</li>
<li><strong>Waiting time</strong>: The CPU scheduling algorithm does not affect the amount of 
time during which a process executes or does I/O. It only affects the amount of 
time that a process spends waiting in the ready queue. Waiting time is the sum 
of the periods spent waiting in the ready queue.</li>
<li><strong>Response time</strong>: In an interactive system, turnaround time may not be the 
best criterion. Often, a process can produce some output fairly early and can 
continue computing new results while previous results are being output to the 
user. Thus, another measure is the time from the submission of a request until 
the first response is produced. This measure, called response time, is the time 
it takes to start responding, not the time it takes to output the response.</li>
</ul>
<p>In general, it is desirable to maximise CPU utilisation and thoughput, but 
minimise turnaround time, waiting time, and response time. In some cases however,
we may prefer to optimize the minimum or maximum values rather than the average.</p>
<h2 id="cpu-scheduling-alogrithms"><a class="header" href="#cpu-scheduling-alogrithms">CPU Scheduling Alogrithms</a></h2>
<ul>
<li><strong>First-Come, First-Served Scheduling</strong>: First-come, first-served (FCFS) scheduling
is the simplest scheduling algorithm, but it can cause short processes to wait 
for very long processes.</li>
<li><strong>Shortest-Job-First Scheduling</strong>: Shortest-job-first (SJF) scheduling is provably
optimal, providing the shortest average waiting time. Implementing SJF scheduling 
is difficult, how- ever, because predicting the length of the next CPU burst is 
difficult.</li>
<li><strong>Round-Robin Scheduling</strong>: Round-robin (RR) scheduling allocates the CPU to 
each process for a time quantum. If the process does not relinquish the CPU before 
its time quantum expires, the process is preempted, and another process is scheduled 
to run for a time quantum.</li>
<li><strong>Priority Scheduling</strong>: Priority scheduling assigns each process a priority,
and the CPU is allocated to the process with the highest priority. Processes with 
the same priority can be scheduled in FCFS order or using RR scheduling.</li>
<li><strong>Multilevel Queue Scheduling</strong>: Multilevel queue scheduling partitions processes 
into several separate queues arranged by priority, and the scheduler executes the 
processes in the highest-priority queue. Different scheduling algorithms may be 
used in each queue.</li>
<li><strong>Multilevel Feedback Queue Scheduling</strong>: Multilevel feedback queues are similar
to multilevel queues, except that a process may migrate between different queues.</li>
</ul>
<h2 id="thread-scheduling"><a class="header" href="#thread-scheduling">Thread Scheduling</a></h2>
<p>On systems implementing the many-to-one and many-to-many models for thread management,
the thread library schedules user-level threads to run on an available lightweight
process (LPW). This scheme is known as process-contention scope (PCS) as competition
for the CPU takes place among threads belonging to the same process. To determine
which kernel-level thread to schedule onto a CPU, the kenel uses system-contention
scope (SCS). Competition for the CPU with SCS scheduling takes place among all 
threads in the system. Systems that use the one-to-one model schedule threads
use only SCS.</p>
<p>Typically, PCS is done according to priority. User-level thread priorities
are set by the programmer and are not adjusted by the thread libray. PCS will
typically preempt the thread currently running in favor of a higher-priority 
thread; however, there is no guarantee of time slicing among threads of equal
priority.</p>
<p>Pthreads identifies the following contention scope values:</p>
<ul>
<li><code>PTHREAD_SCOPE_PROCESS</code> schedules threads using PCS scheduling.</li>
<li><code>PTHREAD_SCOPE_SYSTEM</code> schedules threads using SCS scheduling.</li>
</ul>
<p>On systems implementing the many-to-many model, the <code>PTHREAD_SCOPE_PROCESS</code> policy
schedules user-level threads onto available LWPs. The <code>PTHREAD_SCOPE_SYSTEM</code> 
scheduling policy will create and bind an LWP for each user-level thread on
many-to-many systems. This effectively maps threads using the one-to-one policy.</p>
<p>The Pthread IPC provides two functions for setting and getting the contention
scope policy:</p>
<ul>
<li><code>pthread_attr_setscope(pthread_attr_t *attr, int scope)</code></li>
<li><code>pthread_attr_getscope(pthread_attr_t *attr, int *scope)</code></li>
</ul>
<p>Below is an example program that will first determine the existing contention 
scope and set it to <code>PTHREAD_SCOPE_SYSTEM</code>. It will then create five separate
threads that will run using the SCS scheduling policy. It's important to note 
that on some systems, only certain contention scope values are allowed, i.e. 
Linux and macOS only allow <code>PTHREAD_SCOPE_SYSTEM</code>.</p>
<pre><code class="language-c">#include &lt;pthread.h&gt;
#include &lt;stdio.h&gt;

#define NUM_THREADS 5

void *runner(void *param);

int main(void) {
    int scope;
    pthread_t tid[NUM_THREADS];
    pthread_attr_t attr;

    pthread_attr_init(&amp;attr);

    if (pthread_attr_getscope(&amp;attr, &amp;scope) != 0) {
        fprintf(stderr, &quot;[Error] - Unable to get scheduling scope.\n&quot;);
    } else {
        if (scope == PTHREAD_SCOPE_PROCESS) {
            printf(&quot;PTHREAD_SCOPE_PROCESS\n&quot;);
        } else if (scope == PTHREAD_SCOPE_SYSTEM) {
            printf(&quot;PTHREAD_SCOPE_SYSTEM\n&quot;);
        } else {
            fprintf(stderr, &quot;[Error] - Illegal scope value.\n&quot;);
        }
    }

    pthread_attr_setscope(&amp;attr, PTHREAD_SCOPE_SYSTEM);

    for (size_t i = 0; i &lt; NUM_THREADS; i++) {
        pthread_create(&amp;tid[i], &amp;attr, runner, NULL);        
    }

    for (size_t i = 0; i &lt; NUM_THREADS; i++) {
        pthread_join(tid[i], NULL);
    }
}

void *runner(void *param) {
    // Do some work

    pthread_exit(0);
}
</code></pre>
<h2 id="multi-processor-scheduling"><a class="header" href="#multi-processor-scheduling">Multi-Processor Scheduling</a></h2>
<p>If multiple CPUs are available, load sharing, where multiple threads may run
in parallel, becomes possible, however scheduling issues become correspondingly
more complex.</p>
<p>Traditionally, the term multiprocessor reffered to systems that provided multiple
physical processors. However, the definition of multiprocessor now applies to the
following system architectures:</p>
<ul>
<li>Multicore CPUs</li>
<li>Multithreaded cores</li>
<li>NUMA systems</li>
<li>Hetorogeneous multiprocessing</li>
</ul>
<p>One approach to CPU scheduling in a multiprocessor system has all scheduling
decisions, I/O processing, and other system activites handled by a single processor
called the master server. The other processors execute only user code. This 
asymmetric multiprocessing is simple because only one core accesses the system
data structures, reducing the need for data sharing. The downfall for this approach
however is that the master server becomes a potential bottleneck.</p>
<p>The standard approach for supporting multiprocessors is symmetrical multiprocessing
(SMP), where each processor is self-scheduling. The scheduler for each process
examines the ready queue and selects a thread to run. This provides two possible
strategies for organising the threads eligible to be scheduled:</p>
<ol>
<li>All threads may be in a common ready queue.</li>
<li>Each process may have its own private queue of threads.</li>
</ol>
<p><img src="week_8/../media/ready-queue-organisation.png" alt="Figure: Organisation of ready queues." /><br />
<strong>Figure: Organisation of ready queues.</strong></p>
<p>If option one is chosen, a possible race condition on the shared ready queue
could occur and therefore must ensure that two spearate processors do not
choose to schedule the same thread and that threads are not lost from the queue.
To get around this, locking could be used to protect the common ready queue.
This is not a great solution however, as all access to the queue would require 
lock ownership therefore accessing the shared queue would likely be a performance
bottleneck.</p>
<p>The second option permits each processor to schedule threads from its private
run queue. This is the most common approach on systems supporting SMP as it does 
not suffer from the possible performance problems associated with a shared run 
queue. There are possible issues with per-processor run queues such as workloads
of varying size. This however, can be solved with balancing algorithms which
equalise workloads among all processors.</p>
<h2 id="multicore-processors"><a class="header" href="#multicore-processors">Multicore Processors</a></h2>
<p>Traditionally, SMP systems have allowed several processes to run in parallel by
providing multiple physical processors. However, most contempory computer hardware
now places multiple computing cores on the same physical chip resulting in a 
multicore processor. SMP systems that use multicore processors are faster and
consume less power than systems in which each CPU has its own physical chip.</p>
<p>Multicore processors however, may complicate scheduling issues. When a processor 
accesses memory, it spends a significant amount of time waiting for the data to 
become available. This is known as a memory stall and occurs primarily because
modern processors operate at much faster speeds than memory. A memory stall can
also occur because of a cache miss, the accessing of data that is not in cache 
memory. </p>
<p><img src="week_8/../media/memory-stall.png" alt="Figure: Memory stall." /><br />
<strong>Figure: Memory stall.</strong></p>
<p>To remedy this, many recent hardware designs have implemented multithreading
processing cores in which two, or more, hardware threads are assigned to each 
core. If one hardware thread stalls, the core can switch to another thread.</p>
<p><img src="week_8/../media/multithreaded-multicore-system.png" alt="Figure: Multithreaded multicore system." /><br />
<strong>Figure: Multithreaded multicore system.</strong></p>
<p>From an operating systems perspective, each hardware thread maintains its architectural
state thus appearing as a logical CPU. This is known as chip multithreading (CMT).
Intel processors use the term hyper-threading, or simultaneous multithreading, 
to describe assigning multiple hardware threads to a single processing core.</p>
<p><img src="week_8/../media/chip-multithreading.png" alt="Figure: Chip multithreading." /><br />
<strong>Figure: Chip multithreading.</strong></p>
<p>In the above diagram, the processor contains four computing cores, each containing
two hardware threads. From the perspective of the operating system, there are 
eight logical CPUs.</p>
<p>There are two ways to multithread a processing core:</p>
<ol>
<li>Coarse-grained multithreading</li>
<li>Fine-grained multithreading</li>
</ol>
<p>With coarse-grained multithreading, a thread executes on a core until a long-latency
event occurs. Due to the delay caused by the long-latency event, the core must 
switch to another thread to begin execution, this is expensive. Fine-grained
multithreading switches between threads at a much finer level of granularity.
The architectural design of fine-grained systems includes logic for thread switching
resulting in a low cost for switching between threads.</p>
<p>A multithreaded, multicore processor requires two different levels of scheduling.
This is because the resources of the physical cores must be shared among its hardware
threads and can therefore only execute one hardware thread at a time. On one level
are the scheduling decisions that must be made by the operating system as it
chooses which software thread to run on each hardware thread. A second level of
scheduling specifies how each core decides which hardware thread to run. These
two levels are not necessarily mutally exclusive.</p>
<h2 id="load-balancing"><a class="header" href="#load-balancing">Load Balancing</a></h2>
<p>Load balancing attempts to keep the workload evenly distributed across all processors
in an SMP system. Load balancing is typically necessary only on systems where 
each processor has its own private ready queue. On systems with a common run queue,
one a processor becomes idle, it immediately extracts a runnable thread from the
common ready queue.</p>
<p>There are two general approaches to load balancing:</p>
<ul>
<li><strong>Push migration</strong>: A specific task periodically checks the load on each processor 
and, if it finds an imbalance, evenly distributes the load by moving (or pushing)
threads from the overloaded to idle or less-busy processors.</li>
<li><strong>Pull migration</strong>: A pull migration occurs when an idle processor pulls a 
waiting task from a busy processor.</li>
</ul>
<p>The concept of a balanced load may have different meanings. One view may be that
a balanced load requires that all queues have approximately the same number of
threads while another view may be that there must be an equal distribution of 
thread priorities across all queues.</p>
<h2 id="processor-affinity"><a class="header" href="#processor-affinity">Processor Affinity</a></h2>
<p>As a thread runs on a specific processor, the data it uses populates the processors 
cache. If the thread is required to migrate to another process, the contents of 
the cached memory must be invalidated for the first processor, and the cache for 
the second processor must be repopulated. This is a high cost operation and most
operating systems, with the aid of SMP, try to avoid migrating a thread from one
processor to another. Instead, they attempt to keep a thread running on the same 
processor to take advantage of the &quot;warm&quot; cache. This is known as processor affinity,
that is, a process has an affinity for the processor on which is is currently 
running.</p>
<p>If the approach of a common ready queue is adopted, a thread may be selected for 
execution by any processor. Thus, if a thread is scheduled on a new processor, 
that processor’s cache must be repopulated. With private, per-processor ready 
queues, a thread is always scheduled on the same processor and can therefore 
benefit from the contents of a warm cache, essentially providing processor 
affinity for free.</p>
<p>Soft affinity occurs when the operating system has a policy of attempting to 
keep a process running on the same process but doesn't guarantee it will do so.
In contrast, some systems provide system calls that support hard affinity, thereby
allowing a process to specify a subset of processors on which is can run.</p>
<h2 id="real-time-cpu-scheduling"><a class="header" href="#real-time-cpu-scheduling">Real-Time CPU Scheduling</a></h2>
<p>Soft real-time systems provide no guarantee as to when a critical real-time 
process will be scheduled. They guarantee only that the process will be given
preference over non-critical processes. In a hard real-time system, a task must
be serviced by its deadline; service after the deadline has expired is the same
as no service at all.</p>
<h3 id="minimising-latency"><a class="header" href="#minimising-latency">Minimising Latency</a></h3>
<p>Event latency is the amount of time that elapses from when an event occurs to when
it's serviced. Different events have different latency requirements. For example,
the latency requirement for an antilock brake system may be between 3 to 5 milliseconds
while an embedded system controlling a radar in an airliner might tolerate a 
latency period of several seconds.</p>
<p><img src="week_8/../media/event-latency.png" alt="Figure: Event latency." /><br />
<strong>Figure: Event latency.</strong></p>
<p>Two types of latencies affect the performance of real-time systems:</p>
<ol>
<li><strong>Interrupt latency</strong>: The period of time from the arrival of an interrupt to the 
CPU to the start of the routine that services the interrupt.</li>
<li><strong>Dispatch latency</strong>: The amount of time required for the scheduling dispatcher
to stop one process and start another.</li>
</ol>
<h3 id="priority-based-scheduling"><a class="header" href="#priority-based-scheduling">Priority-Based Scheduling</a></h3>
<p>TODO</p>
<h3 id="rate-monotonic-scheduling"><a class="header" href="#rate-monotonic-scheduling">Rate-Monotonic Scheduling</a></h3>
<p>TODO</p>
<h3 id="earliest-deadline-first-scheduling"><a class="header" href="#earliest-deadline-first-scheduling">Earliest-Deadline-First Scheduling</a></h3>
<p>TODO</p>
<h3 id="proportional-share-scheduling"><a class="header" href="#proportional-share-scheduling">Proportional Share Scheduling</a></h3>
<p>TODO</p>
<h3 id="posix-real-time-scheduling"><a class="header" href="#posix-real-time-scheduling">POSIX Real-Time Scheduling</a></h3>
<p>TODO</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="week-9"><a class="header" href="#week-9">Week 9</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="deadlocks"><a class="header" href="#deadlocks">Deadlocks</a></h1>
<p>In a multi-programming environment, several threads may compete for a finite 
number of resources. A thread requests resources; if the resources are not available
at that time, the thread enters a waiting state. Sometimes, a waiting thread can
never again change state because the resources it has requested are heldd by other 
waiting threads. This is what is referred to as a deadlock. </p>
<p>A thread must request a resource before using it and must release the resource 
after using it. A thread may request as many resources as it requires to carry
out its designated task. The number of resources requested may not exceed the 
total number of resources available in the system.</p>
<p>Under the normal mode of operation, a thread may utilise a resource in only the
following sequence:</p>
<ol>
<li><strong>Request</strong>: The thread requests the resource. If the request cannot be granted
immediately, then the requesting thread must wait until it can acquire the resource.</li>
<li><strong>Use</strong>: The thread can operate on the resource.</li>
<li><strong>Release</strong>: The thread releases the resource.</li>
</ol>
<p>The request and release of resources may be system calls such as:</p>
<ul>
<li><code>request()</code> and <code>release()</code> of a device.</li>
<li><code>open()</code> and <code>close()</code> of a file.</li>
<li><code>allocate()</code> and <code>free()</code> memory system calls.</li>
</ul>
<p>Request and release can alos be accomplished through the <code>wait()</code>a nd <code>signal()</code> 
operations on semaphores and through <code>acquire()</code> and <code>release()</code> of a mutex lock.</p>
<p>For each use of a kernel-managed resource by a thread, the operating system checks
to make sure that the thread has requested and has been allocated the resource.
A system table records whether each resource is free or allocated. For each resource
that is allocated, the table also records the thread to which it is allocated.
If a thread requests a resource that is currently allocated to another thread, 
it can be added to a queue of threads waiting for this resource.</p>
<pre><code class="language-c">// thread_one runs in this function
void *do_work_one(void *param) {
    pthread_mutex_lock(&amp;first_mutex);
    pthread_mutex_lock(&amp;second_mutex);

    // Do some work

    pthread_mutex_unlock(&amp;second_mutex);
    pthread_mutex_unlock(&amp;first_mutex);

    pthread_exit(0);
}

// thread_two runs in this function
void *do_work_two(void *param) {
    pthread_mutex_lock(&amp;second_mutex);
    pthread_mutex_lock(&amp;first_mutex);

    // Do some work

    pthread_mutex_unlock(&amp;first_mutex);
    pthread_mutex_unlock(&amp;second_mutex);

    pthread_exit(0);
}
</code></pre>
<p><strong>Figure:</strong> Deadlock example.</p>
<h2 id="livelock"><a class="header" href="#livelock">Livelock</a></h2>
<p>Livelock is another form of liveness failure. It is similar to deadlock; both 
prevent two or more threads from proceeding, but the threads are unable to
processed for different reasons. Whereas deadlock occurs when every thread in a 
set is blocked waiting for an event that can be caused only by another thread in
the set, livelock occurs when a thread continuously attempts an action that fails.</p>
<p>Livelock typically occurs when threads retry failing operations at the same time.
It thus can generally be avoided by having each thread retry the failing operation
at random times.</p>
<pre><code class="language-c">// thread_one runs in this function
void *do_work_one(void *param) {
    int done = 0;

    while (!done) {
        pthread_mutex_lock(&amp;first_mutex);

        if (pthread_mutex_trylock(&amp;second_mutex)) {
            // Do some work

            pthread_mutex_unlock(&amp;second_mutex);
            pthread_mutex_unlock(&amp;first_mutex);

            done = 1;
        } else {
            pthread_mutex_unlock(&amp;first_mutex);
        }
    }

    pthread_exit(0);
}

// thread_two runs in this function
void *do_work_two(void *param) {
    int done = 0;

    while (!done) {
        pthread_mutex_lock(&amp;second_mutex);

        if (pthread_mutex_trylock(&amp;first_mutex)) {
            // Do some work

            pthread_mutex_unlock(&amp;first_mutex);
            pthread_mutex_unlock(&amp;second_mutex);

            done = 1;
        } else {
            pthread_mutex_unlock(&amp;second_mutex);
        }
    }

    pthread_exit(0);
}
</code></pre>
<p><strong>Figure:</strong> Livelock example.</p>
<h2 id="deadlock-charaterisation"><a class="header" href="#deadlock-charaterisation">Deadlock Charaterisation</a></h2>
<p>A deadlock situation can arise if the following four conditions hold simultaneously
in the system:</p>
<ol>
<li><strong>Mutual exclusion</strong>: At least one resource must be held in a non-sharable
mode; that is, only one thread at a time can use the resource. If another thread
requests that resource, the requesting thread must be delayed until the resource
has been released.</li>
<li><strong>Hold and wait</strong>: A thread must be holding at least one resource and waiting
to acquire additional resources that are currently being held by other threads.</li>
<li><strong>No preemption</strong>: Resources cannot be preempted; that is, a resource can be
released only voluntarily by the thread holding it, after that thread has completed
its task.</li>
<li><strong>Circular wait</strong>: A set \(\{T_0, T_1, \dots, T_n\}\) of waiting threads
must exist such that \(T_0\) is waiting for a resource held by \(T_1\), \(T_1\)
is waiting for a resource held by \(T_2, \dots, T_{n-1}\) is waiting for a
resource held by \(T_n\), and \(T_n\) is waiting for a resource held by \(T_0\).</li>
</ol>
<h2 id="resource-allocation-graph"><a class="header" href="#resource-allocation-graph">Resource-Allocation Graph</a></h2>
<p>Deadlocks can be described more precisely in terms of a directed graph called
a system resource-allocation graph. The set of vertices \(V\) is partitioned 
into two different types of nodes:</p>
<ol>
<li>\(T = \{T_0, T_1, \dots, T_n\}\), the set consisting of all the active 
threads in the system.</li>
<li>\(R = \{R_0, R_1, \dots, R_m\}\), the set consisting of all resource types
in the system.</li>
</ol>
<p>A directed edge from thread \(T_i\) to resource type \(R_j\) is denoted
\(T_i \rightarrow R_j\) and is called a request edge; it signifies that thread 
\(T_i\) has requested an instance of resource type \(R_j\) and is currently 
waiting for that resource. A directed edge from resource type \(R_j\) to thread 
\(T_i\) is denoted \(R_j \rightarrow T_i\) and is called an assignment edge; 
it signifies that an instance of resource type \(R_j\) has been allocated to 
thread \(T_i\).</p>
<p>Pictorially, each thread \(T_i\) is represented as a circle and each resource 
type \(R_j\) as a rectangle. An instance of a resource is represented by a dot
within the rectangle. For example, the below graph depicts the following situation:</p>
<ul>
<li>The sets \(T\), \(R\), and \(E\):
<ul>
<li>\(T = \{T_1, T_2, T_3\}\)</li>
<li>\(R = \{R_1, R_2, R_3, R_4\}\)</li>
<li>\(E = \{T_1 \rightarrow R_1, T_2 \rightarrow R_3, R_1 \rightarrow T_2, R_2 \rightarrow T_2, R_2 \rightarrow T_1, R_3 \rightarrow T_3\}\)</li>
</ul>
</li>
<li>Resource instances:
<ul>
<li>One instance of resource type \(R_1\)</li>
<li>Two instances of resource type \(R_2\)</li>
<li>One instance of resource type \(R_3\)</li>
<li>Three instances of resource type \(R_4\)</li>
</ul>
</li>
<li>Thread states:
<ul>
<li>Thread \(T_1\) is holding an instance of resource type \(R_2\) and is
waiting for an instance of resource type \(R_1\).</li>
<li>Thread \(T_2\) is holding an instance of resource type \(R_1\) and an 
instance of \(R_2\) and is waiting for an instance of resource type \(R_3\).</li>
<li>Thread \(T_3\) is holding an instance of resource type \(R_3\).</li>
</ul>
</li>
</ul>
<div align="center">
    <img src="week_9/../media/resource_allocation_graph.png" alt="Figure: Resource-allocation graph." />
    <figure><strong>Figure:</strong> Resource-allocation graph.</figure>
</div>
<p>Given the definition of a resource-allocation graph, it can be shown that, if 
the graph contains no cycles, then no thread in the system is deadlocked. If
the graph does contain a cycle, then a deadlock may exist.</p>
<ul>
<li>If each resource type has exactly one instance, then a cycle implies that a 
deadlock has occurred. </li>
<li>If the cycle involves only a set of resource types, each of which has only a 
single instance, then a deadlock has occurred. In this case, a cycle in the graph 
is both a necessary and sufficient condition for the existence of deadlock.</li>
<li>If each resource type has several instances, then a cycle does not necessarily
imply that a deadlock has occurred. In this case, a cycle in the graph is a necessary
but not a sufficient condition for the existence of deadlock.</li>
</ul>
<div align="center">
    <img src="week_9/../media/resource-allocation-graph-with-deadlock.png" alt="Figure: Resource-allocation graph with a deadlock." />
    <figure><strong>Figure:</strong> Resource-allocation graph with a deadlock.</figure>
</div>
<div align="center">
    <img src="week_9/../media/resource-allocation-graph-with-cycle-no-deadlock.png" alt="Figure: Resource-allocation graph with a cycle but no deadlock." />
    <figure><strong>Figure:</strong> Resource-allocation graph with a cycle but no deadlock.</figure>
</div>
<p>If a resource-allocation graph does not have a cycle, then the system is not in
a deadlocked state. If there is a cycle, then the system may or may not be in a
deadlocked state.</p>
<h2 id="methods-for-handling-deadlocks"><a class="header" href="#methods-for-handling-deadlocks">Methods for Handling Deadlocks</a></h2>
<p>A deadlock problem can generally be dealt with in one of three ways:</p>
<ol>
<li>We can ignore the problem altogether and pretend that deadlocks never occur 
in the system.</li>
<li>We can use a protocol to prevent or avoid deadlocks, ensuring that the system
will never enter a deadlocked state.</li>
<li>We can allow the system to enter a deadlocked state, detect it, and recover.</li>
</ol>
<p>To ensure that deadlocks never occur, the system can use either a deadlock-prevention
or a deadlock-avoidance scheme. Deadlock prevention provides a set of methods
to ensure that at least one of the necessary conditions cannot hold. These methods
prevent deadlocks by constraining how requests for resources can be made. Deadlock
avoidance requires that the operation system be given additional information in
advance concerning which resources a thread will request and use during its lifetime.
With this additional information, the operating system can decide for each request
whether or not the thread should wait.</p>
<h2 id="deadlock-prevention"><a class="header" href="#deadlock-prevention">Deadlock Prevention</a></h2>
<h3 id="mutual-exclusion"><a class="header" href="#mutual-exclusion">Mutual Exclusion</a></h3>
<p>The mutual-exclusion condition must hold. That is, at least one resource must 
be nonsharable. Sharable resources do not require mutually exclusive access and 
thus cannot be involved in a deadlock. Read-only files are a good example of a 
sharable resource. If several threads attempt to open a read-only file at the 
same time, they can be granted simultaneous access to the file. A thread never 
needs to wait for a sharable resource. In general, however, we cannot prevent 
deadlocks by denying the mutual-exclusion condition, because some resources are 
intrinsically nonsharable. For example, a mutex lock cannot be simultaneously 
shared by several threads.</p>
<h3 id="hold-and-wait"><a class="header" href="#hold-and-wait">Hold and Wait</a></h3>
<p>To ensure that the hold-and-wait condition never occurs in the system, we must
guarantee that, whenever a thread requests a resource, it does not hold any other
resources.</p>
<ul>
<li>One protocol that can be used requires each thread to request and be allocated
all its resources before it begins execution. This is impractical for most applications
due to the dynamic nature of requesting resources.</li>
<li>An alternative protocol allows a thread to request resources only when it has
none. A thead may request some resources and use them. Before it can request
additional resources, it must release all the resources that it is currently
allocated.</li>
</ul>
<p>Both these protocols have two main disadvantages:</p>
<ol>
<li>Resource utilisation may be low since resources may be allocated but unused
for a long period.</li>
<li>Starvation is possible. A thread that needs several popular resources may have
to wait indefinitely because at least one of the resources that it needs is always
allocated to some other thread.</li>
</ol>
<h3 id="no-preemption"><a class="header" href="#no-preemption">No Preemption</a></h3>
<p>The third necessary condition for deadlocks is that there be no preemption of 
resources that have already been allocated. To ensure that this condition does 
not hold, we can use the following protocol. If a thread is holding some resources 
and requests another resource that cannot be immediately allocated to it (that 
is, the thread must wait), then all resources the thread is currently holding 
are preempted. In other words, these resources are implicitly released. The 
preempted resources are added to the list of resources for which the thread is 
waiting. The thread will be restarted only when it can regain its old resources, 
as well as the new ones that it is requesting. </p>
<p>Alternatively, if a thread requests some resources, we first check whether they 
are available. If they are, we allocate them. If they are not, we check whether 
they are allocated to some other thread that is waiting for additional resources. 
If so, we preempt the desired resources from the waiting thread and allocate them 
to the requesting thread. If the resources are neither available nor held by a 
waiting thread, the requesting thread must wait. While it is waiting, some of its 
resources may be preempted, but only if another thread requests them. A thread 
can be restarted only when it is allocated the new resources it is requesting 
and recovers any resources that were preempted while it was waiting.</p>
<h3 id="circular-wait"><a class="header" href="#circular-wait">Circular Wait</a></h3>
<p>One way to ensure that this condition never holds is to impose a total ordering
of all resource types and to require that each thread requests resources in an
increasing oredr of enumeration.</p>
<pre><code>void transaction(Account from, Account to, double amount) {
    mutex lock1;
    mutex lock2;
    lock1 = get_lock(from);
    lock2 = get_lock(to);

    acquire(lock1);
        acquire(lock2);

            withdraw(from, amount);
            deposit(to, amount);

        release(lock2);
    release(lock1);
}
</code></pre>
<h2 id="deadlock-avoidance"><a class="header" href="#deadlock-avoidance">Deadlock Avoidance</a></h2>
<p>An alternative method for avoiding deadlocks is to require additional information
about how resources are to be requested. The various algorithms that use this
approach differ in the amount and type of information required. The simplest and
most useful model requires that each thread declare the maximum number of resources
of each type that it may need. Given this prior information, it is possible to
construct an algorithm that ensures that the system will never enter a deadlocked
state.</p>
<h3 id="safe-state"><a class="header" href="#safe-state">Safe State</a></h3>
<p>A system is in a safe state only if there exists a safe sequence. In other words, 
a state is safe if the system can allocate resources to each thread in some order
and still avoid a deadlock. A sequence of threads \(&lt;T_1, T_2, \dots, T_n&gt;\)
is a safe sequence for the current allocation if, for each \(T_i\), the resource
requests that \(T_i\) can still make can be satisfied by the current available
resources plus the resources held by all \(T_j\), with \(j &lt; i\). If no such
sequence exists, then the system state is said to be unsafe.</p>
<p>A safe state is not a deadlocked state. Conversely, a deadlocked state is an 
unsafe state. Not all unsafe states are deadlocks however. As long as the state 
is safe, the operating system can avoid unsafe states.</p>
<h3 id="resource-allocation-graph-algorithm"><a class="header" href="#resource-allocation-graph-algorithm">Resource-Allocation-Graph Algorithm</a></h3>
<p>TODO:</p>
<h3 id="bankers-algorithm"><a class="header" href="#bankers-algorithm">Banker's Algorithm</a></h3>
<p>TODO:</p>
<h3 id="safety-algorithm"><a class="header" href="#safety-algorithm">Safety Algorithm</a></h3>
<p>TODO:</p>
<h3 id="resource-request-algorithm"><a class="header" href="#resource-request-algorithm">Resource-Request Algorithm</a></h3>
<p>TODO:</p>
<h2 id="deadlock-detection"><a class="header" href="#deadlock-detection">Deadlock Detection</a></h2>
<p>If a system does not employ either a deadlock-prevention or a deadlock-avoidance
algorithm, then a deadlock situation may occur. In this environment, the system
may provide:</p>
<ul>
<li>An algorithm that examines the state ofo the system to determine whether a deadlock
has occured.</li>
<li>An algorithm to recover from the deadlock.</li>
</ul>
<h3 id="single-instance-of-each-resource-type"><a class="header" href="#single-instance-of-each-resource-type">Single Instance of Each Resource Type</a></h3>
<p>If all resources have only a single instance, then we can define a deadlock-detection
algorithm that uses a variant of the resource-allocation graph by a wait-for graph.</p>
<p>This graph is obtained from the resource-allocation graph by removing the resource
nodes and collapsing the appropriate edges. More precisely, an edges from \(T_i\)
to \(T_j\) in a wait-for graph implies that thread \(T_i\) is waiting for
thread \(T_j\) to release a resource that \(T_i\) needs. An edge \(T_i \rightarrow T_j\)
exists in a wait-for graph if and only if the corresponding resource-allocation
graph contains two edges \(T_i \rightarrow R_q\) and \(R_q \rightarrow T_i\)
for some resource \(R_q\).</p>
<p>As before, a deadlock exists in the system if and only if the wait-for graph
contains a cycle. To detect deadlocks, the system needs to maintain the wait-for
graph and periodically invoke an algorithm that searches for a cycle in the graph.
An algorithm to detect a cycle in a graph requires \(O(n^2)\) operations, where
\(n\) is the number of vertices in the graph.</p>
<h3 id="several-instances-of-a-resource-type"><a class="header" href="#several-instances-of-a-resource-type">Several Instances of a Resource Type</a></h3>
<p>The wait-for graph scheme is not applicable to a resource-allocation system with
multiple instances of each resource type. This algorithm employs several time-varying
data structures that are similar to those used in the bankers algorithm:</p>
<ul>
<li><strong>Available</strong>: A vector of length \(m\) indicates the number of available 
resources of each type.</li>
<li><strong>Allocation</strong>: An \(n \times m\) matrix defines the number of resources of 
each type currently allocated to each thread.</li>
<li><strong>Request</strong>: An \(n \times m\) matrix indicates the current request of each 
thread. If \(Request[i][j]\) equals \(k\), then thread \(T_i\) is requesting \(k\)
more instances of resource type \(R_j\).</li>
</ul>
<ol>
<li>Let \(Work\) and \(Finish\) be vectors of length \(m\) and \(n\), respectively.
Initialise \(Work = Available\). For \(i = 0, 1, \dots, n-1\), if \(Allocation_i \ne 0\),
then \(Finish[i] = false\). Otherwise, \(Finish[i] = true\).</li>
<li>Find an index \(i\) such that both,<br />
a: \(Finish[i] == false\)<br />
b: \(Request_i \le Work\)<br />
If no such \(i\) exists, go to step 4.</li>
<li>\(Work = Work + Allocation_i\)<br />
\(Finish[i] == true\)<br />
Go to step 2</li>
<li>If \(Finish[i] == false\) for some \(i, 0 \le i \le n\), then the system 
is in a deadlocked state. Moreover, if \(Finish[i] == false\), then thread
\(T_i\) is deadlocked.</li>
</ol>
<p>This algorithm requires an order of \(m \times n^2\) operations to detect whether
the system is in a deadlocked state.</p>
<h2 id="detection-algorithm-usage"><a class="header" href="#detection-algorithm-usage">Detection-Algorithm Usage</a></h2>
<p>When should we invoke the detection algorithm? The answer depends on two factors:</p>
<ol>
<li>How often is a deadlock likely to occur?</li>
<li>How many threads will be affected by deadlock when it happens?</li>
</ol>
<p>If the detection algorithm is invoked at arbitrary points in time, the resource 
graph may contain many cycles. In this case, we generally cannot tell which of
the many deadlocked threads &quot;caused&quot; the deadlock.</p>
<h2 id="recovery-from-deadlock"><a class="header" href="#recovery-from-deadlock">Recovery from Deadlock</a></h2>
<p>When a detection algorithm determines that a deadlock exists, several alternatives
are available. One possibility is to inform the operator that a deadlock has
occurred and to let the operator deal with the deadlock manually. Another possibility
is to let the system recover from the deadlock automatically.</p>
<p>There are two options for breaking a deadlock:</p>
<ol>
<li>Abort one or more threads to break the circular wait.</li>
<li>Preempt some resources from one or more of the deadlocked threads.</li>
</ol>
<h3 id="process-and-thread-termination"><a class="header" href="#process-and-thread-termination">Process and Thread Termination</a></h3>
<p>To eliminate deadlocks by aborting a process or thread, two methods can be used.
In both methods, the system reclaims all resources allocated to the terminated
processes.</p>
<ol>
<li><strong>Abort all deadlocked processes</strong>: This method will break the deadlock cycle,
but at great expense. The deadlocked processes may have computed for a long time, 
and the result of these partial computations must be discarded and probably will 
have to be re-computed later.</li>
<li><strong>Abort one process at a time until the deadlock cycle is eliminated</strong>: This 
method incurs considerable overhead, since after each process is aborted, a
deadlock-detection algorithm must be invoked.</li>
</ol>
<p>Aborting a process may not be easy. If the process was in the midst of updating
a file, terminating it may leave that file in an incorrect state. If the partial
termination method is used, then we must determine which deadlocked process
(or processes) should be terminated. This determination is a policy decision, 
similar to CPU-scheduling decisions. We should abort those processes whose 
termination will incur the minimum cost. Many factors may affect which process
is chosen, such as:</p>
<ul>
<li>What the priority of the process is.</li>
<li>How long the process has computed and how much longer the process will compute
before completing its designated task.</li>
<li>How many and what types of resources the process has used.</li>
<li>How many more resources the process needs in order to complete.</li>
<li>How many processes will need to be terminated.</li>
</ul>
<h2 id="resource-preemption"><a class="header" href="#resource-preemption">Resource Preemption</a></h2>
<p>To eliminate deadlocks using resource preemption, we successively preempt some
resources from processes and give these resources to other processes until the
deadlock cycle is broken. If preemption is required to deal with deadlocks, then
three issues need to be addressed:</p>
<ol>
<li><strong>Selecting a victim</strong>: Which resources and which processes are to preempted?
As in process termination, we must determine the order of preemption to minimise
cost.</li>
<li><strong>Rollback</strong>: If we preempt a resource from a process, what should be done with
that process? Clearly, it cannot continue with its normal excecution; it is missing
some needed resource. We must roll back the process to some safe state and restart
it from that state. Since, in general, it is difficult to determine what a safe
states is, the simplest solution is a total rollback: abort the process and then
restart it.</li>
<li><strong>Starvation</strong>: How do we ensure that starvation will not occur? That is, how
can we guarantee that resources will not always be preempted from the same process.
In a system where victim selection is based prirmarily on cost factors, it may
happen that the same process is always picked as a victim. As a result, this process
never completes its designated task.</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="week-11"><a class="header" href="#week-11">Week 11</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="main-memory"><a class="header" href="#main-memory">Main Memory</a></h1>
<h2 id="background"><a class="header" href="#background">Background</a></h2>
<p>Memory consists of a large array of bytes, each with its own address. The CPU
fetches instructions from memory according to the value of the program counter.
These instructions may cause additional loading from and storing to specific memory
addresses.</p>
<p>A typical instruction-execution cycle:</p>
<ol>
<li>First fetches an instruction from memory. </li>
<li>The instruction is then decoded and may cause operands to be fetched from 
memory addresses.</li>
<li>After the instruction has been executed on the operands, results may be stored 
back into memory.</li>
</ol>
<p>The memory unit sees only a stream of memory addresses; it does not know how 
they are generated or what they are for.</p>
<p>Main memory and the registers built into each processing core are the only general-purpose
storage that the CPU can access directly. Ther eare machin instructions that take
memory addresses as arguments, but none that take disk addresses. Therefore, any
instructions in execution, and any data being used by the instructions, must be 
in one of these direct-access storage devices. If the data are not in memory, they
must be moved there before the CPU can operate on them.</p>
<p>Registers that are built into each CPU core are generally accessible within one
cycle of the CPU clock. Some CPU cores can decode instructions and perform simple
operations on register contents at the rate of one or more operations per clock 
tick. The same cannot be said of main memory, which is accessed via a transaction 
on the memory bus. Completing a memory access may take many cycles of the CPU clock.
In such cases, the processor normally needs to stall, since it does not have the
data required to complete the instruction that it is executing. This situation
is intolerable because of the frequency of memory accesses. The remedy is to add
fast memory (cache) between the CPU and main memory, typically on the CPU chip 
for fast access. To manage a cache built into the CPU, the hardware automatically
speeds up memory access without any operating-system control.</p>
<p>For proper system operation, we must protect the operating system from access by
user processes, as well as protect user processes from one another. This protection
must be provided by the hardware, because the operating system doesn't usually 
intervene between the CPU and its memory accesses (because of the resulting performance
penalty).</p>
<h2 id="base-and-limit-registers"><a class="header" href="#base-and-limit-registers">Base and Limit Registers</a></h2>
<p>We first need to make sure that each process has a separate memory space. Separate 
per-process memory space protects the processes from each other and is fundamental 
to having multiple processes loaded in memory for concurrent execution. To separate 
memory spaces, we need the ability to determine the range of legal addresses that 
the process may access and to ensure that the process can access only these legal 
addresses. We can provide this protection by using two registers, usually a base 
and a limit.</p>
<p><img src="week_11/../media/base-and-limit-register.png" alt="Figure: A base and a limit register define a logical address space." /><br />
<strong>Figure: A base and a limit register define a logical address space.</strong></p>
<p>The base register holds the smallest legal physical memory address; the limit 
register specifies the size of the range. For example, if the base register holds 
300040 and the limit register is 120900, then the program can legally access all 
addresses from 300040 through 420939 (inclusive).</p>
<p>Protection of memory space is accomplished by having the CPU hardware compare
every address generated in user mode with the registers. Any attempt by a program
executing in user mode to access operating-system memory or other users' memory
results in a trap to the operating system, which treats the attempt as a fatal 
error. This scheme prevents a user program from (accidentally or deliberately) 
modifying the code or data structures of either the operating system or other users.</p>
<p><img src="week_11/../media/hardware-address-protection.png" alt="Figure: Hardware address protection with base and limit registers." />
<strong>Figure: Hardware address protection with base and limit registers.</strong></p>
<h2 id="address-binding"><a class="header" href="#address-binding">Address Binding</a></h2>
<p>Usually, a program resides on a disk as a binary executable file. To run, the program
must be brought into memory and placed within the context of a process, where it
becomes eligible for execution on an available CPU. As the process executes, it
access instructions and data from memory. Eventually, the process terminates, and
its memory is reclaimed for use by other processes.</p>
<p>Most systems allow a user process to reside in any part of the physical memory.
Thus, although the address space of the computer may start at 00000, the first
address of the user process need not be 00000. In most cases, a user program goes
through several steps - some of which may be optional - before being executed.
Addresses may be represented in different ways during these steps. Addresses
in the source program are generally symbolic (such as the variable <code>count</code>). A 
compiler typically binds these symbolic address to relocatable addresses (such 
as &quot;14 bytes from the beginning of this module&quot;). The linker or loader in turn
binds the relocatable addresses to absolute addresses (such as 74014). Each binding
is a mapping from one address space to another.</p>
<p><img src="week_11/../media/compiling-diagram.png" alt="Figure: Multistep processing of a user program." />
<strong>Figure: Multistep processing of a user program.</strong></p>
<ul>
<li><strong>Compile time</strong>: If you know at compile time where the process will reside in
memory, then absolute code can be generated. For example, if you know that a user 
process will reside starting at location \(R\), then the generated compiler code 
will start at that location and extend up from there. If, at some later time, 
the starting location changes, then it will be necessary to recompile this code.</li>
<li><strong>Load time</strong>: If it is not known at compile time where the process wil reside
in memory, then the compiler must generate relocatable code. In this case, final
binding is delayed until load time. If the starting address changes, we need only
reload the user code to incorporate this changed value.</li>
<li><strong>Execution time</strong>: If the process can be moved during its execution from one
memory segment to another, then binding must be delayed until run time. Special
hardware must be avaiable for this scheme to work.</li>
</ul>
<h2 id="logical-vs-physical-address-space"><a class="header" href="#logical-vs-physical-address-space">Logical vs. Physical Address Space</a></h2>
<p>An address generated by the CPU is commonly referred to as a logical address,
whereas an address seen by the memory unit - that is, the one loaded into the 
memory-address register of the memory - is commonly referred to as a physical
address.</p>
<p>Binding addresses at either compile or load time generates identical logical
and physical addresses. However, the execution-time address-binding scheme results
in differing logical and physical addresses. In this case, we usually refer to
the logical address as a virtual address. The set of logical address generated
by a program is a logical address space. The set of physical addresses corresponding
to these logical addresses is a physical address space. Thus, in the execution-time
address-binding scheme, the logical and physical address spaces differ.</p>
<h2 id="memory-management-unit-mmu"><a class="header" href="#memory-management-unit-mmu">Memory Management Unit (MMU)</a></h2>
<p>THe run-time mapping from virtual to physical addresses is done by a hardware
device called the memory-management unit (MMU). We can choose from many different
methods to accomplish such mapping. </p>
<p>Consider a simple scheme where the value in the relocation register (the base register) 
is added to every address generated by a user process at the time it is sent to 
memory. For example, if the base is at 14000, then an attempt by the user to 
address location 0 is dynamically relocated to location 14000; an access to location 
346 is mapped to location 14346.</p>
<p>The user program never accesses the real physical addresses. The program can 
create a pointer to location 346, store it in memory, manipulate it, and compare 
it with other addresses all as the number 346. Only when it is used as a memory 
address (in an indirect load or store, perhaps) is it relocated relative to the 
base register. The user program deals with logical addresses. The final location
of a referenced memory address is not determined until the reference is made.</p>
<h2 id="dynamic-linking"><a class="header" href="#dynamic-linking">Dynamic Linking</a></h2>
<p>So far, it has been necessary for the entire program and all data of a process 
to be in physical memory for the process to execute. The size of a process has 
thus been limited to the size of physical memory. To obtain better memory-space 
utilization, we can use dynamic loading. With dynamic loading, a routine is not
loaded until it is called. All routines are kept on disk in a relocatable load
format. The main program is loaded into memory and is executed. When a routine 
needs to call another routine, the calling routine first checks to see whether 
the other routine has been loaded. If it has not, the relocatable linking loader
is called to load the desired routine into memory and to update the programs'
address tables to reflect this change. Then control is passed to the newly loaded
routine. Dynamic loading does not require any special support from the operating
system. It is the responisiblity of the user to design their programs to take 
advantage of such a method. Operating systems may however help the programmer
by providing library routines to implement dynamic loading.</p>
<p>Dynamically linked libraries (DLLs) are system libraries that are linked to user
programs when the programs are run. Some operating systems support only static
linking, in which system libraries are treated like any other object module and
are combined by the loader into the binary program image. Dynamic linking, in 
contrast, is similar to dynamic loading. Here, though, linking, rather than loading,
is postponed until execution time. A second advantage of DLLs is that these libraries
can be shared among multiple processes, so that only one instance of the DLL is 
in main memory. For this reason, DLLs are also known as shared libraries.</p>
<p>Unlike dynamic loading, dynamic linking and shared libraries generally require
help from the operating system. If the processes in memory are protected from
one another, then the operating system is the only entity that can check to see
whether the needed routine is in another process's memory space or that can allow
multiple processes to access the same memory addresses.</p>
<h2 id="contiguous-allocation"><a class="header" href="#contiguous-allocation">Contiguous Allocation</a></h2>
<p>The main memory must accommodate both the operating system and the various user 
procces. We therefore need to allocate main memory in the most efficient way
possible. The main memory is usually divided into two partitions: one for the
operating system and one for the user processes. We can place the operating system
in either low memory addresses or high memory addresses. This decision depends 
on many factors, such as the location of the interrupt vector. However, many operating
systems place the operating system in high memory.</p>
<p>We usually want several user processes to reside in memory at the same time. We 
therefore need to consider how to allocate available memory to the processes that 
are waiting to be brought into memory. In contiguous memory allocation, each 
process is contained in a single section of memory that is contiguous to the section 
containing the next process.</p>
<p>We can prevent a process from accessing memory that it does not own by combining
two ideas previously discussed. If we have a system with a relocation register,
together with a limit register, we accomplish our goal. The relocation register
contains the value of the smallest physical address; the limit register contains
the range of logical addresses. The MMU maps the logical address dynamically by
adding the value in the relocation register. This mapped address is sent to memory.</p>
<p><img src="week_11/../media/hardware-support-for-relocation.png" alt="Figure: Hardware support for relocation and limit registers." /><br />
<strong>Figure: Hardware support for relocation and limit registers.</strong></p>
<p>When the CPU scheduler selects a process for execution, the dispatcher loads the 
relocation and limit registers with the correct values as part of the context 
switch. Because every address generated by a CPU is checked against these registers, 
we can protect both the operating system and the other users’ programs and data 
from being modified by this running process.</p>
<p>One of the simplest methods of allocating memory is to assign processes to variably
sized partitions in memory, where each partition may contain exactly one process.
In this variable-partition shecme, the operating system keeps a table indicating
which parts of memory are available and which are occupied. Initially, all memory
is available for user processes and is considered one large block of available
memory, a hole. Memory contains a set of holes of various sizes.</p>
<p><img src="week_11/../media/variable-partition.png" alt="Figure: Variable partition." /><br />
<strong>Figure: Variable partition.</strong></p>
<p>Initially, the memory is fully utilised, containing processes 5, 8, and 2.
After process 8 leaves, there is one contiguous hole. Later on, process 9
arrives and is allocated memory. Then process 5 departs, resulting in two non-contiguous
holes.</p>
<p>As processes enter the system, the operating system takes into account the memory
requirements of each process and the amount of availabe memory space in determining
which processes are allocated memory. When a process is allocated space, it is
loaded into memory, where it can then compete for CPU time. When a process terminates,
it releases its memory, which the operating system may then provide to another 
process.</p>
<p>In general, the memory blocks available comprise a set of holes of various sizes
scattered throughout memory. When a process arrives and needs memory, the system
searches the set for a hole that is large enough for this process. If the hole 
is too large, it is split into two parts. One part is allocated for the arriving
process; the other is returned to the set of holes. When a process terminates,
it releases its block of memory, which is then placed back in the set of holes.
If the new hole is adjacent to other holes, these adjacent holes are merged to 
form one larger hole. This procedure is a particular instance of the general
dynamic storage-allocation problem, which concerns how to satisfy a request of
size \(n\) from a list of free holes. There are many solutions to this problem.</p>
<ul>
<li><strong>First fit</strong>: Allocate the first hole that is big enough. Searching can start
either at the beginning of the set of holes or at the location where the previous
first-fit search ended. We can stop searching as soon as we find a free hole that
is large enough.</li>
<li><strong>Best fit</strong>: Allocate the smallest hole that is big enough. We must search
the entire list, unless the list is ordered by size. This strategy produces the
smallest leftover hole.</li>
<li><strong>Worst fit</strong>: Allocate the largest hole. Again, we must search the entire list,
unless it is sorted by size. This strategy produces the largest leftover hole,
which may be more useful than the smaller leftover hole from a best-fit approach.</li>
</ul>
<p>Simulations have shown that both first fit and best fit are better than worst 
fit in terms of decreasing time and storage utilization. Neither first fit nor 
best fit is clearly better than the other in terms of storage utilization, but 
first fit is generally faster.</p>
<h2 id="fragmentation"><a class="header" href="#fragmentation">Fragmentation</a></h2>
<p>Both the first-fit and best-fit strategies for memory allocation suffer from
external fragmentation. As processes are loaded and removed from memory, the
free memory space is broken into little pieces. External fragmentation exists
when there is enough total memory space to satisfy a request but the available
spaces are not contiguous.</p>
<p>Depending on the total amount of memory storage and the average process size,
external fragmentation may be a minor or a major problem. Statistical analysis
of first fit, for instance, reveals that, even with some optimisation, given \(N\)
allocated blocks, another 0.5 \(N\) blocks will be lost to fragmentation.
That is, one third of memory may be unusable. This property is known as the
50-percent rule.</p>
<p>Memory fragmentation can be internal as well as external. Consider a multiple-partition 
allocation scheme with a hole of 18,464 bytes. Suppose that the next process 
requests 18,462 bytes. If we allocate exactly the requested block, we are left 
with a hole of 2 bytes. The overhead to keep track of this hole will be substantially 
larger than the hole itself. The general approach to avoiding this problem is 
to break the physical memory into fixed-sized blocks and allocate memory in units 
based on block size. With this approach, the memory allocated to a process may 
be slightly larger than the requested memory. The difference between these two 
numbers is internal fragmentation — unused memory that is internal to a partition.</p>
<p>One solution to the problem of external fragmentation is compaction. The goal
is to shuffle the memory contents so as to place all free memory together in one
large block. Compaction is not always possible, however. If relocation is static
and is done at assembly or load time, compaction cannot be done. It is possible
only if relocation is dynamic and is done at execution time.</p>
<h2 id="paging"><a class="header" href="#paging">Paging</a></h2>
<p>Paging is a memory-management scheme that permits a process's physical address
space to be non-contiguous. Paging avoids external fragmentation and the associated
need for compaction.</p>
<p>The basic method for implementing paging involves breaking physical memory into
fixed-sized blocks called frames and breaking logical memory into blocks of the
same size called pages. When a process is to be executed, its pages are loaded 
into any available memory frames from their source. The backing store is divided
into fized-sized blocks that are the same size as the memory frames or clusters
of multiple frames. For example, the logical address space is now totally separate
from the physical address space, so a process can have a logical 64-bit address
space even though the system has less than \(2^{64}\) bytes of physical memory.</p>
<p>Every address generated by the CPU is divided into two parts: a page numbeer (p)
and a page offset (d). The page number is used as an index into a pre-process
page table. The page table contains the base address of each frame in physical
memory, and the offset is the location in the frame being referenced. Thus the
base address of the frame is combined with the page offset to define the physical
memory address.</p>
<p>The use of registers for the page table is satisfactory if the page table is reasonably 
small (for example, 256 entries). Most contemporary CPUs, however, support much 
larger page tables (for example, 220 entries). For these machines, the use of 
fast registers to implement the page table is not feasible. Rather, the page table 
is kept in main memory, and a page-table base register (PTBR) points to the page 
table. Changing page tables requires changing only this one register, substantially 
reducing context-switch time.</p>
<p>Although storing the page table in main memory can yield faster context switches, 
it may also result in slower memory access times. Suppose we want to access location 
\(i\). We must first index into the page table, using the value in the PTBR 
offset by the page number for \(i\). This task requires one memory access. It 
provides us with the frame number, which is combined with the page offset to 
produce the actual address. We can then access the desired place in memory. With 
this scheme, two memory accesses are needed to access data (one for the page-table 
entry and one for the actual data). Thus, memory access is slowed by a factor 
of 2, a delay that is considered intolerable under most circumstances.</p>
<p>The standard solution to this problem is to use a special, small, fast-lookup 
hardware cache called a translation look-aside buffer (TLB). To be able to execute 
the search within a pipeline step, however, the TLB must be kept small. It is 
typically between 32 and 1,024 entries in size. Some CPUs implement separate 
instruction and data address TLBs. That can double the number of TLB entries 
available, because those lookups occur in different pipeline steps.</p>
<p>If the page number is not in the TLB (known as a TLB miss), address translation 
proceeds, where a memory reference to the page table must be made. When the frame 
number is obtained, we can use it to access memory. In addition, we add the page 
number and frame number to the TLB, so that they will be found quickly on the 
next reference.</p>
<p>Some TLBs store address-space identifier (ASIDs) in each TLB entry. An ASID uniquely 
identifies each process and is used to provide address-space protection for that 
process. When the TLB attempts to resolve virtual page numbers, it ensures that 
the ASID for the currently running process matches the ASID associated with the 
virtual page. If the ASIDs do not match, the attempt is treated as a TLB miss. 
In addition to providing address-space protection, an ASID allows the TLB to contain 
entries for several different processes simultaneously. If the TLB does not support 
separate ASIDs, then every time a new page table is selected (for instance, with 
each context switch), the TLB must be flushed (or erased) to ensure that the next 
executing process does not use the wrong translation information. Otherwise, the 
TLB could include old entries that contain valid virtual addresses but have 
incorrect or invalid physical addresses left over from the previous process.</p>
<p>The percentage of times that the page number of interest is found in the TLB is 
called the hit ratio. An 80-percent hit ratio, for example, means that we find 
the desired page number in the TLB 80 percent of the time. If it takes 10 nanoseconds 
to access memory, then a mapped-memory access takes 10 nanoseconds when the page 
number is in the TLB. If we fail to find the page number in the TLB then we must 
first access memory for the page table and frame number (10 nanoseconds) and then 
access the desired byte in memory (10 nanoseconds), for a total of 20 nanoseconds. 
(We are assuming that a pagetable lookup takes only one memory access, but it can 
take more, as we shall see.) </p>
<p>To find the effective memory-access time, we weight the case by its probability:</p>
<p>\[
\begin{align*}
\text{effective access time} &amp;= 0.80 \times 10 + 0.20 \times 20 \\
&amp;= 12 \text{nanoseconds}
\end{align*}
\]</p>
<p>In this example, we suffer a 20-percent slowdown in average memory-access time 
(from 10 to 12 nanoseconds).</p>
<h2 id="memory-protection"><a class="header" href="#memory-protection">Memory Protection</a></h2>
<p>Memory protection in a paged environment is accomplished by protection bits associated
with each frame. Normally, these bits are kept in the page table. One bit can
define a page to be read - write or read-only. Every reference to memory goes
through the page table to find the correct frame number. At the same time that
physical address is being computed, the protection bits can be checked to verify 
that no writes are being made to a read-only page. At attempt to write to a read-only
page causes a hardware trap to the operating system (or memory-protection violation).</p>
<p>One additional bit is generally attached to each entry in the page table: a valid-invalid
bit. When this bit is set to valid, the associated page is in the process's logical
address space and is thus a legal (or valid) page. When the bit is set to invalid, 
the page is not in the process's logical address space. Illegal addresses are trapped
by use of the valid-invalid bit. The operating system sets this bit for each page
to allow or disallow access to the page.</p>
<p>Rarely does a process use all its address range. In fact, many processes use only
a small fraction of the address space available to them. It would be wasteful in
these cases to create a page table with entries for every page in the address range.
Most of this table would be unused but would take up valuable memory space. Some
systems provide hardware, in the form of a page-table length register (PTLR), to
indicate the size of the page table. This value is checked against every logical 
address to verify that the address is in the valid range for the process. Failure
of this test causes an error trap to the operating system.</p>
<h2 id="shared-pages"><a class="header" href="#shared-pages">Shared Pages</a></h2>
<p>An advantage of paging is the possibility of sharing common code, a consideration
that is particularly important in an environment with multiple processes. On a 
typical Linux system, most user processes require the standard C library libc. 
One option is to have each process load its own copy of libc into its address space. 
If a system has 40 user processes, and the libc library is 2 MB, this would require 
80 MB of memory.</p>
<p>If the code is reentrant code, however, it can be shared.</p>
<p><img src="week_11/../media/sharing-in-paging-env.png" alt="Figure: Sharing of standard C library in a paging environment." /><br />
<strong>Figure: Sharing of standard C library in a paging environment.</strong></p>
<p>Here, we see three processes sharing the pages for the standard C library libc. 
Reentrant code is non-self-modifying code: it never changes during execution. 
Thus, two or more processes can execute the same code at the same time. Each process 
has its own copy of registers and data storage to hold the data for the process’s 
execution. The data for two different processes will, of course, be different. 
Only one copy of the standard C library need be kept in physical memory, and the 
page table for each user process maps onto the same physical copy of libc. Thus, 
to support 40 processes, we need only one copy of the library, and the total space 
now required is 2 MB instead of 80 MB — a significant saving. In addition to run-time 
libraries such as libc, other heavily used programs can also be shared — compilers, 
window systems, database systems, and so on.</p>
<h2 id="hierarchical-page-tables"><a class="header" href="#hierarchical-page-tables">Hierarchical Page Tables</a></h2>
<p>Most modern computer systems support a large logical address space (232 to 264). 
In such an environment, the page table itself becomes excessively large.
Clearly, we would not want to allocate the page table contiguously in main memory. 
One simple solution to this problem is to divide the page table into smaller pieces.</p>
<p>One way of accomplishing this division is to use a two-level paging algorithm,
in which the page table itself is also pages.</p>
<p><img src="week_11/../media/two-level-page-table-scheme.png" alt="Figure: A two-level page-table scheme." /><br />
<strong>Figure: A two-level page-table scheme.</strong></p>
<h2 id="hashed-page-tables"><a class="header" href="#hashed-page-tables">Hashed Page Tables</a></h2>
<p>One approach for handling address spaces larger than 32 bits is to use a hashed 
page table, with the hash value being the virtual page number. Each entry in the 
hash table contains a linked list of elements that hash to the same location 
(to handle collisions). Each element consists of three fields: (1) the virtual 
page number, (2) the value of the mapped page frame, and (3) a pointer to the 
next element in the linked list.</p>
<p>A variation of this scheme that is useful for 64-bit address spaces has been proposed. 
This variation uses clustered page tables, which are similar to hashed page tables 
except that each entry in the hash table refers to several pages (such as 16) 
rather than a single page. Therefore, a single page-table entry can store the 
mappings for multiple physical-page frames. Clustered page tables are particularly 
useful for sparse address spaces, where memory references are noncontiguous and 
scattered throughout the address space.</p>
<h2 id="inverted-page-tables"><a class="header" href="#inverted-page-tables">Inverted Page Tables</a></h2>
<p>An inverted page table has one entry for each real page (or frame) of memory. 
Each entry consists of the virtual address of the page stored in that real memory 
location, with information about the process that owns the page. Thus, only one 
page table is in the system, and it has only one entry for each page of physical 
memory.</p>
<h2 id="swapping"><a class="header" href="#swapping">Swapping</a></h2>
<p>Process instructions and the data they operate on must be in memory to be executed. 
However, a process, or a portion of a process, can be swapped temporarily out of 
memory to a backing store and then brought back into memory for continued execution. 
Swapping makes it possible for the total physical address space of all processes 
to exceed the real physical memory of the system, thus increasing the degree of 
multiprogramming in a system.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="week-12"><a class="header" href="#week-12">Week 12</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="protection"><a class="header" href="#protection">Protection</a></h1>
<p>The role of protection in a computer system is to provide a mechanism for the 
enforcement of the policies governing resource use. These policies can be established 
in a variety of ways. Some are fixed in the design of the system, while others 
are formulated by the management of a system. Still others are defined by individual 
users to protect resources they “own.” A protection system, then, must have the 
flexibility to enforce a variety of policies.</p>
<p>Policies for resource use may vary by application, and they may change over time. 
For these reasons, protection is no longer the concern solely of the designer of 
an operating system. The application programmer needs to use protection mechanisms 
as well, to guard resources created and supported by an application subsystem 
against misuse.</p>
<p>Frequently, a guiding principle can be used throughout a project, such as the 
design of an operating system. Following this principle simplifies design decisions 
and keeps the system consistent and easy to understand. A key, timetested guiding 
principle for protection is the principle of least privilege. This principle 
dictates that programs, users, and even systems be given just enough privileges 
to perform their tasks.</p>
<p>Observing the principle of least privilege would give the system a chance to mitigate 
the attack — if malicious code cannot obtain root privileges, there is a chance 
that adequately defined permissions may block all, or at least some, of the damaging 
operations. In this sense, permissions can act like an immune system at the 
operating-system level.</p>
<p>Another important principle, often seen as a derivative of the principle of least 
privilege, is compartmentalization. Compartmentalization is the process of protecting 
each individual system compo- nent through the use of specific permissions and 
access restrictions. Then, if a component is subverted, another line of defense 
will “kick in” and keep the attacker from compromising the system any further.</p>
<h2 id="domain-of-protection"><a class="header" href="#domain-of-protection">Domain of Protection</a></h2>
<p>Rings of protection separate functions into domains and order them hierarchically.
A process should be allowed to access only those objects for which it has authorization. 
Furthermore, at any time, a process should be able to access only those objects 
that it currently requires to complete its task. This second requirement, the 
need-to-know principle, is useful in limiting the amount of damage a faulty process 
or an attacker can cause in the system. For example, when process <code>p</code> invokes procedure 
<code>A()</code>, the procedure should be allowed to access only its own variables and the 
formal parameters passed to it; it should not be able to access all the variables 
of process <code>p</code>.</p>
<p>In comparing need-to-know with least privilege, it may be easiest to think of 
need-to-know as the policy and least privilege as the mechanism for achieving 
this policy. For example, in file permissions, need-to-know might dictate that 
a user have read access but not write or execute access to a file. The principle 
of least privilege would require that the operating system provide a mechanism 
to allow read but not write or execute access.</p>
<p>To facilitate the sort of scheme just described, a process may operate within a 
protection domain, which specifies the resources that the process may access. 
Each domain defines a set of objects and the types of operations that may be 
invoked on each object. The ability to execute an operation on an object is an 
access right.</p>
<p>A domain is a collection of access rights, each of which is an ordered pair
<code>&lt;object-name, rights-set&gt;</code>. For example, if domain \(D\) has the access right 
<code>&lt;file F, {read,write}&gt;</code>, then a process executing in domain \(D\) can both 
read and write file \(F\). It cannot, however, perform any other operation on 
that object.</p>
<p><img src="week_12/../media/domain-structure.png" alt="Figure: System with three protection domains." /><br />
<strong>Figure: System with three protection domains.</strong></p>
<h2 id="access-matrix"><a class="header" href="#access-matrix">Access Matrix</a></h2>
<p>The general model of protection can be viewed abstractly as a matrix, called an 
access matrix. The rows of the access matrix represent domains, and the columns 
represent objects. Each entry in the matrix consists of a set of access rights. 
Because the column defines objects explicitly, we can omit the object name from 
the access right. The entry <code>access(i,j)</code> defines the set of operations that a 
process executing in domain \(D_i\) can invoke on object \(O_j\).</p>
<p><img src="week_12/../media/access-matrix.png" alt="Figure: Access matrix." /><br />
<strong>Figure: Access matrix.</strong></p>
<p>When we switch a process from one domain to another, we are executing an operation 
(switch) on an object (the domain). Processes should be able to switch from one 
domain to another. Switching from domain \(D_i\) to domain \(D_j\) is allowed 
if and only if the access right \(\text{switch} \in \text{access}(i, j)\).
Allowing controlled change in the contents of the access-matrix entries requires 
three additional operations: copy, owner, and control. We examine these operations 
next. The ability to copy an access right from one domain (or row) of the access 
matrix to another is denoted by an asterisk (*) appended to the access right. 
The copy right allows the access right to be copied only within the column (that 
is, for the object) for which the right is defined.</p>
<p>The problem of guaranteeing that no information initially held in an object can 
migrate outside of its execution environment is called the confinemen problem. 
This problem is in general unsolvable.</p>
<h1 id="file-systems"><a class="header" href="#file-systems">File Systems</a></h1>
<p>Computers can store information on various storage media, such as NVM devices, 
HDDs, magnetic tapes, and optical disks. So that the computer system will be 
convenient to use, the operating system provides a uniform logical view of stored 
information. The operating system abstracts from the physical properties of its 
storage devices to define a logical storage unit, the file. Files are mapped by 
the operating system onto physical devices. These storage devices are usually 
nonvolatile, so the contents are persistent between system reboots.</p>
<p>A file is a named collection of related information that is recorded on secondary 
storage. From a user’s perspective, a file is the smallest allotment of logical 
secondary storage; that is, data cannot be written to secondary storage unless 
they are within a file. Commonly, files represent programs (both source and object 
forms) and data. Data files may be numeric, alphabetic, alphanumeric, or binary. 
Files may be free form, such as text files, or may be formatted rigidly. In general, 
a file is a sequence of bits, bytes, lines, or records, the meaning of which is 
defined by the file’s creator and user.</p>
<h2 id="file-attributes"><a class="header" href="#file-attributes">File Attributes</a></h2>
<p>A file is named, for the convenience of its human users, and is referred to by 
its name. A name is usually a string of characters, such as <code>example.c</code>. Some 
systems differentiate between uppercase and lowercase characters in names, whereas 
other systems do not. When a file is named, it becomes independent of the process, 
the user, and even the system that created it. For instance, one user might create 
the file <code>example.c</code>, and another user might edit that file by specifying its name. 
The file’s owner might write the file to a USB drive, send it as an e-mail attachment, 
or copy it across a network, and it could still be called <code>example.c</code> on the 
destination system. Unless there is a sharing and synchonization method, that 
second copy is now independent of the first and can be changed separately.</p>
<p>A file’s attributes vary from one operating system to another but typically consist 
of these:</p>
<ul>
<li><strong>Name</strong>: The symbolic file name is the only information kept in humanreadable 
form.</li>
<li><strong>Identifier</strong>: This unique tag, usually a number, identifies the file within 
the file system; it is the non-human-readable name for the file.</li>
<li><strong>Type</strong>: This information is needed for systems that support different types 
of files.</li>
<li><strong>Location</strong>: This information is a pointer to a device and to the location 
of the file on that device.</li>
<li><strong>Size</strong>: The current size of the file (in bytes, words, or blocks) and possibly
the maximum allowed size are included in this attribute.</li>
<li><strong>Protection</strong>: Access-control information determines who can do reading, writing, 
executing, and so on.</li>
<li><strong>Timestamps and user identification</strong>: This information may be kept for creation, 
last modification, and last use. These data can be useful for protection, security, 
and usage monitoring.</li>
</ul>
<p>The information about all files is kept in the directory structure, which resides 
on the same device as the files themselves.</p>
<h2 id="file-operations"><a class="header" href="#file-operations">File Operations</a></h2>
<p>A file is an abstract data type. To define a file properly, we need to consider 
the operations that can be performed on files. The operating system can provide 
system calls to create, write, read, reposition, delete, and truncate files.</p>
<ul>
<li><strong>Creating a file</strong>: Two steps are necessary to create a file. First, space in 
the file system must be found for the file. Second, an entry for the new file 
must be made in a directory.</li>
<li><strong>Opening a file</strong>: Rather than have all file operations specify a file name, 
causing the operating system to evaluate the name, check access permissions, 
and so on, all operations except create and delete require a file open() first. 
If successful, the open call returns a file handle that is used as an argument 
in the other calls.</li>
<li><strong>Writing a file</strong>: To write a file, we make a system call specifying both the 
open file handle and the information to be written to the file. The system must 
keep a write pointer to the location in the file where the next write is to take 
place if it is sequential. The write pointer must be updated whenever a write 
occurs.</li>
<li><strong>Reading a file</strong>: To read from a file, we use a system call that specifies 
the file handle and where (in memory) the next block of the file should be put. 
Again, the system needs to keep a read pointer to the location in the file where 
the next read is to take place, if sequential. Once the read has taken place, 
the read pointer is updated. Because a process is usually either reading from 
or writing to a file, the current operation location can be kept as a per-process 
current-file-positio pointer. Both the read and write operations use this same 
pointer, saving space and reducing system complexity.</li>
<li>**Repositioning within a file. The current-file-position pointer of the open 
file is repositioned to a given value. Repositioning within a file need not involve 
any actual I/O. This file operation is also known as a file seek.</li>
<li><strong>Deleting a file</strong>: To delete a file, we search the directory for the named 
file. Having found the associated directory entry, we release all file space, 
so that it can be reused by other files, and erase or mark as free the directory 
entry. Note that some systems allow hard links—multiple names (directory entries) 
for the same file. In this case the actual file contents is not deleted until 
the last link is deleted.</li>
<li><strong>Truncating a file</strong>: The user may want to erase the contents of a file but 
keep its attributes. Rather than forcing the user to delete the file and then 
recreate it, this function allows all attributes to remain unchanged — except 
for file length. The file can then be reset to length zero, and its file space 
can be released.</li>
</ul>
<p>Most of the file operations mentioned involve searching the directory for the 
entry associated with the named file. To avoid this constant searching, many 
systems require that an <code>open()</code> system call be made before a file is first used.
The operating system keeps a table, called the open-file table, containing information 
about all open files. When a file operation is requested, the file is specified 
via an index into this table, so no searching is required. When the file is no 
longer being actively used, it is closed by the process, and the operating system 
removes its entry from the open-file table, potentially releasing locks. 
<code>create()</code> and <code>delete()</code> are system calls that work with closed rather than open 
files.</p>
<p>Typically, the open-file table also has an open count associated with each file 
to indicate how many processes have the file open. Each <code>close()</code> decreases this 
open count, and when the open count reaches zero, the file is no longer in use, 
and the file’s entry is removed from the open-file table.</p>
<p>Some operating systems provide facilities for locking an open file (or sections 
of a file). File locks allow one process to lock a file and prevent other processes 
from gaining access to it. File locks are useful for files that are shared by 
several processes - for example, a system log file that can be accessed and modified 
by a number of processes in the system. File locks provide functionality similar 
to reader–writer locks. A shared lock is akin to a reader lock in that several 
processes can acquire the lock concurrently. An exclusive lock behaves like a 
writer lock; only one process at a time can acquire such a lock.</p>
<p>Furthermore, operating systems may provide either mandatory or advisory file-locking
mechanisms. With mandatory locking, once a process acquires an exclusive lock, 
the operating system will prevent any other process from accessing the locked file.
For example, assume a process acquires an exclusive lock on the file <code>system.log</code>. 
If we attempt to open <code>system.log</code> from another process — for example, a text editor — 
the operating system will prevent access until the exclusive lock is released. 
Alternatively, if the lock is advisory, then the operating system will not prevent 
the text editor from acquiring access to <code>system.log</code>. Rather, the text editor 
must be written so that it manually acquires the lock before accessing the file. 
In other words, if the locking scheme is mandatory, the operating system ensures 
locking integrity. For advisory locking, it is up to software developers to ensure 
that locks are appropriately acquired and released.</p>
<p><img src="week_12/../media/common-file-types.png" alt="Figure: Common file types." /><br />
<strong>Figure: Common file types.</strong></p>
<h2 id="file-structure"><a class="header" href="#file-structure">File Structure</a></h2>
<p>Some operating systems impose (and support) a minimal number of file structures. 
This approach has been adopted in UNIX, Windows, and others. UNIX considers each 
file to be a sequence of 8-bit bytes; no interpretation of these bits is made by 
the operating system. This scheme provides maximum flexibility but little support. 
Each application program must include its own code to interpret an input file as 
to the appropriate structure. However, all operating systems must support at 
least one structure—that of an executable file—so that the system is able to 
load and run programs.</p>
<p>File types also can be used to indicate the internal structure of the file. Source 
and object files have structures that match the expectations of the programs that 
read them. Further, certain files must conform to a required structure that is 
understood by the operating system. For example, the operating system requires 
that an executable file have a specific structure so that it can determine where 
in memory to load the file and what the location of the first instruction is. 
Some operating systems extend this idea into a set of system-supported file structures, 
with sets of special operations for manipulating files with those structures.</p>
<h2 id="access-methods"><a class="header" href="#access-methods">Access Methods</a></h2>
<p>Files store information. When it is used, this information must be accessed and 
read into computer memory. The information in the file can be accessed in several 
ways. Some systems provide only one access method for files. Others (such as 
mainframe operating systems) support many access methods, and choosing the right 
one for a particular application is a major design problem.</p>
<p>The simplest access method is sequential access. Information in the file is processed 
in order, one record after the other. This mode of access is by far the most common; 
for example, editors and compilers usually access files in this fashion. Reads 
and writes make up the bulk of the operations on a file. A read operation — <code>read_next()</code> 
— reads the next portion of the file and automatically advances a file pointer, 
which tracks the I/O location. Similarly, the write operation— <code>write_next()</code> —
appends to the end of the file and advances to the end of the newly written 
material (the new end of file). Such a file can be reset to the beginning, and 
on some systems, a program may be able to skip forward or backward \(n\) records 
for some integer \(n\) — perhaps only for \(n = 1\)).</p>
<p>Another method is direct access (or relative access). Here, a file is made up 
of fixed-length logical records that allow programs to read and write records 
rapidly in no particular order. The direct-access method is based on a disk 
model of a file, since disks allow random access to any file block. For direct 
access, the file is viewed as a numbered sequence of blocks or records. Thus,
we may read block 14, then read block 53, and then write block 7. There are no 
restrictions on the order of reading or writing for a direct-access file.
Direct-access files are of great use for immediate access to large amounts of 
information. Databases are often of this type. When a query concerning a particular 
subject arrives, we compute which block contains the answer and then read that 
block directly to provide the desired information.</p>
<p>For the direct-access method, the file operations must be modified to include 
the block number as a parameter. Thus, we have <code>read(n)</code>, where <code>n</code> is the block 
number, rather than read <code>next()</code>, and <code>write(n)</code> rather than write <code>next()</code>. 
An alternative approach is to retain read <code>next()</code> and write <code>next()</code> and to add 
an operation position <code>file(n)</code> where <code>n</code> is the block number. Then, to effect 
a <code>read(n)</code>, we would position <code>file(n)</code> and then read <code>next()</code>.</p>
<p>The block number provided by the user to the operating system is normally a 
relative block number. A relative block number is an index relative to the beginning 
of the file. Thus, the first relative block of the file is 0, the next is 1, 
and so on, even though the absolute disk address may be 14703 for the first block 
and 3192 for the second. The use of relative block numbers allows the operating 
system to decide where the file should be placed (called the allocation problem) 
and helps to prevent the user from accessing portions of the file system that
may not be part of her file.</p>
<h2 id="other-access-methods"><a class="header" href="#other-access-methods">Other Access Methods</a></h2>
<p>Other access methods can be built on top of a direct-access method. These methods 
generally involve the construction of an index for the file. The index, like an 
index in the back of a book, contains pointers to the various blocks. To find a 
record in the file, we first search the index and then use the pointer to access 
the file directly and to find the desired record.</p>
<p>With large files, the index file itself may become too large to be kept in memory.
One solution is to create an index for the index file. The primary index file 
contains pointers to secondary index files, which point to the actual data items.</p>
<h2 id="types-of-file-systems"><a class="header" href="#types-of-file-systems">Types of File Systems</a></h2>
<p>Computer systems may also have varying numbers of file systems, and the file 
systems may be of varying types. Consider the types of file systems in Solaris:</p>
<ul>
<li><strong>tmpfs</strong>: A “temporary” file system that is created in volatile main memory
and has its contents erased if the system reboots or crashes</li>
<li><strong>objfs</strong>: A “virtual” file system (essentially an interface to the kernel that 
looks like a file system) that gives debuggers access to kernel symbols.</li>
<li><strong>ctfs</strong>: A virtual file system that maintains “contract” information to manage 
which processes start when the system boots and must continue to run during operation.</li>
<li><strong>lofs</strong>: A “loop back” file system that allows one file system to be accessed 
in place of another one.</li>
<li><strong>procfs</strong>: A virtual file system that presents information on all processes 
as a file system.</li>
<li><strong>ufs, zfs</strong>: A general-purpose file systems.</li>
</ul>
<h2 id="directory-structure"><a class="header" href="#directory-structure">Directory Structure</a></h2>
<p>The directory can be viewed as a symbol table that translates file names into 
their file control blocks. If we take such a view, we see that the directory 
itself can be organized in many ways. The organization must allow us to insert 
entries, to delete entries, to search for a named entry, and to list all the 
entries in the directory.</p>
<p>When considering a particular directory structure, we need to keep in mind the 
operations that are to be performed on a directory:</p>
<ul>
<li><strong>Search for a file</strong>: We need to be able to search a directory structure to
find the entry for a particular file. Since files have symbolic names, and similar
names may indicate a relationship among files, we may want to be able to
find all files whose names match a particular pattern.</li>
<li><strong>Create a file</strong>: New files need to be created and added to the directory.</li>
<li><strong>Delete a file</strong>: When a file is no longer needed, we want to be able to remove 
it from the directory. Note a delete leaves a hole in the directory structure 
and the file system may have a method to defragement the directory structure.</li>
<li><strong>List a directory</strong>: We need to be able to list the files in a directory and 
the contents of the directory entry for each file in the list.</li>
<li><strong>Rename a file</strong>: Because the name of a file represents its contents to its 
users, we must be able to change the name when the contents or use of the file 
changes. Renaming a file may also allow its position within the directory structure 
to be changed.</li>
<li><strong>Traverse the file system</strong>: We may wish to access every directory and every 
file within a directory structure. For reliability, it is a good idea to save 
the contents and structure of the entire file system at regular intervals. 
Often, we do this by copying all files to magnetic tape, other secondary storage, 
or across a network to another system or the cloud. This technique provides a 
backup copy in case of system failure. In addition, if a file is no longer in use, 
the file can be copied the backup target and the disk space of that file released 
for reuse by another file.</li>
</ul>
<h2 id="single-level-directory"><a class="header" href="#single-level-directory">Single-Level Directory</a></h2>
<p>The simplest directory structure is the single-level directory. All files are 
contained in the same directory.</p>
<ul>
<li>Advantages:
<ul>
<li>Easy to implement.</li>
<li>Easy to understand.</li>
</ul>
</li>
<li>Disadvantages:
<ul>
<li>Since all files are in the same directory, they must have unique names.</li>
<li>Relevant files cannot be grouped together.</li>
</ul>
</li>
</ul>
<p><img src="week_12/../media/single-level-dir.png" alt="Figure: Single-level directory." /><br />
<strong>Figure: Single-level directory.</strong></p>
<h2 id="two-level-directory"><a class="header" href="#two-level-directory">Two-Level Directory</a></h2>
<p>In the two-level directory structure, each user has his own user file directory 
(UFD). The UFDs have similar structures, but each lists only the files of a single 
user. When a user refers to a particular file, only his own UFD is searched. 
Thus, different users may have files with the same name, as long as all the file 
names within each UFD are unique.</p>
<p><img src="week_12/../media/two-level-dir.png" alt="Figure: Two-level directory structure." /><br />
<strong>Figure: Two-level directory structure.</strong></p>
<h2 id="tree-structured-directories"><a class="header" href="#tree-structured-directories">Tree-Structured Directories</a></h2>
<p>Once we have seen how to view a two-level directory as a two-level tree, the 
natural generalization is to extend the directory structure to a tree of arbitrary 
height. This generalization allows users to create their own subdirectories and 
to organize their files accordingly. A tree is the most common directory structure. 
The tree has a root directory, and every file in the system has a unique path name.
A directory (or subdirectory) contains a set of files or subdirectories. In many 
implementations, a directory is simply another file, but it is treated in a special 
way. All directories have the same internal format. One bit in each directory 
entry defines the entry as a file (0) or as a subdirectory (1).</p>
<p>Path names can be of two types: absolute and relative. In UNIX and Linux, an 
absolute path name begins at the root (which is designated by an initial “/”) 
and follows a path down to the specified file, giving the directory names on the 
path. A relative path name defines a path from the current directory. For example 
if the current directory is <code>/spell/mail</code>, then the relative path name <code>prt/first</code> 
refers to the same file as does the absolute path name <code>/spell/mail/prt/first</code>.</p>
<p><img src="week_12/../media/tree-structure-dir.png" alt="Figure: Tree-structured directory structure." /><br />
<strong>Figure: Tree-structured directory structure.</strong></p>
<h2 id="acyclic-graph-directories"><a class="header" href="#acyclic-graph-directories">Acyclic-Graph Directories</a></h2>
<p>Consider two programmers who are working on a joint project. The files associated 
with that project can be stored in a subdirectory, separating them from other 
projects and files of the two programmers. But since both programmers are equally 
responsible for the project, both want the subdirectory to be in their own directories. 
In this situation, the common subdirectory should be shared. A shared directory 
or file exists in the file system in two (or more) places at once.</p>
<p>A tree structure prohibits the sharing of files or directories. An acyclic graph
— that is, a graph with no cycles — allows directories to share subdirectories 
and files. The same file or subdirectory may be in two different directories. 
The acyclic graph is a natural generalization of the tree-structured directory 
scheme.</p>
<p>Shared files and subdirectories can be implemented in several ways. A common way, 
exemplified by UNIX systems, is to create a new directory entry called a link. 
A link is effectively a pointer to another file or subdirectory. For example, a 
link may be implemented as an absolute or a relative path name. When a reference 
to a file is made, we search the directory. If the directory entry is marked as 
a link, then the name of the real file is included in the link information. We 
resolve the link by using that path name to locate the real file. Links are easily 
identified by their format in the directory entry (or by having a special type 
on systems that support types) and are effectively indirect pointers. The operating 
system ignores these links when traversing directory trees to preserve the acyclic 
structure of the system.</p>
<p><img src="week_12/../media/acyclic-graph-dir.png" alt="Figure: Acyclic-graph directory structure." /><br />
<strong>Figure: Acyclic-graph directory structure.</strong></p>
<h2 id="file-system-mounting"><a class="header" href="#file-system-mounting">File System Mounting</a></h2>
<p>Just as a file must be opened before it can be used, a file system must be mounted
before it can be available to processes on the system. More specifically, the 
directory structure may be built out of multiple file-system-containing volumes, 
which must be mounted to make them available within the file- system name space.</p>
<p>The mount procedure is straightforward. The operating system is given the name 
of the device and the mount point — the location within the file structure where 
the file system is to be attached. Some operating systems require that a file-system 
type be provided, while others inspect the structures of the device and determine 
the type of file system. Typically, a mount point is an empty directory.</p>
<h2 id="disk-structure"><a class="header" href="#disk-structure">Disk Structure</a></h2>
<ul>
<li>Disk can be subdivided into partitions. Disks or partitions can be RAID 
(Redundant Array of Independent Disks) protected against failure.</li>
<li>Disk or partition can be used raw – without a file system, or formatted
with a file system.</li>
<li>A disk can be sliced into multiple partitions, or a volume can span multiple 
partitions on multiple disks.</li>
<li>An entity containing a file system known as a volume. Each volume containing 
file system also tracks that file system’s info in a device directory or volume 
table of contents.</li>
<li>As well as general-purpose file systems there are many special-purpose file systems, 
frequently all within the same operating system or computer.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="week-13"><a class="header" href="#week-13">Week 13</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="virtual-machines"><a class="header" href="#virtual-machines">Virtual Machines</a></h1>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            MathJax.Hub.Register.StartupHook('End', function() {
                window.setTimeout(window.print, 100);
            });
        });
        </script>

    </div>
    </body>
</html>
